% !TeX spellcheck = en_US
% !TeX encoding = UTF-8
% !TeX root = ../thesis.tex

\chapter{Evaluation}
\label{ch:Evaluation}
% pages: 7.14-9.52
% Structure:
%   - Describe what the experiments wanted to show:
%       - Which environments used.
%       - What we wanted to show.
%       - What is this experiment, what we wanted to look at?
%   - Show and describe the actual results:
%
%
%
%
%
%
%
%

% Basically all of the experiments have the goal to show the value of multiple-hops in comparison to robins architecture.
\section{Number of Hops}
Questions to answer:
\begin{itemize}[noitemsep,nolistsep]
    \item Multi-Hops (multiple Layers) => Agents use Evader-Nodes to cache information/data.
    \item In which situations do you benefit from more layers? Show benefit of multiple layers.
    \item Effect of Multi-Hops on different levels of culling on more difficult tasks. Are they able to negate worse communication ranges? Compare policy on large communication, 1 layer vs. small communication, alot of layers. knn: num-layers: everyone can always communicate.
    \item What happens with really tight observation range?
    \item Pursuit-Multi: Strategy better then Robin? (are they able to create groups?).
    \item Describe the strategies in the different scenarios.
\end{itemize}
Expected Answers:
\begin{itemize}[noitemsep,nolistsep]
    \item Harsher Culling => More Layers better. Can partially compensate. Still not the same information. Information about neighbors-neighbor still contains information about neighbor itself => it is influenced.
    \item policy on large communication, 1 layer > small communication, alot of layers. BUT may take longer?
    \item More Hops => Usually Better, but has limit. Complexer task => More Hops better (especially Pursuit-Multi).
    \item tight observation: at certain point it fails, even with alot of layers.
\end{itemize}
\begin{itemize}[noitemsep,nolistsep]
    \item Environments: Rendezvous, Pursuit-Multi
    \item Environment: Culling Methods: more culling vs less culling, num-agents and dynamics?
    \item Network: num-blocks, latent-dimension?, aggregation-function?
\end{itemize}


\section{Neighbor Aggregation Type}
Questions to answer:
\begin{itemize}[noitemsep,nolistsep]
    \item Agents better/worse being able to distinguish themselves from evaders for concat(aggr())?
    \item How does this effect aggregation function for aggr(aggr()). Where do I get better performance?
    \item How is the effect of more/less hopps here?
\end{itemize}
Expected Answers:
\begin{itemize}[noitemsep,nolistsep]
    \item concat(aggr()): worse iteration time, but easier to learn heterogeneous graphs.
    \item aggr(aggr()): probably mean aggregation, there are more features "preserved".
    \item num-hops: more hops should have more of an effect in concat(aggr()).
\end{itemize}
aggr(aggr()) vs concat(aggr())
\begin{itemize}[noitemsep,nolistsep]
    \item Environments: Pursuit-Single, Pursuit-Multi
    \item Environment: Base-Pursuit-Multi with 3+ Hops?
    \item Network: latent-dimension, aggregation-function, neighbor-aggregation, num-blocks
\end{itemize}


\section{Randomized Number of Agents and Evaders}
Questions to answer:
\begin{itemize}[noitemsep,nolistsep]
    \item How good is the architecture able to abstract task to random number of agents?
    \item How does this effect num-blocks?
    \item Extrapolate number of agents? Range (5-10), Evaluate on 10, 15, 20? and without range? (5?)
    \item How is this effected by culling?
\end{itemize}
Expected Answers:
\begin{itemize}[noitemsep,nolistsep]
    \item Random abstraction based on the range. More Randomness to train to => Better.
    \item Extrapolate: Better with range or without? 
    \item In theory GNN are not effected by rearranging input data and latent-dimension stays the same!
    \item Culling: Should learn to work better with varying effective communication ranges (hops).
\end{itemize}
random number of agents
\begin{itemize}[noitemsep,nolistsep]
    \item Environments: Rendezvous
    \item Environment: Rendezvous: Culling Methods: more culling vs less culling
    \item Network: latent-dimension, num-blocks
\end{itemize}
random number of agents + random number of evaders
\begin{itemize}[noitemsep,nolistsep]
    \item Environments: Pursuit-Multi
    \item Environment: Multi-Pursuit: Culling Methods: more culling vs less culling
    \item Network: latent-dimension, num-blocks
\end{itemize}


\section{GNN - directed vs. undirected}
Questions to answer:
\begin{itemize}[noitemsep,nolistsep]
    \item what "learns" better or has better information propagation
\end{itemize}
Expected Answers:
\begin{itemize}[noitemsep,nolistsep]
    \item 
\end{itemize}
\begin{itemize}[noitemsep,nolistsep]
    \item Environments: Pursuit-Multi, Pursuit-Single
    \item Just add this to each of the other experiments (?)
    \item Environements: use-directed-graph.
    \item Network: num-blocks, latent-dimension, aggregation-function, neighbor-aggregation, value-function-scope
\end{itemize}

\section{node-wise or graph-wise value function}
Questions to answer:
\begin{itemize}[noitemsep,nolistsep]
    \item graph-wise: more "true" to the original Single agent RL algorithms. Should correlate better? But need to figure out themselves which agent is responsible for a reward.
    \item node-wise: better able to to gauge which agent was responsible for a given value or reward. 
\end{itemize}
Expected Answers:
\begin{itemize}[noitemsep,nolistsep]
    \item 
\end{itemize}
\begin{itemize}[noitemsep,nolistsep]
    \item Environments: Rendezvous, Pursuit-Single
    \item Just add this to each of the other experiments (?)
    \item Environements: 
    \item Network: num-blocks, latent-dimension, aggregation-function, neighbor-aggregation, value-function-scope
\end{itemize}


\section{PPO - Code Level optimizations}
Questions to answer:
\begin{itemize}[noitemsep,nolistsep]
    \item How do these optimizations effect num-layers?
\end{itemize}
Expected Answers:
\begin{itemize}[noitemsep,nolistsep]
    \item a
\end{itemize}
%https://openreview.net/pdf?id=r1etN1rtPB
29.11: list of new PPO features implemented!
\begin{itemize}[noitemsep,nolistsep]
    \item Environments: Rendezvous
    \item value-function-clipping (0.0 - 1.0), 1.0 = no clipping
    \item normalize rewards
    \item reward-clipping: graph-normalized constructor: reward-clip = 5, currently no parameter
    \item observation-normalization
    \item global gradient clipping: max-grad-norm
    \item tanh (insted of LeakyReLU)
    \item can those have an effect on num-layers, aggregation-function and neighbor-aggregation?
\end{itemize}


\section{Dispersion}
Questions to answer:
\begin{itemize}[noitemsep,nolistsep]
    \item How do different aggregation-functions and reward-type learn easier/harder?
    \item How is this effected by num-layers? with certain aggregation-function/reward-type combinations?
\end{itemize}
Expected Answers:
\begin{itemize}[noitemsep,nolistsep]
    \item a
\end{itemize}
\begin{itemize}[noitemsep,nolistsep]
    \item Environments: Dispersion
    \item Environment: Culling Methods: more culling vs less culling, reward-type
    \item Network: latent-dimension, aggregation-function, num-blocks
\end{itemize}