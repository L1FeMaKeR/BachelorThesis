% !TeX spellcheck = en_US
% !TeX encoding = UTF-8
% !TeX root = ../thesis.tex

\chapter{Notepad}
\label{ch:Notepad}

Reference for short forms: \\
(App): Applications
(Arch): Architecture
(AC): Actor-Critic
(Base): Basic Papers
(Book): Book
(Com): Communication
(Con): Conference
(Evo): Evolutionary
(Het): Heterogeneous
(Hie): Hierarchy
(MO): Multi-Objective
(Role): Role-Based
(Sca): Scaling
(Sur): Survey/Reviews
(Tra): Transfer Learning\\
! means done.

\section{Artificial Intelligence}
\subsection{!Artificial Intelligence - A modern Approach}
\underline{Agents and Environments (p.34)}
\begin{itemize}[noitemsep,nolistsep]
	\item \textbf{agent}: anything that perceives its \textbf{environment} through \textbf{sensors} and acting upon that environment using \textbf{actuators}.
	\item \textbf{percept}: agent’s perceptual inputs at any given instance. Percept sequence is a complete history of perception.
	\item agents choice of action decided upon the history of perception, but not anything it has not perceived.
	\item its behavior is described by the \textbf{agent function}, which is internally implemented by the \textbf{agent program}.
\end{itemize}

\underline{Rational Agent (p.36)}
\begin{itemize}[noitemsep,nolistsep]
	\item \textbf{rational agent}: it does the correct thing. Correctness is determined by a performance measure, which is determined by the changed environment states.
	\item design \textbf{performance measures} according to what one actually wants in the environment, rather than according to how one thinks the agent should behave.
	\item rational depends on:
	\begin{itemize}
		\item the performance measure that defines the criterion of success
		\item the agent’s prior knowledge of the environment.
		\item The actions that the agent can perform.
		\item The agent’s percept sequence of data.
	\end{itemize}
	\item depending on the measures the agent might be rational or not. 
	\item an \textbf{omniscient agent} knows the actual outcome of its actions and can act accordingly, but this is impossible in reality.
	\item rationality maximizes expected performance, while perfection (omniscient) maximizes actual performance.
	\item agents can do actions in order to modify future percepts, called \textbf{information gathering, or exploration}.
	\item rational agents learn as much as possible from what it perceives.
	\item his knowledge can be augmented and modified as it gains experience.
	\item if the agent relies on the prior knowledge of its designer rather than on its own percepts, we say that the agent lacks \textbf{autonomy}.
	\item it should learn what it can to compensate for partial or incorrect prior knowledge.
	\item give it some initial knowledge and the ability to learn, so it will become independent of its prior knowledge.
\end{itemize}

\underline{Nature of Environments (p.40)}
\begin{itemize}[noitemsep,nolistsep]
	\item \textbf{task environments}: the “problems” to which rational agents are the “solutions”.
	\item Describe the task environment in the following aspects P(Performance measure), E(Environment), A(Actuators), S(Sensors).
	\item \textbf{fully observable}: the agent’s sensors give it access to the complete state of the environment. All aspects that are relevant to the choice of actions
	\item \textbf{partially observable}: otherwise. Because of missing sensors or noise.
	\item no sensors: unobservable
	\item single-agent environments and multi-agent environments.
	\item multi-agent can be either competitive (chess) or cooperative (avoiding collisions maximizes performance).
	\item \textbf{communication} emerges as a rational behavior in multiagent environments.
	\item randomized behavior is rational because it avoids the pitfalls of predictability.
	\item \textbf{Deterministic}: next state of environment is completely determined by the current state and the action executed by the agent, otherwise it is \textbf{stochastic}.
	\item you can ignore uncertainty that arises purely from the actions of other agents in a multiagent environment.
	\item If the environment is partial observable, it could appear to be stochastic, which implies quantifiable outcomes in terms of probabilities.
	\item an environment is \textbf{uncertain} if it is not fully observable or not deterministic. 
	\item \textbf{episodic}: the agent’s experience is divided into atomic episodes. In each the agent receives a percept and performs a single episode. The next episode does not depend on the actions taken in previous episodes, otherwise it is \textbf{sequential}.
	\item When the environment can change while the agent is deliberating, then the environment is \textbf{dynamic} for that agent otherwise it is \textbf{static}.
	\item if the environment itself does not change with the passage of time but the agent’s performance score does, then we say the environment is \textbf{semi dynamic}.
	\item \textbf{discrete/continuous} applies to the state of the environment, to the way time is handled, and to the percepts and actions of the agents.
	\item \textbf{known vs. unknown}: refers to the agent’s state of knowledge about the “laws of physics” of the environment. Known environment, the outcomes for all actions are given, otherwise the agent needs to learn how it works. An environment can be known, but partially observable (solitaire: I know the rules but still unable to see the cards that have not yet been turned over)
	\item hardest case: partially observable, multiagent, stochastic, sequential, dynamic, continuous, and unknown
	\item \textbf{environment class}: multiple environment scenarios to train it for multiple situations.
	\item you can create an \textbf{environment generator}, that selects environments in which to run the agent.
\end{itemize}

\underline{Structure of Agents (p.46)}
\begin{itemize}[noitemsep,nolistsep]
	\item agent = architecture (computing device) + program (agent program).
	\item agent programs take the current percept as input and return an action to the actuators.
	\item agent program takes the current percept, agent function which takes the entire percept history.
	\item \textbf{table driven agent}: Uses a table of actions indexed by percept sequences. This table grows way to fast and is therefore not practical.
\end{itemize}

\underline{Simple reflex agents}
\begin{itemize}[noitemsep,nolistsep]
	\item \textbf{simple reflex agents}: Select the actions on the basis of the current percept, ignoring the rest of the history.
	\item \textbf{condition-action-rule}: these agents create actions in a specific condition (if-then). These connections can be seen as reflexes.
	\item uses an \textbf{interpret-input} function as well as a \textbf{rule-match} function.
	\item they need the environment to be fully observable. They could run into infinite loops.
	\item you can mitigate this by using randomization for the actions. Which is non-rational for single agent environments.
\end{itemize}

\underline{Model-based reflex agents}
\begin{itemize}[noitemsep,nolistsep]
	\item keep track of the part of the world an agent cannot see now. It maintains some sort of \textbf{internal state} that depends on the percept history.
	\item agents needs to know how the world evolves independently of the agent and how the agent’s own actions affect the world.
	\item with this it creates a \textbf{model} of the world hence it is called model-based agent.
	\item it needs to update this state given sensor data.
	\item this model is a \textbf{best guess} and does not determine the entire current state of the environment exactly.
\end{itemize}

\underline{Goal-based agents}
\begin{itemize}[noitemsep,nolistsep]
	\item an agent needs some sort of \textbf{goal information} that describes situations that are desirable. This can also be combined with the model.
	\item Usually agents need to do multiple actions to fulfill a goal which requires \textbf{search} and \textbf{planning}.
	\item this also involves consideration of the future.
	\item the goal-based agent’s behavior can be easily changed to go to a different destination by using a goal where a reflex agent needs completely now rules.
\end{itemize}

\underline{Utility-based agents}
\begin{itemize}[noitemsep,nolistsep]
	\item goals provide a crude binary distinction between good and bad states.
	\item use an internal \textbf{utility function} to create a performance measure.
	\item if the external performance measure and the internal utility function agree, the agent will act rationally.
	\item if you have conflicting goals the utility function can specify the appropriate \textbf{tradeoff}.
	\item if multiple goals cannot be achieved with certainty, utility provides a way to determine the \textbf{likelihood} of success.
	\item a rational utility-based agent chooses the action that \textbf{maximizes the expected utility}.
	\item any rational agent must behave as if it possesses a utility function whose expected value it tries to maximize.
	\item a utility-based agent must model and keep track of its environment.
\end{itemize}

\underline{Learning Agents}
\begin{itemize}[noitemsep,nolistsep]
	\item it allows the agent to operate in initially unknown environments and to become more competent than its initial knowledge alone might allow.
	\item 4 conceptual components: \textbf{learning element} (responsible for improvements), \textbf{performance element} (select external action), \textbf{critic} (gives feedback to change the learning element), \textbf{problem generator} (suggesting actions that lead to new and informative experiences).
	\item critic tells the learning element how well the agent is doing given a performance standard. It tells the agent which percepts are good and which are bad.
	\item problem generator allows for exploration and suboptimal actions to discover better actions in the long run.
	\item learning element: simplest case: learning directly from the percept sequence.
	\item the \textbf{performance standard} distinguishes part of the incoming percept as a reward or penalty that provides direct feedback on the quality of the agent’s behavior.
\end{itemize}

\underline{How the components of agent programs work}
\begin{itemize}[noitemsep,nolistsep]
	\item \textbf{atomic representation}: Each state of the world is indivisible. Algorithms like search and game-playing, Hidden Markov models and Markov decision models work like this.
	\item \textbf{factored representation}: splits up each state of a fixed set of variables or attributes which each can have a value. Used in constraint satisfaction algorithms, propositional logic, planning, Bayesian networks.
	\item \textbf{structured representation}: here the different states have connections to each other. Used in relational databases, first-order logic, first-order probability models, knowledge-based learning and natural language understanding.
	\item more complex representations are more \textbf{expressive} and can capture everything more concise.
\end{itemize}

\section{Ant Colony Optimization}

\subsection{!Wikipedia Article}
\href{https://en.wikipedia.org/wiki/Ant_colony_optimization_algorithms}{Ant Colony Optimization Algorithm, Wikipedia}
\begin{itemize}[noitemsep,nolistsep]
	\item is used for solving computational problems which can be reduced to finding good paths through graphs.
	\item artificial ants locate optimal soluions by moving through a parameter space represneting all possible solutions.
	\item they record their positions and the quality of their solutions for later iterations to find better solutions (pheromones).
\end{itemize}

\subsection{Ant Colony Optimization (ACO)}
\href{https://www.sciencedirect.com/science/article/pii/S0888613X02000919}{ACO - Ant Colony Optimization for learning Bayesian network - 2002}

\section{Reinforcement Learning}
\subsection{!Algorithmia Blog}
\href{https://algorithmia.com/blog/introduction-to-reinforcement-learning}{Introduction to Reinforcement Learning}
\begin{itemize}[noitemsep,nolistsep]
	\item \textbf{Policy Learning}: Policy is a function: (state) -> (action). (if you approach an enemy and the enemy is stronger than you, turn backwards).
	\item Can use Neural Nets to approximate complicated functions
	\item \textbf{Q-Learning / Value Functions}: (state, action) -> (value). It also adds in all of the potential future values that this action might bring you.
	\item Approximate Q-Learning Functions with Neural Nets: DQN (RL - DQN - Human-level control through deep reinforcement - 2015)
	\item Newer way to approximate Q-Functions: A3C ( \href{https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2}{Tutorial}, RL - A3C - Asynchronous Methods for Deep Reinforcement Learning - 2016)
	\item \textbf{Challenges}:
	\begin{itemize}[noitemsep,nolistsep]
		\item Reinforcement Learning requires a ton of training data, that other algorithms can get to more efficiently.
		\item RL is a general algorithm. If the problem has a domain-specific solution that might work better than RL. Tradeoff between scope and intensity.
		\item Most pressing Issue: Design of the reward function. it could get stuck in local optima
	\end{itemize} 
\end{itemize}

\subsection{!Freecodecamp}
\href{https://www.freecodecamp.org/news/an-introduction-to-reinforcement-learning-4339519de419/}{An introduction to Reinforcement Learning}
\begin{itemize}[noitemsep,nolistsep]
	\item $State\ S_t, Reward\ R_t, Action\ A_t$
	\item \textbf{Reward Hypothesis}: All goals can be described by the maximization of the expected cumulative reward: $G_t = \sum_{k=0}^T R_{t+k+1}$
	\item But as earlier rewards are more probable to happen you need to increase their perceived value. Therefore you need a factor $0 \leq \gamma < 1$.
	\item Large $\gamma$, Agent cares about long-term reward. Small $\gamma$, Agent cares more about short term reward.
	\item \textbf{Discounted Accumulative Rewards (return)}: $G_t = \sum_{k=0}^\infty \gamma^k R_{t+k+1},\ where\ \gamma \in [0,1)$ 
	\item \textbf{Episodic tasks}: starting point and an ending point (terminal state), this creates an episode.
	\item \textbf{Continuous Tasks}: Tasks that continue forever (no terminal state).
	\item Learning Methods: Collecting the rewards at the end of the episode for the feature (Monte-Carlo), or Estimate the rewards at each step (Temporal Difference Learning)
	\item \textbf{Monte-Carlo}: $V(S_t) \leftarrow V(S_t) + \alpha [G_t - V(S_t)]$. Left-Side: $V(S_t)$ Maximum expected Future, Right-Side: $V(S_t)$ Former estimation of maximum expected future. $\alpha$: learning rate.
	\item \textbf{TD-Learning}: $V(S_t) \leftarrow V(S_t) + \alpha [R_{t+1} + \gamma V(S_{t+1}) - V(S_t)]$. $R_{t+1} + \gamma V(S_{t+1})$ is the TD-Target. TD-Target is an estimation, by updating it via a one-step target.
	\item \textbf{Exploration/Exploitation Tradeoff}: Exploration (finding more information about the environment), Exploitation (using known information to maximize the reward). The Agent might find better rewards by doing exploration.
	\item \textbf{Value Based RL}: OPtimize the value function V(s), that tells us the maximum expected future reward.
	\begin{itemize}[noitemsep,nolistsep]
		\item The value of each state is the total amount of the reward an agent can expect to accumulate over the future, starting at that state.
		\item $v_\pi(s) = \mathbb{E}_\pi [\sum_{k=0}^\infty \gamma^k R_{t+k+1} | S_t = s]$. The Expected Reward given an State s.
		\item The agent takes the state with the biggest expected reward.
	\end{itemize} 
	\item \textbf{Policy Based}: optimize the policy function $a = \pi(s)$, without using the value function, a being the action to take, given a state.
	\begin{itemize}[noitemsep,nolistsep]
		\item The policy can either be deterministic, or stochastic $\pi(a|s) = \mathbb{P}[A=a|S=s]$(output is a distribution probebility over actions.)
		\item It directly indicates the best action to take for each step.
	\end{itemize} 
	\item \textbf{Model Based}: Model the environment. Each environment needs a different model foreach environment.
	\item Deep Reinforcement Learning: Uses deep neural networks to solve it.
\end{itemize}
\href{https://www.freecodecamp.org/news/diving-deeper-into-reinforcement-learning-with-q-learning-c18d0db58efe/}{Diving deeper into Reinforcement Learning with Q-Learning}
\begin{itemize}[noitemsep,nolistsep]
	\item \textbf{Q-learning} is value-based RL.
	\item \textbf{Q(Quality)-Table} gives you foreach action-state pair a value which moves gives the best maximum expected future reward.
	\item you don't implement a policy, you improve the Q-table to always choose the best action. The values in the table need to be learned.
	\item Action-Value Function (Q-Function) takes state and action as input and returns the expeced future reward.
	\item $Q^\pi(s_t,a_t) = \mathbb{E}\ [\sum_{k=0}^\infty \gamma^k R_{t+k+1} | s_t, a_t]$
	\item As we explore the environment, the Q-table will give us a better and better approximation by iteratively updating Q(s,a) using the \textbf{Bellman Equation}.
	\item Algorithm process: 1. Initialize Q-Table -> 2. Choose action a -> 3. perform action -> 4. measure reward -> 5. update Q -> goto 2.
	\begin{itemize}[noitemsep,nolistsep]
		\item 1. Initialize: e.g. initialize everything 0
		\item 2-3. choose an action. Use the epsilon greedy strategy. $ 0 \leq \epsilon \leq 1$ defines the exploration rate. It starts of with 1. We start of doing alot of random guesses what actions to choose (exploration). It is like a chance. We reduce the epsilon progressively to do more exploitation of the knowledge we gained.
		\item 4-5. update q: We update Q with the Bellman equation (given a new state s' and a reward r): $newQ(s,a) = Q(s,a) + \alpha[\Delta Q(s,a)],\ \Delta Q(s,a) = R(s,a) + \gamma \max(Q'(s',a')) - Q(s,a)$
		\item $\max(Q'(s',a'))$: Maxium expected future reward given the new s' and all possible actions at that new state. The highest Q-value between possible actions from the new state s'.
	\end{itemize} 
\end{itemize} 
\href{https://www.freecodecamp.org/news/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8/}{An introduction to Deep Q-Learning: let’s play Doom}
\begin{itemize}[noitemsep,nolistsep]
	\item Instead of using a \textbf{Q-table}, use a Neural Network that takes a state and \textbf{approximates Q-values} for each action based on that state.
	\item In a videogame states can be associated with frames. you need multiple state inputs (like 4).
	\item preprocessing is important to reduce the complexity of the states to reduce the computation time needed for training.
	\item \textbf{temporal limitation}: you need multiple frames to percept motion in the environment.
	\item using convolutional layers with ELU. Use fully connected layers with ELU and one output layer that produces the Q-value estimataion for each action.
	\item Making more efficient use of observed experience using experience Replay:
	\begin{itemize}[noitemsep,nolistsep]
		\item \textbf{Avoid forgetting previous experiences}: given that we use sequential samples from interactions with our environment, the network tends to forget the previous experiences. You could use previous experiences by learning it multiple times.
		\item reducing correlation between experiences: every action affects the next state, the sequence of experiences can be highly correlated. If we train in sequential order we might risk the agent bein influenced by it. Two strategies:
		\item stop learning while interacting with the environment. Play a little randomly to explore the state space. Then recall these experiences and learn from then, then play again with the updated value function.
		\item This way you have better set of examples. This prevents reinforcing the same action over and over.
	\end{itemize} 
	\item $\Delta w = \alpha [(R + \gamma\ max_a \hat{Q}(s',a,w)) - \hat{Q}(s,a,w)]\bigtriangledown_w \hat{Q}(s,a,w)$ 
	\item $\Delta w = \alpha * TD-Error * Gradient\ of\ our\ Prediction$
\end{itemize} 
\href{https://www.freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682/}{Improvements in Deep Q Learning: Dueling Double DQN, Prioritized Experience Replay, and fixed Q-targets}
\begin{itemize}[noitemsep,nolistsep]
	\item Fixed Q-targets:
	\begin{itemize}[noitemsep,nolistsep]
		\item We calculate \textbf{TD-Error} (aka the loss), but we don't have any idea of the real TD-target. Bellman equation states that the TD-Target is the reward of taking that action at that state plus the discounted highest Q-value for the next state.
		\item But we use the weights for the target and the Q-value and therefore our Q-value and our target value shifts.
		\item \textbf{Q-Targets}: Using a seperate network with a fixed parameter (w-tilde) for estimating the TD-Target. At every tau step, we copy the parameters from our DQN network to update the target network:
		\item $\Delta w = \alpha [(R + \gamma\ max_a \hat{Q}(s',a,\tilde{w})) - \hat{Q}(s,a,w)]\bigtriangledown_w \hat{Q}(s,a,w),\ At\ every \tau step: \tilde{w} \leftarrow w$
	\end{itemize} 
	\item \textbf{Double DQN}: Handles the problem of the overestimation of Q-values.
	\begin{itemize}[noitemsep,nolistsep]
		\item TD-Target = Q-target = reward + discounted max-q.
		\item How are we sure the best action for the next state ist the action with the highest Q-value, it depends on what actions we tried and what neighbors we explored.
		\item In the beginning of the training the max-q value will obviously b noisy and can lead to false positives. Learning will be complicated.
		\item Solution: When computing q-target, use two networks to decouple the action selection from the target Q-value generation
		\item Use our DQN network to select what is the best action to take for the next state (the action with the highest Q-value). We use our target network to calculate the target Q-value of taking that action at the next state.
		\item $argmax_a Q(s',a) = DQN\ choose\ action\ for\ next\ state,\ Q(s',argmax_a Q(s',a)) = Target\ network\ calculates\ the\ qvalue.$
		\item $Q(s,a) = r(s,a) + \gamma Q(s',argmax_a Q(s',a))$
		\item this helps us reduce the overestimation of q values and helps us train faster and have more stable learning.
	\end{itemize} 
	\item \textbf{Dueling DQN (aka DDQN)}: Seperate the estimator into two parts:
	\begin{itemize}[noitemsep,nolistsep]
		\item Q(s,a) can be decomposed as the sum of: V(s): the value of being at that state. A(s,a): the advantage of taking that action at that state (how much better it is to all other actions).
		\item With DDQN, we seperate the estimator using two streams one for V(s) and one for A(s,a) and then combine these two streams through a special aggregation layer to get an estimate of Q(s,a). Two streams in the NN.
		\item By decoupling the estimation we can learn which states are valuable without having to learn the effect of each action at each state.
		\item Being able to calculate V(s) can be useful for state where their actions do not affect the environment in a relevant way.
		\item Aggregation: Simply adding both streams will be problemantic for the back propagation, you can force the advantage function estimator to have 0 advantag at the chosen action. To do that, we subtract the average advantage of all actions possible of the state.
		\item $Q(s,a;\theta,\alpha,\beta) = V(s;\theta,\beta) + (A(s,a;\theta,\alpha) - \frac{1}{\mathcal{A}}\sum_a' A(s,a';\theta,\alpha))$
		\item $\theta: common\ network\ parameters,\ \alpha: advantage\ stream\ parameters,\ \beta: value\ stream\ parameters,\ the\ sum\ is\ the\ average.$
		\item This helps us accelerate the training. This helps us find much more relaible Q-values for each action by decoupling the estimation between two streams.
	\end{itemize} 
	\item \textbf{Prioritized Experience Replay}: Some experiences may be more important than others for our training, but might occur less frequently.
	\begin{itemize}[noitemsep,nolistsep]
		\item If we sample the experiences randomly these rich experiences that occur rarely have practilly no chance to be selected.
		\item Use a priority. where there is a big difference between our prediction and the TD target, since it means that we have a lot to learn about it.
		\item We use the absolute value of the magnitude of our TD-error: $p_t = |\delta_t| + e$,\ e = const, that assures that no experience has no 0 probability.
		\item Put that priority in the experience of each replay buffer to select the experiences.
		\item Do not go greedy prioritization: overfitting!. Stochastic prioritization: $P(i) = \frac{p_i^a}{\sum_k p_k^a}$,\ a reintroduces some randomness, a = 0 pure uniform randomness, a = 1 only select the experiences with the highest priorities.
		\item To combat over-fitting by prioritization of high-priority samples use Importance sampling weights (IS): $(\frac{1}{N} * \frac{1}{P(i)})^b$,\  b = controls how much the w affects learning. Close to 0 at the beginning of learning and annealed up to 1 over the duration of training. Because these weights are more important in the end of learning when our q-values begin to converge.
		\item To sort the replays use an unsorted sumtree
	\end{itemize} 
\end{itemize} 
\href{https://www.freecodecamp.org/news/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f/}{An introduction to Policy Gradients with Cartpole and Doom}
\begin{itemize}[noitemsep,nolistsep]
	\item in policy-based methods we directly learn the policy function that maps state to action. we directly parameterize $\pi$
	\item Deterministic policies are used in deterministic environments. stochastic policy is used when the environment is uncertain. We call this process a Partially Observable Markov Decision Process (POMDP).
	\item \textbf{Advantage of Policy Gradients}:
	\begin{itemize}[noitemsep,nolistsep]
		\item \textbf{convergence}: policy-based methods have better convergence properties. value-based methods might oscillate alot. Policy based methods follow gradients we converge on a local maximum (worst case), or global maximum (best case).
		\item Policy gradient are more effective in \textbf{high dimensional action spaces}: as Deep Q-learning is that their prediction assign a score for each eaction at each time step, given the current state.
		\item Policy gradients \textbf{can learn stochastic policies}: value functions can't. In Policy we donÄt need to implement an exploration/explotation trade off. 
	\end{itemize}
	\item \textbf{Disadvantages of Policy Gradients}:
	\begin{itemize}[noitemsep,nolistsep]
		\item Alot of the time, they converge on a \textbf{local maximum} rather than on the global optimum.
		\item \textbf{Slower convergence}: Then Deep Q-Learning.
	\end{itemize}
	\item \textbf{Policy Search}: We have our policy $\pi$ that has a parameter $\theta$. THis pi outputs a probability distribution of actions.
	\begin{itemize}[noitemsep,nolistsep]
		\item $\pi_\theta (a|s) = P[a|s]$
		\item Good policy: theta that maximizes the score function: $J(\theta) = E_{\pi \theta} [\sum \gamma r]$
		\item \textbf{Steps}: 1st: Measure the quality of policy with a policy score function, 2nd: use policy gradient ascent to find best parameter theta that improves our policy.
		\item \textbf{1st Step}: The Policy Score function J(theta):
		\begin{itemize}[noitemsep,nolistsep]
			\item Episodic environment: Calculate the mean of the return from the first time Step (G1): $J_1(\theta) = E_\pi[G_1 = \sum_{k=0}^\infty \gamma^k R_{1 + k}] = E_\pi (V(s_1))$. We want a policy that optimizes G1, as this will be the best policy.
			\item Continious Environment: We can use the average value, because we can't rely on a specific start state and their values are now weighted by the probability of the occurrence of the respected state: $J_{avgv}(\theta) = E_\pi(V(s)) = \sum d(s)V(s),\ where\ d(s) = \frac{N(s)}{\sum_s'N(s')}$
			\item N(s) = Number of occurrences of the state.
			\item use the average reward per timestap: $J_{avR}(\theta) = E_\pi(r) = \sum_s d(s) \sum_a \pi_\theta(s,a) R_s^a$. sum over a: Probability that I take this action a from that state under this policy, Rsa: immediate reward that I get.
		\end{itemize}
		\item \textbf{2nd Step}: Policy gradient ascent.
		\begin{itemize}[noitemsep,nolistsep]
			\item To maximize the score function J(theta), we need to do gradient ascent on policy parameters.
			\item We use gradient ascent as the score function is not an error function (there we would use gradient descent.)
			\item Goal: $\theta^* = \underset{\theta}{argmax}E_{\pi \theta}[\sum_t R(s_t,a_t)]$, Score function: $J(\theta) = E_\pi[R(\tau)]$
			\item Problem: How do we estimate the Gradient with respect to theta, when the gradient depends on the unknown effect of policy changes on the state distribution?
			\item Solution: $\bigtriangledown_\theta J(\theta) = E_\pi [\bigtriangledown_\theta(log \pi (\tau|\theta))R(\tau)], \pi (\tau|\theta): policy\ function, R(\tau): score\ function$
			\item Update Rule: $\Delta \theta = \alpha * \bigtriangledown_\theta(log \pi (s, a, \theta))R(\tau)$
			\item R(tau): High value: it means thatn on average we took actions that lead to high rewards. If it is low, we want to push down the probabilities of the actions seen.
		\end{itemize}
		\item Policy gradient can be improved with Proximal Policy Gradients (ensure that the deviations from the previous policy stays relatively small) and Actor Critic (a hybrid between value-based algorithms and policy-based algorithms).
	\end{itemize}
\end{itemize} 
\href{https://www.freecodecamp.org/news/an-intro-to-advantage-actor-critic-methods-lets-play-sonic-the-hedgehog-86d6240171d/}{An intro to Advantage Actor Critic methods: let’s play Sonic the Hedgehog!}
\begin{itemize}[noitemsep,nolistsep]
	\item \textbf{Actor Critic}: Hybrid method. Use two neural networks: A Critic that measures how good the action takesn is (value-based) and an Actor that controls how our agent behaves (policy-based).
	\item State of the art: \textbf{Proximal Policy Optimization (PPO)}, is based on Advantage Actor Critic.
	\item \textbf{Policy Gradient Problem}: Reward is done for-each episode, so small bad decisions will be averaged out. And we won't find an optimal policy.
	\item Use TD-Learning: $\Delta \theta = \alpha * \bigtriangledown_\theta * (log \pi(S_t,A_t,\theta)) * Q(S_t,A_t)$. We do update each step sou we don't use the total rewards R(t). The Critic model approximates the value function.
	\item The critic will help to find the policy and update their own way to provide better feedback.
	\item Actor: $\pi(s,a,\theta)$  Critic: $\hat{q}(s,a,w)$
	\item Weights: Policy: $\Delta \theta = \alpha \bigtriangledown_\theta(log \pi_\theta(s,a)) * \hat{q}_w(s,a)$, Value: $\Delta w = \beta (R(s,a) + \gamma \hat{q}_w(s_{t+1},a_{t+1}) - \hat{q}_w((s_t,a_t))\ \bigtriangledown_w \hat{q}_w(s_t,a_t)$
	\item \textbf{Process}: At each time-step: current State St into Actor and Critic. Policy outputs Action At and receives a new State and a reward.
	\item The Critic computes the value of taking that action at that state and the actor updates is policy parameters (weights) using this q-value.
	\item To reduce the Variability: Use Advantage function: $A(s,a) = Q(s,a) - V(s)$ Q(s,a): q-value for action a in state s, V(s): average value of that state.
	\item This function calculates the extra reward I get if I take this action. A(s,a) > 0:  our gradient is pushed in that direction, A(s,a) < 0: our gradient is pushed in the opposite direction.
	\item Use the TD-Error as an good estimator: $A(s,a) = r + \gamma V(s') - V(s)$
	\item Strategies: Synchronous: \textbf{A2C} (Advantage Actor Critic), Asynchronous: \textbf{A3C} (Asynchronous Advantage Actor Critic).
	\item A3C uses different agents in parallel on multiple instances of the environment. Each worker will update the global network asynchronously.
	\item Problem of A3C: \href{https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html#a2c}{Link}. Because of asynchronous nature of A3C, some workers will be playing with older version of the parameters, thus the aggregating update will not be optimal. In A2C it waits for each actor to finish before updating the global parameters. Therefore the training will be more cohesive and faster.
	\item Each worker in A2C will ahve the same set of weights since, contrary to A3C, A2C updates all their workers at the same time. YOu can create multiple versions of environments and then execute them in parallel.
\end{itemize}
\href{https://towardsdatascience.com/proximal-policy-optimization-ppo-with-sonic-the-hedgehog-2-and-3-c9c21dbed5e}{Proximal Policy Optimization (PPO) with Sonic the Hedgehog 2 and 3}

\subsection{!RL Lectures from Deepmind}
\href{https://www.davidsilver.uk/teaching/}{RL Course by DeepMind}
\\
\href{https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ}{RL Course by DeepMind - Part 1: Introduction}
\begin{itemize}[noitemsep,nolistsep]
	\item Actions may have long term consequences and rewards may be delayed. May need to sacrifice immediate reward to gain more long-term reward.
	\item $Observation\ O_t, Reward\ R_t, Action\ A_t, History\ H_t\ (sequence\ of\ O_t,\ A_t,\ R_t)$
	\item $State\ S_t$ (simpler information to determine what happens next, usually function of history: $S_t = f(H_t)$
	\item State Definitions:
	\begin{itemize}[noitemsep,nolistsep]
		\item enviroment state $S_t^e$ is the enviroments private representation. Environment state not visible to the agent.
		\item agent state $S_t^a$ is the agents internal representation. Used to pick next action. $S_t^a = f(H_t)$
		\item markov (property) state $A\ state\ S_t\ is\ Markov\ iff:\ \mathbb{P}[S_{t+1} | S_t] = \mathbb{P}[S_{t+1}|S_1,...,S_t]$ . You only need the current state to infer the next state or the future. A helicopter state needs velocity. Otherwise you need the complete history to calculate velocity if it only stored position.
		\item environment state $S_t^e$ and the history $H_t$ is Markov.
	\end{itemize} 
	\item Environments:
	\begin{itemize}[noitemsep,nolistsep]
		\item fully observability: agent directly observes environment state $O_t = S_t^a = S_t^e$. This is a Markov decision process (MDP).
		\item partial observability: $S_t^a \neq S_t^e$. This is a partially observable Markov decision process (POMDP). Agent constructs it's own $S_t^a$.
		\item partial observability state: complete history $S_t^a = H_t$, beliefs: $S_t^a = (\mathbf{P}[S_t^e = s^1],...,\mathbf{P}[S_t^e = s^n])$, recurrent NN: $S_t^a = \sigma(S_{t-1}^a W_s + O_t W_o)$ (linear transformation)
	\end{itemize} 
	\item Inside an RL Agent
	\begin{itemize}[noitemsep,nolistsep]
		\item policy (agent's behavior), value function (how good is state-action pair), model (agents representation of the environment).
		\item model: predicts what the environment will do next. you don't need to do models.
		\item Transitions: $\mathcal{P}$ predicts next state (dynamics). Rewards $\mathcal{R}$ predicts next immediate reward
		\item e.g.: $\mathcal{P}_{ss'}^a = \mathbf{P}[S=s'| S=s, A=a],\ \mathcal{R}_s^a = \mathbf{E}[R|S=s, A=a]$
		\item model-free agent: Policy and/or Value Function and no model.
		\item model-based agent: Policy and/or Value Function and a model. first build the dynamics of the environmant es with the model
	\end{itemize} 
	\item Problems with RL
	\begin{itemize}[noitemsep,nolistsep]
		\item RL-Problem: Environment initially unknown and the agent learns by interaction.
		\item Planning-Problem: Environment-model is known from the start. 
		\item Prediction: evaluate the future (given a policy) vs. Control: optimise the future (find the pest policy)
	\end{itemize} 
\end{itemize} 
\href{https://www.youtube.com/watch?v=lfHX2hHRMVQ&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ&index=2}{RL Course by DeepMind - Part 2: Markov Decision Processes}
\begin{itemize}[noitemsep,nolistsep]
	\item Markov Processes:
	\begin{itemize}[noitemsep,nolistsep]
		\item Markov Decision Processes Describe the environment for RL and is fully observable.
		\item State Transition: $\mathcal{P}_{ss'} = \mathbb{P}[S_{t+1} = s' | S_t = s]$. 
		\item This allows a Matrix to be defined: $\begin{pmatrix} \mathcal{P}_{11} & ... & \mathcal{P}_{1n}\\  \vdots &  & \\  \mathcal{P}_{n1} & ... & \mathcal{P}_{nn} \end{pmatrix}$. Each Row sums up to 1
		\item Markov Process: tuple $\langle \mathcal{S,P} \rangle$. S is a (finite) set os states. and P is a state transition probability matrix.
	\end{itemize} 
	\item Markov Reward Processes:
	\begin{itemize}[noitemsep,nolistsep]
		\item A MRP is a Markov Processes with the additions: tuple $\langle \mathcal{S,P,R}, \gamma \rangle$.
		\item R is a reward function $\mathcal{R}_s = \mathbb{E}[R_{t+1}|S_t=s]$ and $\gamma \in [0,1]$ is a discount factor.
		\item $G_t$ is the total discounted reward form time-step t. Value function v(s) (see above).
		\item Bellam Equation: $v(s) = \mathbb{E}[G_t | S_t = s] = \mathbb{E}[R_{t+1} + \gamma G_{t+1} | S_t = s] = \mathbb{E}[R_{t+1} + \gamma v(S_{t+1}) | S_t = s]$
		\item This allows: $v(s) = \mathcal{R}_s + \gamma \sum_{s' \in S} \mathcal{P}_{ss'}v(s')$
		\item In Matrix form: $\begin{bmatrix} v(1)\\ \vdots \\ v(n)\end{bmatrix} = \begin{bmatrix} \mathcal{R}_{1}\\ \vdots \\ \mathcal{R}_{n}\end{bmatrix} + \gamma \begin{bmatrix} \mathcal{P}_{11} & ... & \mathcal{P}_{1n}\\  \vdots &  & \\  \mathcal{P}_{n1} & ... & \mathcal{P}_{nn} 
		\end{bmatrix} \cdot \begin{bmatrix} v(1)\\ \vdots \\ v(n)\end{bmatrix}$. 
	\end{itemize}
	\item Markov Decisions Processes:
	\begin{itemize}[noitemsep,nolistsep]
		\item A MDP Is a Markov reward Process with a finite set of actions. The State Transition and reward function now also depend on the action chosen.
		\item stochastic policy: $\pi(a|s) = \mathbb{P}[A_t=a|S_t=s]$. They depend only on the current state. Policies are stationary (time-independent).
		\item The state sequence given by any policy is itself a markov process (chain) $\langle \mathcal{S,P}^\pi \rangle$. If we add the rewards we got through this policy induced sequence we get a MRP $\langle \mathcal{S,P^\pi,R^\pi,}\gamma\rangle$.
		\item So: $\mathcal{P}_{s,s'}^\pi = \sum_{a \in \mathcal{A}} \pi(a|s) \mathcal{P}_{s,s'}^a$ and $\mathcal{R}_{s}^\pi = \sum_{a \in \mathcal{A}} \pi(a|s) \mathcal{R}_{s}^a$
		\item So the transition dynamics and rewards are averaged over what our policy gives us.
		\item state-value function $v_\pi(s) = \mathbb{E}_\pi[G_t | S_t = s]$
		\item action-value function $q_\pi(s,a) = \mathbb{E}_\pi [G_t | S_t = s, A_t = a]$.
		\item bellman equation for state-value functions: $v_\pi(s) =  \mathbb{E}_\pi[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s]$
		\item bellman equation for action-value functions: $q_\pi(s,a) = \mathbb{E}_\pi [R_{t+1} + \gamma q_\pi(S_{t+1}, A_{t+1})  | S_t = s, A_t = a]$
		\item V-Step: $v_\pi(s) =  \sum_{a \in \mathcal{A}} \pi(a|s)q_\pi(s,a)$. For a given state we average the actions we can take
		\item Q-Step: $q_\pi(s,a) = \mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a v_\pi(s)$. For a given state how good is it to do a given action we averate the situations we could go to.
		\item Equation for $v_\pi$: $v_\pi(s) = \sum_{a \in \mathcal{A}}(\mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a v_\pi(s))$. state-value relates to the state-value of the next step.
		\item Equation for $q_\pi$: $p_\pi(s,a) = \mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a \sum_{a \in \mathcal{A}} \pi(a|s)q_\pi(s,a)$. q-value relates to the q-value of the next step.
		\item Bellman Expectation Equation in Matrix form: $v_\pi = \mathcal{R}^\pi + \gamma \mathcal{P}^\pi v_\pi$. Direct Solution $v_\pi = (I - \gamma \mathcal{P}^\pi)^{-1}\mathcal{R}^\pi$. $\mathcal{P}^\pi$ and $\mathcal{R}^\pi$ are averages.
		\item Optimality 
		\begin{itemize}[noitemsep,nolistsep]
			\item optimal state-value function $v_*(s) = \underset{\pi}{max}\ v_\pi(s)$. optimal action-value function $q_*(s,a) = \underset{\pi}{max}\ q_\pi(s,a)$
			\item discounted reward does not have the problem of infinite loops of positive rewards.
			\item un-discounted rewards need to fulfill either an average reward (average reward RL) or certain technical conditions must be met so un-discounted MDP are guaranteed to terminate. (well defined).
			\item partial ordering over policies: $\pi \geq \pi'\ if\ v_\pi(s) \geq v_\pi'(s), \forall s$
			\item Optimal Policy: For any MDP there exists at least one an optimal policy thate is better or erqual to all other policies: $\pi_* \geq \pi, \forall \pi$
			\item All optimal policies achieve the optimal value and optimal action-value function.
			\item optimal policy through optimal q-function $\pi_*(a|s) = \begin{cases} 1 & if\ a = \underset{a \in \mathcal{A}}{argmax}\ q_*(s,a) \\  0 & otherwise \end{cases}$.
			\item There is always a deterministic optimal policy for any MDP.
			\item Bellman Optimality Equation for v*. $v_*(s) = \underset{a}{max}\ \mathcal{R}_s^a + \gamma \sum_{s' \in S} \mathcal{P}_{ss'}^a v_*(s')$. This is a 1-step look ahead.
			\item Bellman Optimality Equation for q*. $q_*(s,a) = \mathcal{R}_s^a + \gamma \sum_{s' \in S} \mathcal{P}_{ss'}^a \underset{a'}{max}\ q_*(s',a')$.
			\item Bellman Optimality Equation is non-linear. No closed form solution (in general). Needs iterative solution: value-iteration, policy-iteration, Q-learning, Sarsa.
		\end{itemize}
	\end{itemize}
	\item Infinite and continuous MDPs
	\begin{itemize}[noitemsep,nolistsep]
		\item countably infinite state and/or action spaces: straightforward
		\item continuous state and/or action spaces: Closed form for linear quadratic model (LQR)
		\item continuous time: Requires partial differntial equations (Hamilton-Jacobi-Bellman).
	\end{itemize}
	\item Partially observable MDPs
	\begin{itemize}[noitemsep,nolistsep]
		\item A POMDP is a tuple $\langle \mathcal{S,A,O,P,R,Z},\gamma \rangle$ with hidden states. Hidden Markov Model with actions.
		\item $\mathcal{O}$ is a finite set of observations.
		\item $\mathcal{Z}$ is an observation function, $\mathcal{Z}_{s'o}^a = \mathbb{P}[O_{t+1} = 0 | S_{t+1} = s', A_t = a]$
		\item History changes $H_t = A_0,O_1,R_1,...,A_{t-1},O_t,R_t$
		\item Beilief state is a probability over states conditioned on the history $b(h) = (\mathbb{P}[S_t = s^1 | H_t = h],...,\mathbb{P}[S_t = s^n | H_t = h])$
		\item A POMDP can be reduced to an infinite history tree or an infinite belief state tree.
	\end{itemize}
	\item Undiscounted, average reward MDPs
	\begin{itemize}[noitemsep,nolistsep]
		\item Ergodic Markov Process: Is recurrent (each state is visited an infinite number of times) and Aperiodic (each state is visited without any systematic period)
		\item An ergodic Markov process has a limiting stationary distribution $d^\pi(s) = \sum_{s' \in S}d^\pi(s')\mathcal{P}_{s's}$ 
		\item An MDP is ergodic if the Markov chain induced by any policy is ergodic.
		\item Average reward per time-step $\rho^\pi = \underset{T \rightarrow \infty}{\lim} \frac{1}{T} \mathbb{E}[\sum_{t=1}^T R_t] $
		\item extra reward due to starting from satte s: $\tilde{v}_\pi(s) = \mathbb{E}_\pi [\sum_{k=1}^\infty (R_{t+k} - \rho^\pi) | S_t = s]$
	\end{itemize}
\end{itemize} 
\href{https://www.youtube.com/watch?v=Nd1-UUMVfz4}{RL Course by DeepMind - Part 3: Planning by Dynamic Programming}
\begin{itemize}[noitemsep,nolistsep]
	\item Introduction:
	\begin{itemize}[noitemsep,nolistsep]
		\item \textbf{Dynamic} sequential or tomporal component to the problem.
		\item \textbf{Programming} optimising a "programm", i.e. policy
		\item can be used when problems can be divided into subproblems they can be solved individually and the result can be combined again.
		\item property: Optimal substructure, Principle of optimality applies, Optimum by divide-solve-combine. Optimum of the pieces tell you about optimum of your problem.
		\item property: Overlapping subproblems. They occur multiple times. Can cache and reuse Solutions.
		\item MDP satisfy both these properties because of the bellman equation and the value function that stores and reuses solutions.
		\item Dynamic Programming can be used for planning in an MDP. Planning: We already learned everything, now we need to solve the problem.
		\item Plan to solve prediction problem: Given MDP and policy, output: value function of the policy.
		\item Plan to solve control problem: Given MDP, output: optimal value function and therefore policy.
	\end{itemize}
	\item Policy Evaluation:
	\begin{itemize}[noitemsep,nolistsep]
		\item Iterative Policy Evaulation by using Bellman expectation backup. $v_1 (0 = no\ reward\ anywhere) \rightarrow v_2 \rightarrow ... \rightarrow v_\pi$.
		\item Synchronous backup: $At\ each\ iter\ k+1\ \forall s \in S$: Update $v_{k+1}(s)\ from\ v_k(s')$. s' is sucessor state of s.
		\item For each state make a 1step look-ahead with the bellman equation that uses the current value function as input. The result is the value for this state for the next value function. Then go over each state to have all the values for the next value function.
		\item Asynchronous Backup later.
		\item This converges to the best value function (proven later).
		\item $v_{k+1}(s) = \sum_{a \in \mathcal{A}}\pi(a|s)(\mathcal{R}_s^a + \gamma \sum_{s' \in S}\mathcal{P}_{ss'}^a v_k(s'))$. $\mathbf{v^{k+1} = \mathcal{R}^\pi + \gamma \mathcal{P}^\pi + v^k}$.
	\end{itemize}
	\item Policy Iteration:
	\begin{itemize}[noitemsep,nolistsep]
		\item start of with arbitray value function. It doesn't matter where you start you will always end with the optimal policy as an MDP always has atleast one.
		\item Improve Policy: Step 1: \textbf{Evaluate} a given policy: $v_\pi(s) = \mathbb{E}[R_{t+1} + \gamma R_{t+2}+ ... | S_t = s]$. Create value function of that policy. This needs multiple iterations of the bellman expectation backup.
		\item Improve Policy: Step 2: \textbf{Improve} the policy by acting greedily with respect to value function: $\pi' = greedy(v_\pi)$
		\item In general you need to iterate between these two steps. policy iteration always converges to $\pi^*$.
		\item acting greedily always makes the policy deterministic. Acting Greedily $\pi'(s) = \underset{a \in \mathcal{A}}{argmax} q_\pi (s,a)$
		\item the total reward if we acted greedily is at least as much as before we greedified it. $v_\pi(s) \leq v_{\pi'}(s)$.
		\item When improvements stops you have satisfied the bellman optimality equation. Therefore the policy we end up with is $v_*(s)$ and is optimal.
		\item Modified Policy Iteration:
		\begin{itemize}[noitemsep,nolistsep]
			\item you may not need to iterate until the value function is fixed as a crude approximation would already lead to the same greedy policy as the one you get after the value function is fixed. you may be able to save iterations.
			\item with an $\epsilon$-convergenc of value function or early stopping after k iterations. Both still converge on the optimal policy.
			\item Why not update policy every iteration (k = 1): this is equivalent to value iteration (next section).
		\end{itemize}
	\end{itemize}
	\item Value Iteration:
	\begin{itemize}[noitemsep,nolistsep]
		\item if my first action i choose is optimal and the policy i use after that is optimal, then the policy is optimal.
		\item Principle of Optimality: A policy $\pi(a|s)$ achieves the optimal value from state s, $v_\pi(s) = v_*(s)$ iff: For any state $s'$ reachable from s $\pi$ achieves the optimal value from state $s',\ v_\pi(s') = v_*(s')$.
		\item using the bellman optimality equation:
		\item If we know the solution to subproblems $v_*(s')$. Then solution $v_*(s)$ an be found by one-step lookahead: $v_*(s) \leftarrow \underset{a \in \mathcal{A}}{max}\ \mathcal{R}_s^a + \gamma \sum_{s' \in S} \mathcal{P}_{ss'}^a v_*(s')$
		\item value iteration: apply these updates iteratively. The values propagate through the states and we end up with the optimal value function.
		\item Intuition: start with final rewards (one step before goal) and work backwards. We use this here through our entire statespace.
		\item Iterative Value Iteration by using Bellman optimality backup. $v_1 (0 = no\ reward\ anywhere) \rightarrow v_2 \rightarrow ... \rightarrow v_*$.
		\item Synchronous backup: $At\ each\ iter\ k+1\ \forall s \in S$: Update $v_{k+1}(s)\ from\ v_k(s')$. s' is sucessor state of s.
		\item Intermediate values functions in this iterative process may not correspond to any valid policy.
		\item $v_{k+1}(s) = \underset{a \in \mathcal{A}}{max}\ (\mathcal{R}_s^a + \gamma \sum_{s' \in S}\mathcal{P}_{ss'}^a v_k(s'))$. $\mathbf{v^{k+1} = \underset{a \in \mathcal{A}}{max}\ (\mathcal{R}^a + \gamma \mathcal{P}^a + v_k})$.
		\item Prediction-Problem: Use Bellman Expectation Equation in the Iterative Policy Evaluation Algorithm.
		\item Control-Problem: Use Bellman Expectation Equation and Greedy Policy Improvement in the Policy Iteration Algorithm.
		\item Control-Problem: Use Bellman Optimality Equation in the Value Iteration Algorithm.
		\item Complexity for $v_\pi(s)$, $\mathcal{O}(mn^2)$. Complexity for $q_\pi(s,a)$, $\mathcal{O}(m^2n^2)$. For m actions and n states.
	\end{itemize}
	\item Extensions to Dynamic Programming:
	\begin{itemize}[noitemsep,nolistsep]
		\item Asynchronous Dynamic Programming:
		\begin{itemize}[noitemsep,nolistsep]
			\item Once you created the new value for the new value function you can use this for the other states instead of having to do all states on the old values first. Can reduce computation. If you slected all states at least sometimes you are guaranteed to converge.
			\item \textbf{In-Place Dynamic Programming}: Synchronous: you have two copies of the value function (new and old) in in-place you overwrite the old values in your one copy of the value function.
			\item \textbf{Prioritised Sweeping}: Some measure how important how it is to update a state with the Bellman error $|\underset{a \in \mathcal{A}}{max}\ (\mathcal{R}_s^a + \gamma \sum_{s' \in S}\mathcal{P}_{ss'}^a v(s')) - v(s)|$
			\item this uses a priority queue. Backup the state with the largest remaining Bellman error.
			\item \textbf{Real-Time Dynamic Programming}: only select the states that the agent actually visits. Collect real samples and look which states to update.
		\end{itemize}
		\item Full-width and sample backups:
		\begin{itemize}[noitemsep,nolistsep]
			\item DP uses full-width backups: Every step we consider all possible branching factor (considering all actions and all states). For that we need to know the dynamics.
			\item So we use just sample particular trajectory.
			\item Use Sample backups. Using sample rewards and sample transitions instead of complete reward function and transition dynamics.
			\item Advantages: Model-free, Breaks curse of dimensionality, Cost of backup is constant.
		\end{itemize}
		\item Approximate Dynamic Programming:
		\begin{itemize}[noitemsep,nolistsep]
			\item ???
		\end{itemize}
	\end{itemize}
	\item Contraction Mapping:
	\begin{itemize}[noitemsep,nolistsep]
		\item ??? Shows that they convert to the optimal. the solution is unique
	\end{itemize}
\end{itemize}
\href{https://www.youtube.com/watch?v=PnHCvfgC_ZA}{RL Course by DeepMind - Part 4: Model-Free Prediction}
\begin{itemize}[noitemsep,nolistsep]
	\item Monte-Carlo Learning
	\begin{itemize}[noitemsep,nolistsep]
		\item Monte Carlo (MC) learns from complete episodes (no bootsraping) and is model free.
		\item \textbf{First-Visit Monte-Carlo Policy Evaluation}:
		\begin{itemize}[noitemsep,nolistsep]
			\item The first time-step t that state s is visited in this particular episode (might be visited more than once).
			\item Increment counter $N(s) \leftarrow N(s) + 1$, and total return $S(s) \leftarrow S(s) + G_t$, Estimation $V(s) = S(s)/N(s)$.
			\item Law of large numbers: $V(s) \rightarrow v_\pi(s)\ as\ N(s) \rightarrow \infty$
		\end{itemize}
		\item \textbf{Every-Visit Monte-Carlo Policy Evaluation}:
		\begin{itemize}[noitemsep,nolistsep]
			\item every time-step t that state s is visited in this particular episode (might be visited more than once).
			\item Increment counter $N(s) \leftarrow N(s) + 1$, and total return $S(s) \leftarrow S(s) + G_t$, Estimation $V(s) = S(s)/N(s)$.
			\item Law of large numbers: $V(s) \rightarrow v_\pi(s)\ as\ N(s) \rightarrow \infty$
		\end{itemize}
		\item Incremental Mean with sequence of means ($\mu_1,\mu_2,...$)  $\mu_k = \frac{1}{k}\sum_{j=1}^k x_j = \mu_{k-1} + \frac{1}{k}(x_k - \mu_{k-1})$
		\item \textbf{Incremental Monte-Carlo Updates}: Counter is incremented the same, but $V(S_t) \leftarrow V(S_t) + \frac{1}{N(S_t)} (G_t - V(S_t))$.
		\item non-stationary Problem, forget old episodes: $V(S_t) \leftarrow V(S_t) + \alpha (G_t - V(S_t))$
	\end{itemize}
	\item Temporal-Difference Learning
	\begin{itemize}[noitemsep,nolistsep]
		\item Temporal-Difference Learning (TD) learns from incomplete episodes (bootstrapping) and is model free.
		\item TD(0): Update by one step do not use the actual return but the estimated return $R_{t+1} + \gamma V(S_{t+1})$ which is called the TD Target.
		\item $V(S_t) \leftarrow V(S_t) + \alpha (R_{t+1} + \gamma V(S_{t+1}) - V(S_t))$
		\item TD error: $\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t)$
		\item TD can learn before knowing the final outcome (every step, MC must wait until end of episode).
		\item TD can learn without the final outcome from incomplete sequences and therefore works in continuing (non-terminating) environments (MC only for episodic).
	\end{itemize}
	\item TD vs MC
		\begin{itemize}[noitemsep,nolistsep]
		\item Return $G_t$ is unbiased. TD Target is a biased estimate of $v_\pi (S_t)$
		\item TD has low variance, some bias, MC has high variance, zero bias.  the State is noisy but the bias from V is not equal to $v_\pi$.
		\item MC also works for POMDP but TD(0) does not. But it's estimates with MC might not be good. 
		\item MC also updates the intermediate values not only the state we start with.
		\item MC and TD converge $V(s) \rightarrow v_\pi(s)$ as experience $\rightarrow \infty$
		\item MC converges to solution with minimum mean-squared error $\sum_{k=1}^K \sum_{t=1}^{T_k} (G_t^k - V(s_t^k))^2$ and exploits the Markov property (usually more effective there).
		\item TD(0) converges to solution of max lilekihood markov model and does not exploits the Markov property (usually more effective in non-Markov).
		\item MC: One full trajectory (that terminates), TD-0: One Step along the way. Dynamic Programming: One-Step Lookahead to compute the full expectation where you need to know the dynamics for the environment.
		\item Bootstrapping: update involves an estimate, Sampling: update samples an expectation.
	\end{itemize}
	\item $TD(\lambda)$
	\begin{itemize}[noitemsep,nolistsep]
		\item Let TD target look n steps into the future. $n = 1$ is TD(0), $n = \infty$ is MC (or n is termination).
		\item n-step return: $G_t^{(n)} = \sum_{k = 1}^{n} \gamma^{k-1} R_{t+k} + \gamma^nV(S_{t+n})$
		\item n-step TD-learning $V(S_t) \leftarrow V(S_t) + \alpha(G_t^{(n)} - V(S_t))$
		\item Forward-view:
		\begin{itemize}[noitemsep,nolistsep]
			\item Forward View of TD($\gamma$): Using weights to average the n-step returns $(1-\gamma)\gamma^{n-1}$.
			\item Forward Return: $G_t^\lambda = (1-\lambda)\sum_{n=1}^\infty \lambda^{n-1}G_t^{(n)}$.
			\item Forward-view TD($\gamma$): $V(S_t) \leftarrow V(S_t) + \alpha (G_t^gamma - V(S_t))$.
			\item Update value function towards the $\lambda$-return. It looks into the future.
		\end{itemize}
		\item Backward-view:
		\begin{itemize}[noitemsep,nolistsep]
			\item online (backward) update: do updates immediately. offline updates: Do the update at the end of the episode.
			\item Eligibility Traces: Combines recency and frequency of events as the cause of states $E_0(s) = 0,\ E_t(s) = \gamma \lambda E_{t-1}(s) + \textbf{1}(S_t = s)$
			\item TD-Error $\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t)$
			\item update: $V(S) \leftarrow V(s) + \alpha \delta_t E_t(s)$
		\end{itemize}
		\item TD-0 is TD(lambda) with lambda = 0. TD(1) is similar to every-visit MC.
	\end{itemize}
\end{itemize}
\href{https://www.youtube.com/watch?v=0g4j2k_Ggc4}{RL Course by DeepMind - Part 5: Model-Free Control}
\begin{itemize}[noitemsep,nolistsep]
	\item Introduction
	\begin{itemize}[noitemsep,nolistsep]
		\item On-policy: Learn about policy $\pi$ from experience sampled from $\pi$. Off-policy: Learn about policy $\pi$ from experiences sampled from $\mu$.
	\end{itemize}
	\item On-Policy Monte-Carlo Control
	\begin{itemize}[noitemsep,nolistsep]
		\item Greedy MC policy evaluation: $\pi'(s) = \underset{a \in \mathcal{A}}{argmax}\ Q(s,a)$ 
		\item Policy Improvement with e-Greedy: probability $1 - \epsilon$ choose greedy, probability $\epsilon$ choose random.
		\item $\pi(a|s) = \begin{cases} \epsilon/m + 1 - \epsilon & if a^* = \underset{a \in \mathcal{A}}{argmax}\ Q(s,a) \\ \epsilon/m & otherwise \end{cases}$
		\item the stochasticity of the e-greedy policy improvement ensures that we at some reate et least explore everythings in the environment.
		\item greedy action selection for model is a problem as you might only explore the most immediate reward/greedy and get stuck on a local maximum. epsion-greedy: it is guaranteed that you make progress.
		\item after one episode you can already update the value function to something slightly better might aswell use this.
		\item GLIE Monte-Carlo Control:
		\begin{itemize}[noitemsep,nolistsep]
			\item Greedy in the Limit with Infinite Exploration (GLIE).
			\item All state-action pairs are explored infinitely many times.
			\item Then the policy converges on a greedy policy.
			\item epsilon-greedy is GLIE if epsislon is redecued to zero at $\epsilon_k = \frac{1}{k}$.
			\item kth Episode sampled from $\pi$.
			\item $\forall S_t\ and\ A_t: N(S_t,A_t) \leftarrow N(S_t,A_t) + 1,\ Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \frac{1}{N(S_t,A_t)}(G_t - Q(S_t,A_t))$
			\item GLIE Monte-Carlo Control converges to the optimal action-value function $Q(s,a) \rightarrow q_*(s,a)$
		\end{itemize}
	\end{itemize}
	\item On-Policy Temporal-Difference Learning
	\begin{itemize}[noitemsep,nolistsep]
		\item TD has the following advantages over MC: Lower variance, online, imcomplete sequences.
		\item On-Policy Control with Sarsa: Policy Evaluation Sarsa ($\lambda$), $Q \approx q_\pi$, Policy improvement: $\epsilon$-greedy policy improvement.
		\item Sarsa converges to th eoptimal action-value function iff: GLIE sequence of policies $\pi_t(a|s)$ and for the step-sizes $\alpha: \sum_{t=1}^\infty \alpha_t = \infty,\ \sum_{t=1}^\infty \alpha_t^2 < \infty$
		\item n-Step Sarsa:
		\begin{itemize}[noitemsep,nolistsep]
			\item n-step Q-return $q_t^{(n)} = R_{t+1} + \gamma R_{t+2} + ... + \gamma^{n-1}R_{t+n} + \gamma^n Q(S_{t+n})$
			\item Update $Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha(q_t^{(n)} - Q(S_t,A_t))$
			\item Forward weight $q_t^\lambda = (1-\lambda)\sum_{n=1}^\infty \lambda^{n-1}q_t^{(n)}$
			\item Forward Sarsa $Q(S_t,A_t) = \leftarrow Q(S_t,A_t) + \alpha(q_t^\lambda - Q(S_t,A_t))$
			\item Eligibility Traces $E_0(s,a) = 0,\ E_t(s,a) = \gamma \lambda E_{t-1}(s,a) + \textbf{1}(S_t = s, A_t = a)$
			\item Backward Sarsa $\delta_t = R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t,A_t),
			\ Q(s,a) \leftarrow Q(s,a) + \alpha \delta_t E_t(s,a)$
		\end{itemize}
	\end{itemize}
	\item Off-Policy Learning
	\begin{itemize}[noitemsep,nolistsep]
		\item Evaluate target policy $\pi(a|s)$, while following behaviour policy $\mu(a|s)$
		\item Learn about optimal policy while following exploratory policy.
		\item Importance Sampling:
		\begin{itemize}[noitemsep,nolistsep]
			\item Estiamte the expectation of a different distribution $\mathbb{E}_{X \approx P}[f(X)] = \sum P(X)f(X) = \mathbb{E}_{X \approx Q}\lbrack \frac{P(X)}{Q(X)}f(X)\rbrack   $
			\item Weight return $G_t$ according to similiarity between policies
			\item Monte-Carlo $G_t^{\pi / \mu} = \frac{\pi(A_t|S_t)}{\mu(A_t|S_t)} \frac{\pi(A_{t+1}|S_{t+1})}{\mu(A_{t+1}|S_{t+1})} ... \frac{\pi(A_T|S_T)}{\mu(A_T|S_T)}$
			\item Update value towards corrected return $V(S_t) \leftarrow V(S_t) + \alpha (G_t^{\pi / \mu}- V(S_t))$
			\item TD $V(S_t) \leftarrow V(S_t) + \alpha (\frac{\pi(A_t|S_t)}{\mu(A_t|S_t)}(R_{t+1} + \gamma V(S_{t+1})) - V(S_t))$
		\end{itemize}
		\item Q-Learning control: Learning of action-values without importance sampling.
		\begin{itemize}[noitemsep,nolistsep]
			\item Next action is chosen using behaviour policy $A_{t+1} \sim \mu(\cdot |S_t)$ but consider an alternative $A' \sim \pi(\cdot|S_t)$
			\item target policy $\pi$ is greedy w.r.t. Q(s,a) $\pi(S_{t+1}) = \underset{a'}{argmax}\ Q(S_{t+1}, a')$
			\item the behaviour policy $\mu$ is e.g. $\epsilon$-greedy w.r.t Q(s,a)
			\item Update $Q(S,A) \leftarrow Q(S,A) + \alpha (R + \gamma \underset{a'}{max}\ Q(S',a') - Q(S,A))$
			\item Q-learning control converges to the optimal action-value function
		\end{itemize}
	\end{itemize}
\end{itemize}
\begin{center}
	\begin{tabular}{ m{9em} |c|c } 
	  & Full Backup (DP) & Sample Backup (TD) \\ 
	 \hline
	 Bellman Expectation Equation for $v_\pi(s)$ & Iterative Policy Evaluation & TD Learning \\ 
	 \hline
	 Bellman Expectation Equation for $q_\pi(s,a)$ & Q-policy Iteration & Sarsa \\ 
	 \hline
	 Bellman Expectation Equation for $q_*(s,a)$ & Q-value Iteration & Q-Learning \\ 
	 \hline
	\end{tabular}
\end{center}
\href{https://www.youtube.com/watch?v=UoPei5o4fps}{RL Course by DeepMind - Part 6: Value Function Approximation}
\begin{itemize}[noitemsep,nolistsep]
	\item Introduction
	\begin{itemize}[noitemsep,nolistsep]
		\item For Large scale MDPs a lookup table is not feasible and it is to slow to learn. Therefore use function approximation.
		\item Generalise from seen states to unseen states and Update parameter w using MC or TD learning.
		\item Types $Input: s,\ Output: \hat{v}(s,\mathbf{w})$, $Input: s, a\ Output: \hat{v}(s,a, \mathbf{w})$, $Input: s,\ Output: \hat{q}(s,a_1,\mathbf{w}),...,\hat{q}(s,a_m,\mathbf{w})$
	\end{itemize}
	\item Incremental Methods
	\begin{itemize}[noitemsep,nolistsep]
		\item Use gradient of a differentiable function $J(\mathbf{w})$ to adjust the weights $\Delta \mathbf{w} = -\frac{1}{2}\alpha \bigtriangledown_\mathbf{w} J(\mathbf{w})$.
		\item Goal: minimise mean-squared error between approximation $\hat{v}$ and true function $v_\pi$, $J(\mathbf{w}) = \mathbb{E}_\pi [(v_\pi(S) - \hat{v}(S,\mathbf{w}))^2]$
		\item Stochastic gradient descent uses samples $\Delta \mathbf{w} = \alpha (v_\pi(S) - \hat{v}(S,\mathbf{w})) \bigtriangledown_w \hat{v}(S,\mathbf{w})$
		\item Linear Function Approximation
		\begin{itemize}[noitemsep,nolistsep]
			\item Represent state by a feature vector $x(S) = \begin{pmatrix} x_1(S)\\ \vdots \\ x_n(S) \end{pmatrix}$
			\item value function by linear combination of features $\hat{v}(S,w) = x(S)^Tw = \sum_{j=1}^n x_j(S)w_j$
			\item Objective function $J(w) = \mathbb{E}_\pi [(v_\pi(S) - x(S)^Tw)^2]$
			\item Update rule $\bigtriangledown_w \hat{v}(S,w) = x(S),\ \Delta w = \alpha (v_\pi(S) - \hat{v}(S,w))x(S)$
			\item Table loolip for features $x^{table}(S) = \begin{pmatrix} \mathbf{1} (S = s_1)\\ \vdots \\ \mathbf{1} (S = s_n) \end{pmatrix}$
			\item Table function $\hat{v}(S,w) = \begin{pmatrix} \mathbf{1} (S = s_1)\\ \vdots \\ \mathbf{1} (S = s_n) \end{pmatrix} \cdot \begin{pmatrix} w_1\\ \vdots \\ w_n \end{pmatrix}$
		\end{itemize}
		\item Incremental Prediction Algorithm
		\begin{itemize}[noitemsep,nolistsep]
			\item As the target $v_\pi(s)$ is not given to us we need a supstitude, a target.
			\item MC-Target $v_\pi(s) = G_t$, TD(0)-Target $v_\pi(s) = TD-Target = R_{t+1} + \gamma \hat{v}(S_{t+1},w)$, TD(lambda) $v_\pi(s) = G_t^\lambda$
			\item MC: G-t is unbiased, but noisy. We get supervised learning to "training data" $\langle S_1,G_1 \rangle, ... \langle S_T,G_T \rangle$
			\item MC: converges to local optimum epen on non-linear functions.
			\item TD(0): TD-target is biased sample. We still get supervised learning similar to MC.
			\item TD(0): converges (close) to global optimum.
			\item TD(lambda): Better
		\end{itemize}
		\item Incremental Control Algorithm
		\begin{itemize}[noitemsep,nolistsep]
			\item Evaluation: Approximate policy evaluation, Improvement $\epsilon$-greedy policy improvement.
			\item Approximate action-value function $\hat{q}(S,A,w) \approx q_\pi(S,A)$
			\item Minimise Mean-Squared error $J(w) = \mathbb{E}_\pi [(q_\pi(S,A) - \hat{q}(S,A,w))^2]$
			\item Use Stochastic Gradient Descent: $\Delta w = \alpha(q_\pi(S,A) - \hat{q}(S,A,w)) \nabla_w \hat{q}(S,A,w)$
			\item Linear: use feature vector to represent action-value function by linear combination $x(S,A)$ (Feature Vector).
			\item Linear: Stochastic Gradient Descent: $\Delta w = \alpha(q_\pi(S,A) - \hat{q}(S,A,w)) x(S,A)$
			\item Targets: For MC: $G_t$, TD(0): $R_{t+1} + \gamma\hat{q}(S_{t+1},A_{t+1},w)$, Forward-TD(lambda): $q_t^\lambda$
		\end{itemize}
	\end{itemize}
	\item Batch Methods
	\begin{itemize}[noitemsep,nolistsep]
		\item Batch samples to find best fitting value function for our training data (batch).
		\item The Batch samples are the experiences so far.
		\item Least Squares Prediction
		\begin{itemize}[noitemsep,nolistsep]
			\item Given: Approximation $\hat{v}(s,w)$ and Oracle $v_\pi(s)$
			\item Dataset of state-value pairs: $\mathcal{D} = \{\langle s_1,v_1^\pi\rangle, \langle s_T,v_T^\pi\rangle\}$
			\item Least-squars minimizes sum-squared error between approximation and target oracle.
			\item $LS(w) = \sum_{t=1}^T(v_t^\pi - \hat{v}(s_t,w))^2 = \mathbb{E}_\mathcal{D} [(v^\pi - \hat{v}(s,w))^2]$
			\item This Dataset can be called experience replay
			\item Stoachstic Gradient Decent: 1st: Sample state,value from experience, 2nd: apply $\Delta w = \alpha (v^\pi - \hat{v}(s,w))\nabla_w \hat{v}(s,w)$
			\item Stochastic Gradient Descent converges to $w^\pi = \underset{w}{argmin}\ LS(w)$
			\item DQN uses experience replay and fixed Q-targets
		\end{itemize}
		\item Linear Least Squares Prediction.
		\begin{itemize}[noitemsep,nolistsep]
			\item In Linear cases the normal prediction takes longer than necessary, so the following approach solves it directly:
			\item $w = (\sum_{t=1}^T x(s_t)x(s_t)^T)^{-1} \sum_{t=1}^T x(s_t)v_t^\pi$
			\item Direct solution scales $O(N^3)$ for N features.
			\item In practice we do not know $v_t^\pi$, therefore we use Monte-Carlo, TD(0), or TD(lambda) as approximation.
		\end{itemize}
		\item Least Squares Policy Prediction
		\begin{itemize}[noitemsep,nolistsep]
			\item policy evaluation: use least squares Q-learning.
			\item q-learning Update: $\delta = R_{t+1} + \gamma \hat{q}(S_{t+1},\pi(S_{t+1}), w) - \hat{q}(S_t,A_t,w),\ \Delta w = \alpha \delta x(S_t,A_t)$
			\item LSTDQ algorithm: solve fot total update = 0
			\item $w = (\sum_{t=1}^T x(S_t,A_t)(x(S_t,A_t) - \gamma x(S_{t+1},\pi(S_{t+1})))^T)^{-1} \sum_{t=1}^T x(S_t,A_t)R_{t+1}$
		\end{itemize}
	\end{itemize}
\end{itemize}
\href{https://www.youtube.com/watch?v=KHZVXao4qXs}{RL Course by DeepMind - Part 7: Policy Gradient Methods}
\begin{itemize}[noitemsep,nolistsep]
	\item Introduction
	\begin{itemize}[noitemsep,nolistsep]
		\item Advantages of Policy-Based RL
		\begin{itemize}[noitemsep,nolistsep]
			\item Better convergence properties
			\item Effective in high-dimensional or continuous action spaces. Quality based methods need a max(). Policy-Based does not need that.
			\item Can learn stochastic policies. 
		\end{itemize}
		\item Disadvantages of Policy-Based RL
		\begin{itemize}[noitemsep,nolistsep]
			\item Typically converge to a local rather than global optimum.
			\item Evaluating a policy is typically inefficient and high variance.
		\end{itemize}
		\item State Aliasing: Two states that look the same from your feature-vector representation. This can happen when a MDP does not hold the markov property all the time.
		\item When State Aliasing occurs a stochastic policy can do better than a deterministic one.
		\item Policy Objective Functions
		\begin{itemize}[noitemsep,nolistsep]
			\item Goal: given policy $\pi_\theta (s,a)$ with parameters $\theta$, find best $\theta$. How do we measure quality of policy?
			\item $d^{\pi_\theta}(s)$ is stationary distribugion of Markov chain for $\pi_\theta$
			\item episodic environments: use start value: $J_1(\theta) = V^{\pi_\theta}(s_1) = \mathbb{E}_{\pi_\theta}[v_1]$
			\item continuing environments: use average value: $J_{avV}(\theta) = \sum_s d^{\pi_\theta}(s)V^{\pi_\theta}(s)$
			\item or: average reward per time-step $J_{avR}(\theta) = \sum_s d^{\pi_\theta}(s) \sum_a \pi_\theta(s,a)\mathcal{R}_s^a$
			\item or: discounted average: $\frac{1}{1-\gamma} J_{avV}(\theta)$
		\end{itemize}
		\item Policy based reinforcement learning is an optimisation problem. Find $\theta$ that maximises $J(\theta)$
		\item Approaches without gradient: Hill climbing, Simplex/ amoeba / Nelder Mead, Genetic algorithms.
		\item Usually greater efficiency with gradient: Gradient descent, conjugate gradient, quasi-newton
	\end{itemize}
	\item Finite Difference Policy Gradient
	\begin{itemize}[noitemsep,nolistsep]
		\item parameter update: $\Delta \theta = \alpha \nabla_\theta J(\theta)$, where $\alpha$ is step-size.
		\item policy gradient: $\nabla_\theta J(\theta) = \begin{pmatrix} \frac{\partial J(\theta)}{\partial \theta_1} \\ \vdots \\ \frac{\partial J(\theta)}{\partial \theta_n}\end{pmatrix}$
		\item Computing Gradients by finite differences
		\begin{itemize}[noitemsep,nolistsep]
			\item naive approach
			\item for each dimension $k \in [1,n]$:
			\item Estimate kth partial derivative of objective function w.r.t $\theta$
			\item By pertubing $\theta$ by small amount $\epsilon$ in kth dimension: $\frac{\partial J(\theta)}{\partial \theta_k} \approx \frac{J(\theta + \epsilon u_k) - J(\theta)}{\epsilon}$
			\item $u_k$ is unit vector with 1 in kth component, 0 elsewhere.
			\item Uses n evaluations to compute policy gradient in n dimensions.
			\item Simple, noisy, inefficient.
			\item Also works for non-differentiable policies.
		\end{itemize}
	\end{itemize}
	\item Monte-Carlo Policy Gradient
	\begin{itemize}[noitemsep,nolistsep]
		\item Likelihood Ratios:
		\begin{itemize}[noitemsep,nolistsep]
			\item Assume policy $\pi_\theta$ is differentiable whenever it is non-zero and we know the gradient $\nabla_\theta \pi_\theta(s,a)$
			\item Likelihood ratio: $\nabla_\theta \pi_\theta (s,a) = \pi_\theta(s,a) \frac{\nabla_\theta \pi_\theta (s,a)}{\pi_\theta (s,a)} = \pi_\theta(s,a) \nabla_\theta log \pi_\theta(s,a)$
			\item score function: $\nabla_\theta log \pi_\theta(s,a)$. This is similar to maximum likelihood!
			\item Softmax policy: Weight actions using linear combination of features $\phi(s,a)^T \theta$. 
			\item Softmax policy: Probability of action $\pi_\theta(s,a) \propto e^{\phi(s,a)^T \theta}$
			\item Softmax policy: score function $\nabla_\theta log \pi_\theta(s,a) = \phi(s,a) - \mathbb{E}_{\pi_\theta}[\phi(s,\cdot)]$ (how much more do i get compared to the usual)
			\item Gaussian policy: Mean using linear combination of features $\mu(s) = \phi(s,a)^T \theta$. Variance can be fixed or parametrised.
			\item Gaussian policy: Policy is Gaussian $a \sim \mathcal{N}(\mu(s),\sigma^2)$
			\item Gaussian policy: score function $\nabla_\theta log \pi_\theta(s,a) = \frac{(a - \mu(s))\phi(s)}{\sigma^2}$
		\end{itemize}
		\item Policy Gradient Theorem
		\begin{itemize}[noitemsep,nolistsep]
			\item One-Step MDPs: $J(\theta) = \mathbb{E}_{\pi_\theta}[r] = \sum_{s \in \mathcal{S}}d(s) \sum_{a \in \mathcal{A}} \pi_\theta(s,a) \mathcal{R}_{s,a},\ \nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta}[\nabla_\theta log\ \pi_\theta(s,a) r]$
			\item Policy Gradient Theorem: For anny differentiable $\pi_\theta(s,a)$, and any objective function J the policy gradient is:
			\item $\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta}[\nabla_\theta log\ \pi_\theta(s,a) Q^{\pi_\theta}(s,a)]$, $Q^{\pi_\theta}(s,a)$ is the long-term value.
			\item Monte-Carlo Policy Gradient uses $v_t$ as an unbiased sample.
		\end{itemize}
	\end{itemize}
	\item Actor-Critic Policy Gradient
	\begin{itemize}[noitemsep,nolistsep]
		\item use critic to estimate $Q_w(s,a) \approx Q^{\pi_\theta}(s,a)$. This is similar to policy evaluation
		\item Critic: Updates action-value function parameters w, Actor: Updates policy paramters $\theta$, in direction suggested by critic.
		\item Uses approximate policy gradient: $\nabla_\theta J(\theta) \approx \mathbb{E}_{\pi_\theta}[\nabla_\theta log\ \pi_\theta(s,a) Q_w(s,a)], \nabla \theta = \alpha \nabla_\theta log\ \pi_\theta(s,a) Q_w(s,a)$
		\item Simple: Linear approx: $Q_w(s,a) = \phi(s,a)^Tw$. Critic: Updates w by linear TD(0). Actor: Updates $\theta$ by policy gradient.
		\item Compatible Function Approximation
		\begin{itemize}[noitemsep,nolistsep]
			\item Approximation of policy gradient introduces bias and may not find the right solution. a good function approximation can still be not biased.
			\item If the following two conditions are satisfied:
			\item 1. Value function approximator is compatible to the policy: $\nabla_w Q_w(s,a) = \nabla_\theta log\ \pi_\theta(s,a)$
			\item 2. Value function parameters w minimise mean-squared error: $\epsilon = \mathbb{E}_{\pi_\theta} [(Q^{\pi_\theta}(s,a) - Q_w(s,a))^2]$ 
			\item Then the policy gradient is exact: $\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta}[\nabla_\theta log\ \pi_\theta(s,a) Q_w(s,a)]$
		\end{itemize}
		\item Advantage Function Critic
		\begin{itemize}[noitemsep,nolistsep]
			\item We can subtract baseline function B(s) from the poolicy gradient to reduce variance. Can use state value function $B(s) = V^{\pi_\theta}(s)$.
			\item rewrite policy gradient using the advantage function: $A^{\pi_\theta}(s,a) = Q^{\pi_\theta}(s,a) - V^{\pi_\theta}(s)$
			\item $\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta}[\nabla_\theta log\ \pi_\theta(s,a) A^{\pi_\theta}(s,a)]$
			\item The critic can reduce variance by extimating both $V^{\pi_\theta}(s)$ and $Q^{\pi_\theta}(s,a)$ with two parameter vectors and $V_v(s)$, $Q_w(s,a)$. Then update both value functions.
			\item Value function TD-Error: $\delta^{\pi_\theta} = r + \gamma V^{\pi_\theta}(s') - V^{\pi_\theta}(s)$
			\item Use it for the policy gradient: $\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta}[\nabla_\theta log\ \pi_\theta(s,a) \delta^{\pi_\theta}]$
			\item In practice we use one set of parameters for TD-Error: $\delta_v = r + \gamma V_v(s') - V_v(s)$
		\end{itemize}
		\item Eligibility Traces
		\begin{itemize}[noitemsep,nolistsep]
			\item Critic Estimations for the value function target:
			\item MC: $\Delta \theta = \alpha(v_t - V_\theta(s))\phi(s)$
			\item TD(0): $\Delta \theta = \alpha(r + \gamma V(s') - V_\theta(s))\phi(s)$
			\item TD(lambda): $\Delta \theta = \alpha(v_t^\lambda - V_\theta(s))\phi(s)$
			\item Backward view TD with eligibility: $\delta_t = r_{t+1} + \gamma V(s_{t+1}) - V(s_t),\ e_t = \gamma \lambda e_{t-1} + \phi(s_t),\ \Delta \theta = \alpha \delta_t e_t$
			\item Actor Estimations for the policy gradient:
			\item MC: $\Delta \theta = \alpha(v_t - V_v(s_t))\nabla_\theta log\ \pi_\theta(s_t,a_t)$
			\item TD(0): $\Delta \theta = \alpha(r + \gamma V_v(s_{t+1}) - V_v(s_t))\nabla_\theta log\ \pi_\theta(s_t,a_t)$
			\item TD(lambda): $\Delta \theta = \alpha(v_t^\lambda - V_v(s_t))\nabla_\theta log\ \pi_\theta(s_t,a_t)$
			\item Backward view TD with eligibility: $\delta = r_{t+1} + \gamma V_v(s_{t+1}) - V_v(s_t),\ e_{t+1} = \gamma \lambda e_t + \nabla_\theta log\ \pi_\theta(s_t,a_t),\ \Delta \theta = \alpha \delta e_t$
		\end{itemize}
		\item Natural Policy Gradient
		\begin{itemize}[noitemsep,nolistsep]
			\item A good ascent direction can speed up convergence.
			\item The vanilla gradient is sensitive to a policy that is reparametrised.
			\item Natural policy gradient finds ascent direction that is cosest to vanila gradient, when changing policy
			\item $\nabla_\theta^{nat} \pi_\theta(s,a) = G_\theta^{-1} \nabla_\theta \pi_\theta (s,a)$
			\item $G_\theta = E_{\pi_\theta} [\nabla_\theta log\ \pi_\theta(s,a)\ \nabla_\theta log\ \pi_\theta(s,a)^T]$. The fisher information matrix.
			\item Natural policy gradient simplifies: $\nabla_\theta^{nat} J(\theta) = G_\theta w = w$. In the direction of the critic parameters.
		\end{itemize}
	\end{itemize}
\end{itemize}
\href{https://www.youtube.com/watch?v=ItMutbeOHtc}{RL Course by DeepMind - Part 8: Integrating Learning an Planning}
\begin{itemize}[noitemsep,nolistsep]
	\item Introduction
	\begin{itemize}[noitemsep,nolistsep]
		\item 
	\end{itemize}
	\item Model-Based Reinforcement Learning
	\begin{itemize}[noitemsep,nolistsep]
		\item 
		\item Learning a Model
		\begin{itemize}[noitemsep,nolistsep]
			\item 
		\end{itemize}
		\item Planning with a Model
		\begin{itemize}[noitemsep,nolistsep]
			\item 
		\end{itemize}
	\end{itemize}
	\item Integrated Architectures
	\begin{itemize}[noitemsep,nolistsep]
		\item Dyna
		\begin{itemize}[noitemsep,nolistsep]
			\item 
		\end{itemize}
	\end{itemize}
	\item Simulation-Based Search
	\begin{itemize}[noitemsep,nolistsep]
		\item Monte Carlo Search
		\begin{itemize}[noitemsep,nolistsep]
			\item 
		\end{itemize}
		\item  Temporal-Difference Search
		\begin{itemize}[noitemsep,nolistsep]
			\item 
		\end{itemize}
	\end{itemize}
\end{itemize}
\href{https://www.youtube.com/watch?v=sGuiWX07sKw}{RL Course by DeepMind - Part 9: Exploration and Exploitation}
\begin{itemize}[noitemsep,nolistsep]
	\item Introduction
	\begin{itemize}[noitemsep,nolistsep]
		\item Dilemma: Exploitation: Make the best decision given current information (acting according to max) v.s. exploration: gather more information.
		\item Best long-term strategy may involve short-term sacrifices.
		\item Naive Exploration: Add noise to greedy policy $\epsilon$-greedy.
		\item Optimistic Initialisation: Assume the best until proven otherwise.
		\item Optimism in the Face of Uncertainty: Prefer actions with uncertain values.
		\item Probability Matching: Select actions according to probability they are best.
		\item Information State Search: Lookahead search incorporating value of information.
		\item State-action exploration: Systematically explore state space / action space.
		\item Parameter exploration: Pick different parameters and try for a while. 
	\end{itemize}
	\item Multi-Armed Bandits
	\begin{itemize}[noitemsep,nolistsep]
		\item tuple $\langle A,R\rangle$. A is a set of m actions, R is reward probability distribution. Goal is to maximise cumulative reward $\sum_{t=1}^T r_t$
		\item Regret
		\begin{itemize}[noitemsep,nolistsep]
			\item mean reward: $Q(a) = \mathbb{E}[r|a]$, optimal value $V^* = Q(a^*) = \underset{a \in A}{max}\ Q(a)$
			\item regret: opportunity loss: $l_t = \mathbb{E}[V^* - Q(a_t)]$, total regret $L_t = \mathbb{E}[ \sum_{t=1}^T V^* - Q(a_t)]$
			\item maximise cumulative reward = minimise total regret
			\item count: $N_t(a)$ how often action a was selected. gap: $\Delta_a = V^* - Q(a)$ (difference of action and optimal action).
			\item Regret as gaps and counts: $L_t = \sum_{a \in A} \mathbb{E}[N_t(a)] \Delta_a$.
			\item Goal: small counts for large gaps => minimizes regret.
			\item always explore and never explore will have linear total regret.
		\end{itemize}
		\item Greedy and $\epsilon$-greedy algorithms
		\begin{itemize}[noitemsep,nolistsep]
			\item greedy: Estimation $\hat{Q}_t(a) = \frac{1}{N_t(a)} \sum_{t=1}^T r_t \mathbb{1}(a_t = a)$. choose highest value: $a_t^* = \underset{a \in A}{argmax}\ \hat{Q}_t(a)$. Has linear total regret.
			\item $\epsilon$-greedy : With 1 - $\epsilon$ select $a = \underset{a \in A}{argmax}\ \hat{Q}(a)$ otherwise select random. Has linear total regret.
			\item Optimistic Initilisation: Initialise Q(a) to high value => encourages systematic exploration early on => greedy or $\epsilon$-greedy still has linear total regret.
			\item Optimistic: Update action value by Monte-Carlo: $\hat{Q}_t(a_t) = \hat{Q}_{t-1} + \frac{1}{N_t(a_t)}(r_t - \hat{Q}_{t-1})$
			\item Decaying $\epsilon$-greedy: use decay schedule e.g.: $c > 0,\ d = \underset{a|\Delta_a > 0}{min} \Delta_i,\ \epsilon_t = min\{1, \frac{c|A|}{d^2t}\}$
			\item Decying has logarithmic asymptotic total regret but needs advance knowledge of gaps.
		\end{itemize}
		\item Lower Bound
		\begin{itemize}[noitemsep,nolistsep]
			\item Hard problems have similar-looking arms with different means. 
			\item This is described fomally by the gap $\Delta_a$ and the similarity $KL(\mathcal{R}^a||\mathcal{R}^{a^*})$
			\item Asymptotic total regret is at least logarithmic in number of steps: $\lim_{t \rightarrow \infty} L_t \geq log\ t \sum_{a|\Delta_a > 0} \frac{\Delta_a}{KL(\mathcal{R}^a|| \mathcal{R}^{a^*})}$
		\end{itemize}
		\item Upper Confidence Bound
		\begin{itemize}[noitemsep,nolistsep]
			\item The more uncertain we are about an action-value the more important it is to explore that action => optimisim in the face of uncertainty.
			\item Estimate an upper confidence $\hat{U}_t(a)$ for each action value. Such that $Q(a) \leq \hat{Q}_t(a) + \hat{U}_t(a)$ with high probability.
			\item Select an action maximising Upper Confidence Bound (UCB): $a_t = \underset{a \in A}{argmax}\ \hat{Q}_t(a) + \hat{U}_t(a)$
			\item Using $U_t(a) = \sqrt{\frac{2 log(t)}{N_t(a)}}$ we have the UCB1 algorithm: $a_t = \underset{a \in A}{argmax}\ Q(a) + \sqrt{\frac{2 log(t)}{N_t(a)}}$
			\item UCB achievbes logarithmic asymptotic total regret: $\lim_{t \rightarrow \infty} L_t \leq 8 log(t)\ \sum_{a|\Delta_a > 0} \Delta_a$
		\end{itemize}
		\item Bayesian Bandits
		\begin{itemize}[noitemsep,nolistsep]
			\item Bayersian bandits exploit prior knowledge of rewards $p[\mathcal{R}]$
			\item computes posterior distribution of rewards $p[\mathcal{R}|h_t]$, where $h_t = a_1,r_1,...,a_{t-1},r_{t-1}$ is the history.
			\item Better performance if prior knowledge is accurate.
			\item e.g. assume reward distribution is Gaussian: $\mathcal{R}_a(r) = \mathcal{N}(r;\mu_a,\sigma_a^2)$
			\item probability matching: select action a according to probability that a is optimal: $\pi(a|h_t) = \mathbb{P}[Q(a) > Q(a'), \forall a' \neq a | h_t]$
			\item probability matching is optimistic in the face of uncertainty. They have a higher probability of being max.
			\item Thompson sampling: $\pi(a|h_t) = \mathbb{P}[Q(a) > Q(a'), \forall a' \neq a | h_t] = \mathbb{E}_{\mathcal{R}|h_t}[\mathbb{1}(a = \underset{a \in A}{argmax}\ Q(a))]$
			\item Bayes law to compute $p[\mathcal{R}|h_t]$, sample a reward distribution from posterior, compute action-value function $Q(a) = \mathbb{E}[\mathcal{R}_a]$
			\item then selection action maximising value on sample: $a_t = \underset{a \in A}{argmax}\ Q(a)$
		\end{itemize}
		\item Information State Search
		\begin{itemize}[noitemsep,nolistsep]
			\item Quantify the value of information. Gain is higher in uncertain situation.
			\item bandits as sequential decision-making problems.
			\item At each step there is an information state $\tilde{s} = f(h_t)$ derived from the history.
			\item Each action a causes information state transition $\tilde{\mathcal{P}}_{\tilde{s},\tilde{s}'}^a$
			\item This defines MDP in augmented information state space $\tilde{\mathcal{M}} = \langle \mathcal{\tilde{S},A,\tilde{P},R},\gamma\rangle $
			\item This MDP can be solved by reinforcement learning
		\end{itemize}
	\end{itemize}
	\item Contextual Bandits
	\begin{itemize}[noitemsep,nolistsep]
		\item A contextual bandit is a normal multi armed bandit with a State distribution S and $\mathcal{R}_s^a(r) = \mathbb{P}[r|s,a]$ the reward distribution.
		\item Linear UCB
		\begin{itemize}[noitemsep,nolistsep]
			\item Estimate value function with a linear function approximator $Q_\theta(s,a) = \phi(s,a)^T\phi \approx Q(s,a)$
			\item Estimate parameters by least squares: $A_t = \sum_{\tau=1}^t \phi(s_\tau, a_\tau)\phi(s_\tau,a_\tau)^T,\ b_t = \sum_{\tau=1}^t \phi(s_\tau,a_\tau)r_\tau,\ \theta_t = A_t^{-1}b_t$
			\item Add on a UCB $U_\theta (s,a) = c \sigma$. UCB is standard deviations above the mean.
			\item Select action maximising upper confidence bound $a_t = \underset{a \in A}{argmax}\ Q_\theta(s_t,a) + c \sqrt{\phi(s_t,a)^TA_t^{-1}\phi(s_t,a)}$
		\end{itemize}
	\end{itemize}
	\item MDPs
	\begin{itemize}[noitemsep,nolistsep]
		\item Optimistic Initialisation
		\begin{itemize}[noitemsep,nolistsep]
			\item Model-Free: Initialise action-value function Q(s,a) to $\frac{r_{max}}{1 - \gamma}$. This enourages systematic exploration of states and actions.
			\item Model-Based: Construct optimistic model of the MDP. Initialise transitions to $r_{max}$. This enourages systematic exploration of states and actions.
		\end{itemize}
		\item Optimisim in the Face of Uncertainty
		\begin{itemize}[noitemsep,nolistsep]
			\item Model-Free: Maximise UCB on action-value function: $Q^\pi(s,a): a_t = \underset{a \in A}{argmax}\ Q(s_t,a) + U(s_t,a)$. 
			\item Estimate uncertainty in policy evaulation (easy), ignores uncertainty from policy improvement.
			\item Maximise UCB on optimal action-value function: $Q^*(s,a): a_t = \underset{a \in A}{argmax}\ Q(s_t,a) + U_1(s_t,a) + U_2(s_t,a)$. 
			\item Estimate uncertainty in policy evaulation (easy), plus uncertainty from policy improvement (hard).
			\item Model-Based: Maintain posterior distribution over MDP and estimate transitions and rewards $p[\mathcal{P,R}|h_t]$
			\item Use posterior to guide exploration.
		\end{itemize}
		\item Probability Matching
		\begin{itemize}[noitemsep,nolistsep]
			\item Use Thompson Sampling. But Sample an MDP $\mathcal{P,R}$ from posterior (not reward distribution.)
		\end{itemize}
		\item Information State Search
		\begin{itemize}[noitemsep,nolistsep]
			\item MDP uses augmented state $\langle s, \tilde{s} \rangle$. s is original MDP-state and $\tilde{s}$ is history statistic.
			\item Each action causes a state transition and a information state transition.
			\item MDP is know augmented to $\mathcal{\tilde{M} = \langle \tilde{S}, A, \tilde{P}, R }, \gamma \rangle$
			\item Posterior distribution over MDP is information state: $\tilde{s}_t = \mathbb{P}[\mathcal{P,R}|h_t]$
			\item Augmented MDP over $\langle s,\tilde{s} \rangle$ is called Bayes-adaptive MDP.
		\end{itemize}
	\end{itemize}
\end{itemize}
\href{https://www.youtube.com/watch?v=sGuiWX07sKw}{RL Course by DeepMind - Part 10: Classic Games}
\begin{itemize}[noitemsep,nolistsep]
	\item Game Theory
	\begin{itemize}[noitemsep,nolistsep]
		\item Optimality in Games
		\begin{itemize}[noitemsep,nolistsep]
			\item we consider all the policies of all the players called the joint policy
			\item What is the optimal policy $\pi^i$ for the ith player?
			\item If all players fix their policies $\pi^{-i}$, then best response is $\pi_*^i (\pi^{-i})$
			\item Nash equilibrium is a joint policy for all players: $\pi^i = \pi_*^i (\pi^{-i})$ such that every player's policy is a best response.
		\end{itemize}
		\item Single-Agent and Self-Play Reinforcement Learning
		\begin{itemize}[noitemsep,nolistsep]
			\item Best response is solution to single-agent RL problem: Game is reduced to an MDP. The other players become part of the environment.
			\item Nash equilibrium is fixed-point of self-play RL. Agents play against themselves and their policies.
			\item Experience is generated by playing games between agents: $a_1 \tilde \pi^1,\ a_2 \tilde \pi^2$
			\item Each agent learns best response to other players. One player's policy determines another player's environment.
			\item All players are adapting to each other.
			\item Not all RL methods converge on a fixpoint. If we ever reach a fixpoint that is an nash equilibrium.
			\item There can be multiple nash-equilibria
		\end{itemize}
		\item Two-Player Zero-Sum Games
		\begin{itemize}[noitemsep,nolistsep]
			\item A two-player game has two (alternating) players.
			\item A zero sum game has equal and opposite rewards for player 1 and player 2: $R^1 + R^2 = 0$
		\end{itemize}
		\item Perfect and Imperfect Information Games
		\begin{itemize}[noitemsep,nolistsep]
			\item perfect information or markov game is fully observed.
			\item imperfect information game is partially observed.
		\end{itemize}
	\end{itemize}
	\item Minimax Search
	\begin{itemize}[noitemsep,nolistsep]
		\item 
	\end{itemize}
	\item Self-Play Temporal-Difference Learning
	\begin{itemize}[noitemsep,nolistsep]
		\item 
	\end{itemize}
	\item Combining Reinforcement Learning and Minimax Search
	\begin{itemize}[noitemsep,nolistsep]
		\item 
	\end{itemize}
	\item Reinforcement Learning in Imperfect-Information Games.
	\begin{itemize}[noitemsep,nolistsep]
		\item Players have different information states and therefore seperate search trees.
		\item There is one node for each information state that summarises what a player knows.
		\item Many real states may share the same information state.
		\item Can be solvec by iterative forward-search methods (e.g. Cunterfactual regret minimization).
		\item Or Self-play reinforcement Learning (e.g. Smooth UCT)
		\item Smooth UCT Search:
	\end{itemize}
\end{itemize}

\subsection{Reinforcement Learning - An Introduction} 
\href{http://incompleteideas.net/book/the-book.html}{Reinforcement Learning}
\underline{17.4 Designing Reward Signals (p.491)}
\begin{itemize}[noitemsep,nolistsep]
	\item designing reward signal is a critical so that the agent reaches the goal the designer actually desires.
	\item some problems involve goals that are difficult to translate into reward signals.
	\item reinforcement agents can discover unexpected ways to make their environment deliver reward.
	\item often it is found by trial-and-error search.
	\item If the learning is to slow the reward signal might be to sparse:
	\item sparse reward problem:
	\begin{itemize}[noitemsep,nolistsep]
		\item state-action pairs that trigger reward may be few and far between.
		\item and reward for progress might not be able to detect. The Agent may wander aimlessly for a long time, sometimes called the "plateau problem"
		\item tempting to adress this by rewarding subgoals that are important way stations for the goal.
		\item This may lead the agent to behave differently and they might not achieve to overarching goal.
		\item Bettwer way: augment value-function approximation with guesses of what it should be or parts of it schould be.
		\item $v_0: \mathcal{S} \rightarrow \mathbb{R}$ is our initial guess for optimal value function $v_*$. With linear features:
		\item $\hat{v}(s,w) = w^Tx(s) + v_0(s)$
		\item This works for arbitrary nonlinear approximators and v0, though it might not accelerate learning.
		\item effective approach: \textbf{shaping technique}:
		\item shaping involves changing the ward signal as learning proceeds. It starts as not being sparse and graudaly modifying it to reward sparsly so that it suits the actual problem.
		\item The agent faces a sequence of increasingly-difficult reinforcement learning problems.
		\item Each State is not as hard as the previous one as some basic knowledge exists. 
		\item It has similarities to transfer learning.
	\end{itemize}
	\item imitation learning
	\begin{itemize}[noitemsep,nolistsep]
		\item no idea what the wards should be, but there is another person or agent whose behavior can be observed.
		\item Benefit from the expert agent, but leave open the possibility of eventually performing better.
		\item you can either drectly use the experts behavior (supervised learning) or by extracting reward signals with "inverse reinforcement learning".
		\item inverse RL tries to recover the expert's reward signal from the expert's behavior alone.
		\item This cannot be 100 \% accurate.
		\item This needs strong assumptions about environment dynamics or feature vectors in which the reward is linear.
	\end{itemize}
	\item automate trial-and-error
	\begin{itemize}[noitemsep,nolistsep]
		\item reward signal is a perameter of the learning algorithm.
		\item define a space of feasible candidates and apllying an optimization algorithn.
		\item it runs a new agent for a few steps and scores the overall result against a high-level objective function which has the deisgners true goal.
		\item this can be improved with online gradient ascent. The gradient comes from the high-level function
	\end{itemize}
	\item given that the agent has restraints like computational power or partial observability the agents actual goal might differ from the designer's goal.
	\item the performance comparison against the high-level function is very sensitive to details in the reward signal in subtle ways.
\end{itemize}

\subsection{Medium Blog Post}
\href{https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0}{Medium Blog Post}

\subsection{RL - Base - DQN - Human-level control through deep reinforcement - 2015}
\href{https://www.nature.com/articles/nature14236}{RL - Base - DQN - Human-level control through deep reinforcement - 2015}

\subsection{RL - Base - A3C - Asynchronous Methods for Deep Reinforcement Learning - 2016}
\href{http://proceedings.mlr.press/v48/mniha16.html}{RL - Base - A3C - Asynchronous Methods for Deep Reinforcement Learning - 2016}

\subsection{RL - Sur - State-of-the-art Reinforcement Learning Algorithms - 2020}
\href{https://www.researchgate.net/publication/338396174_State-of-the-Art_Reinforcement_Learning_Algorithms}{RL - Sur - State-of-the-art Reinforcement Learning Algorithms - 2020}

\section{Deep Learning}
\subsection{Probabilistic Deep Learning Book}
\href{https://probml.github.io/pml-book/book1.html}{Probabilistic Deep Learning Book}
\\

\subsection{Deep Learning Book}
\href{https://www.deeplearningbook.org/}{Deep Learning Book}\\
\href{https://www.youtube.com/watch?v=qaMdN6LS9rA}{Chapter 11: Practical Methodology}
\begin{itemize}[noitemsep,nolistsep]
	\item Determine your goals: which error metric, your target value for this error metric. 
	\item Establish a working end-to-end pipeline as soon as possible, including the estimation of the appropriate performance metrics.
	\item Instrument the system well to detmin bottlenecks in performance. Which component is performing bad? Is it due to overfitting, underfitting or a software defect?
	\item Repeatedly make incremental changes such as gathering new data, adjusting Hyperparameters, or changing algorithms based on findings of your instrumentation. 
	\item Performance Metrics:
	\begin{itemize}[noitemsep,nolistsep]
		\item Usually Impossible to achieve zero error. Bayes errore defines the minimum error rate.
		\item May not be able to gather more data.
		\item Either from previous results or the real-world problem you can infer the minimum error rate you need.
		\item precision: fraction of detections reported by the model that were correct.
		\item recall: fraction of true events that were detected.
		\item You can plot a PR curve with precision on the y-axis and recall on the x-axis.
		\item You choose to report a result whenever its score exceeds some threshhold. By varying it you can trade precision for recall.
		\item Combine them into an F-Score $F = \frac{2pr}{p+r}$. To have a performance of the classifier in a single number.
		\item You can also look at the total area lying benath the PR curve.
		\item Some systems do not give a response if they are not confidence enought. Here coverage is a good metric. It is the fraction of examples for which the machine system is able to produce a response.
		\item Important: which performance metric to improve ahead of time and focus on that.
	\end{itemize}
	\item Default Baseline Models:
	\begin{itemize}[noitemsep,nolistsep]
		\item Which base method to use when you start out as a baseline:
		\item reasonable optimization Algorithm: SGD + Momentum and decaying learning rate.
	\end{itemize}
	\item Get more Data?
	\begin{itemize}[noitemsep,nolistsep]
		\item Is the performance of the training set acceptable? Is it more it still needs to learn more.
		\item You can then increase the size of the model by adding more layers or more hidden units to each layer.
		\item Or try improving the learning altorithm. Improve the hyperparameters.
		\item If fine tuned algorithms do not work well, the problem might lie in the data itself.
		\item Testset performance poor? more data!
		\item Create curves showing relationship between training set size and generalization error. So you can predict how much data to add
	\end{itemize}
	\item Selecting Hyperparameters:
	\begin{itemize}[noitemsep,nolistsep]
		\item Manual:
		\begin{itemize}[noitemsep,nolistsep]
			\item You need to understand the relationship between hyperparameters, training error, generalization error and computational ressources.
			\item Goal: lower generalization error subject to runtime and memory budget.
		\end{itemize}
		\item Automatic:
		\begin{itemize}[noitemsep,nolistsep]
			\item 
		\end{itemize}
		\item Grid Search:
		\begin{itemize}[noitemsep,nolistsep]
			\item 
		\end{itemize}
		\item Random Search:
		\begin{itemize}[noitemsep,nolistsep]
			\item First define a marginal distribution for each hyperparameter (Bernoulli/Multinoulli for binary/discrete params, uniform distribution on log-scale for positive real-value hyperparams).
			\item We may often want to run repeated versions of random search, to refine the search based on the results of the first run.
			\item 
		\end{itemize}
		\item Model-Based Hyperparameter Optimization
		\begin{itemize}[noitemsep,nolistsep]
			\item Simple Settings: feasable to compute the gradient of some differntiable error measure.
			\item Can build a model of the validation set error. And propose new guesses by optimization within this model.
			\item Most models use a Bayesian regresision model.
			\item Optimization is a tradeoff of exploration and exploitation.
			\item Drawback: Needs an entire run or epoch to see if the parameter where wrong. A human doing this manually can see this earlier.
		\end{itemize}
	\end{itemize}
	\item Debugging Strategies
	\begin{itemize}[noitemsep,nolistsep]
		\item A problem: If one part of the model is proken, the other parts can adapt and still get acceptable performance.
		\item Two kinds of strategies: Design a case that is so simple that the correct behavior can be predicted or we design a test that exercises one part of the Neural net implrementation in isolation.
		\item Visualize the model in action: view the detections and results of your network.
		\item Visualize the worst mistakes: View the data that has the lowest confidence which might give you insight into if the data has been processed or labled.
		\item Reason about software using training and test-error: test-error might be calculated increicctly.
		\item Fit a tiny dataset: Even small models can fit tiny dataset. If it can't there might be a software defect.
		\item Compare back-propagated derivatives to numerical derivatives: Maybe your back-propagation is wrong.
		\item Monitor histograms of activations and gradient: The preactivation value of hidden units can tell us if the units saturate or how often they do. Are some units always off? 
	\end{itemize}
\end{itemize}

\section{Deep Reinforcement Learning}
\subsection{Deep RL Bootcamp}
\href{https://sites.google.com/view/deep-rl-bootcamp/lectures}{Deep RL Bootcamp}\\
\href{https://www.youtube.com/watch?v=qaMdN6LS9rA}{Lecture 1: Intro to MDP and Exact Solution Methods}\\
Covers Value Iteration and Policy Iteration. Also covered by David Silver (Part 3)\\
\href{https://www.youtube.com/watch?v=qaMdN6LS9rA}{Lecture 2: Sample-based Approximations and Fitted Learning}\\
Covers Tabular Learning Methods. Also covered by David Silver ()\\
\href{https://www.youtube.com/watch?v=qaMdN6LS9rA}{Lecture 3: DQN and Variants}\\
Covers Experience Replay (David Silver Part 6), Fixed Q-Targets (Freecodecamp Part 4)\\
\begin{itemize}[noitemsep,nolistsep]
	\item Experience Replay gives you the most benefit of the Q-Learning Stability techniques.
	\item But using both Experience Replay and Fixed Q-Targets ist optimal.
	\item DQN uses Huber loss for Bellman error: $L_\delta (a) = \begin{cases} \frac{1}{2}a^2 & for |a| \leq \delta, \\ \delta(|a| - \frac{1}{2}\delta), & otherwise \end{cases}$
	\item It helps to anneal the exporation rate. it starts at 1 and decreases.
	\item Neural Fitted Q Iteration (Riedmiller, 2005):
	\begin{itemize}[noitemsep,nolistsep]
		\item Trains neural networks with Q-learning.
		\item Alternates between collcecting new data and fitting a new Q-function to all previous experience with batch gradient descent.
		\item DQN is an online Variant of Neural Fitted Q Iteration.
	\end{itemize}
	\item Lin's Networks (Long-Ji Lin, 1993):
	\begin{itemize}[noitemsep,nolistsep]
		\item Introduced experience replay.
		\item This network does nit share parameters among other actions. Each actions has different parameters.
	\end{itemize}
	\item Double DQN
	\begin{itemize}[noitemsep,nolistsep]
		\item There is an bias in DQN.
		\item Double DQN maintains two sets of weights $\theta$ and $\theta'$.
		\item $\theta$ is for selection the best action, $\theta'$ is for evaluating the best action.
		\item Loss: $L_i(\theta_i) = \mathbb{E}_{s,a,s',r} (r + \gamma Q(s', \underset{a'}{arg\ max}\ Q(s',a';\theta);\theta'_i) - Q(s,a;\theta_i))^2$
		\item Usually Better.
	\end{itemize}
	\item Prioritised Experience Replay
	\begin{itemize}[noitemsep,nolistsep]
		\item Replay transitions in proportion to absoulte Bellman error: $|r + \gamma \underset{a'}{max}\ Q(s',a';\theta') - Q(s,a;\theta)|$
		\item Leads to much faster learning as supposed to replaying with equal probability.
	\end{itemize}
	\item Dueling DQN
	\begin{itemize}[noitemsep,nolistsep]
		\item Value-Advantage decomposition of Q $Q^\pi(s,a) = V^\pi(s) + A^\pi(s,a)$
		\item Dueleing DQN: $Q(s,a) = V(s) + A(s,a) - \frac{1}{|\mathcal{A}|} \sum_{a=1}^{|\mathcal{A}|} A(s,a)$
		\item The last Layer is not a single Q-Layer but two Layers. One for the Value Function and one for the Advantage Function.
		\item “Dueling Network Architectures for Deep Reinforcement Learning”, Wang et al 2016
	\end{itemize}
	\item Noisy Nets for Exploration
	\begin{itemize}[noitemsep,nolistsep]
		\item Add noise to network parameters for better exploration
		\item Standard linear layer $y = wx + b$
		\item Noisy linear Layer $y = (\mu^w + \sigma^w \odot \epsilon^w)x + \mu^b + sigma^b \odot \epsilon^b$
		\item $\epsilon^w, \epsilon^b$ contain noise.
		\item $\sigma^w, \sigma^b$ are learned parameters that determine the amount of noise.
	\end{itemize}
\end{itemize}
\href{https://www.youtube.com/watch?v=qaMdN6LS9rA}{Lecture 4a: Policy Gradients and Actor Critic}\\
\begin{itemize}[noitemsep,nolistsep]
	\item Why Policy Optmization:
	\begin{itemize}[noitemsep,nolistsep]
		\item policy can be simpler than q or v.
		\item value-function: does not prescribe actions. This might need a model of the dynamics.
		\item action-value-function: needs to be able to efficiently solve and argmax over q: Challenge for continuous / high-dimensional action spaces.
	\end{itemize}
\end{itemize}
\href{https://www.youtube.com/watch?v=qaMdN6LS9rA}{Lecture 5: Natural Policy Gradients, TRPO, PPO}\\
\begin{itemize}[noitemsep,nolistsep]
	\item this lecture: once you have your advantage estimate how do you update your policy with that?
	\item Limitations of Vanilla Policy Gradient Methods:
	\begin{itemize}[noitemsep,nolistsep]
		\item Hard to choose stepsizes: Input data is nonstationary due to changing policy => observation and reward distribution change.
		\item Can partially be adressed with normalization techniques.
		\item Also: Bad step is damaging since it affects visitation distribution. Too Far => Bad Policy => Can't recover!
		\item Sample Efficiency: Only one gradient step per environment sample. Depent on scaling of coordinates.
	\end{itemize}
	\item What Loss to optimize
	\begin{itemize}[noitemsep,nolistsep]
		\item Policy gradients $\hat{g} = \hat{\mathbb{E}}_t[\nabla_\theta log\ \pi_\theta(a_t|s_t)\hat{A}_t]$ hat = empirical!
		\item Loss: $L^{PG}(\theta) = \hat{\mathbb{E}}_t[log\ \pi_\theta(a_t|s_t)\hat{A}_t]$.
		\item This loss should not be optimized too far (to a solution), as if you just take this loss it might diverge to infinity.
		\item Noisy estimate => radical changes in policy.
		\item Another Version $L_{\theta_{old}}^{IS}(\theta) = \hat{\mathbb{E}}_t [\frac{\pi_\theta (a_t, s_t)}{\pi_{\theta_{old}}(a_t | s_t)} \hat{A}_t]$
		\item at $\theta = \theta_{old}$, state-actions are samples using the old one.
		\item We get the same gradient. In practice $L^{IS}$ is not much different than the logprob version, for reasonably small policy changes.
	\end{itemize}
	\item Trust Region Policy Optimisation
	\begin{itemize}[noitemsep,nolistsep]
		\item Idea: A function I want to optimise and some local approximation of that function which is only accurate locally. We have a trust region where we trust our local approximator.
		\item Function to optimise: $\underset{\theta}{maximize}\ \hat{\mathbb{E}}_t [\frac{\pi_\theta (a_t, | s_t)}{\pi_{\theta_{old}}(a_t | s_t)} \hat{A}_t]$
		\item If you differentiate this you get the policy gradient!
		\item with a constraint: we are not too far from the starting point of that approximation.
		\item Example: Euclidian Distance between starting point $\theta_{old}$ and final point $\theta$ is small.
		\item Better: KL-Divergence: $\hat{\mathbb{E}}_t [KL[\pi_{\theta_{old}}(\cdot|s_t),\pi_{\theta}(\cdot|s_t)]] \leq \delta$
		\item We could penalize the constraint problem by $\underset{\theta}{maximize}\ Function\ to\ optimize - \beta \cdot KL-Divergence$
		\item Method of Lagrange multipliers: optimality point of delta-constrained problem is also an optimality point of beta-penalized problem for some beta.
		\item Practice: delta is easier to tune.
	\end{itemize}
	\item Monotonic Improvement Result: If you look at the KL penalized objective. Then in theory if we use max KL instead of mean KL in penalty, then we get a lower (= pessimistic) bound on policy performance. This is the theory for the result.
	\item Maximize the max over KL in the state-space => we are guaranteed to improve the policy with the penalized objective and a lower bound.
	\item objective: $\underset{\theta}{maximize}\ \sum_{n=1}^N \frac{\pi_\theta (a_n | s_n)}{\pi_{\theta_{old}}(a_n | s_n)} \hat{A}_n$ constraint: $\bar{KL}_{\pi_{\theta_{old}}}(\phi_{\theta}) \leq \delta$. $\hat{A}_n$ is an estimation.
	\item TRPO can sole constrained optimization problem efficiently by using conjugate gradient. Related to natural policy gradients and natural actor critic
	\item Solving KL Penalized Problem:
	\begin{itemize}[noitemsep,nolistsep]
		\item The Penalized Version is calculated using a linear approximation of the objective and a quadratic approximation of the KL term
		\item $\underset{\theta}{maximize}\ g \cdot (\theta - \theta_{old}) - \frac{\beta}{2}(\theta - \theta_{old})^T F(\theta - \theta_{old})$
		\item where $g = \frac{\partial}{\partial \theta} L_{\pi_{\theta_{old}}}(\pi_\theta)|_{\theta = \theta_{old}},\ F = \frac{\partial^2}{\partial^2 \theta} \bar{KL}_{\pi_{\theta_{old}}}(\pi_\theta)|_{\theta = \theta_{old}}$
		\item Solution: $\theta - \theta_{old} = \frac{1}{\beta}F^{-1}g$, F is the Fisher Information matrix, g is policy gradient. This is called the natural policy gradient.
		\item Use Conjugate Gradient Algorithm to approximately solve: Fx = g. 
	\end{itemize}
	\item Solving KL Constrained Problem:
	\begin{itemize}[noitemsep,nolistsep]
		\item TO-DO!
	\end{itemize}
	\item For PPO use the penality version. Do a SGD on above objective for some number of epochs. 
	\item If KL too high, then increase beta. If KL is too low, decrease beta
	\item Review:
	\begin{itemize}[noitemsep,nolistsep]
		\item suggested optimizing surrogate loss $L^{PG}$ or $L^{IS}$
		\item suggested using KL to constrain size of update.
		\item Correspondes to natural gradient step $F^{-1}g$ under linear quadratic approximation.
		\item Can solve for this step approximately using conjugate gradient method.
	\end{itemize}
	\item Linear-quadratic approximation + penalty => natural gradient.
	\item No contraint => policy iteration.
	\item Eucleidean penalty instead of KL => vanilla policy gradient.
	\item Limitations of TRPO
	\begin{itemize}[noitemsep,nolistsep]
		\item Hard to use with architectures with multiple outputs, e.g. policy and value functions
		\item Empirically performs poorly on tasks requiring deep CNNs and RNNs.
		\item Conjugate Gradient makes implementation more complicated.
	\end{itemize}
	\item KFAC
	\begin{itemize}[noitemsep,nolistsep]
		\item Make Approximation of Fischer Information Matrix, that exploits the structure of Neural Networks. Block Diagonal Approximation of the weight Matrix.
		\item TO-DO
	\end{itemize}
	\item ACKTR: Combine A2C with KFAC Natural Gradient.
	\begin{itemize}[noitemsep,nolistsep]
		\item Combined with A2C, gives excellent on continuous control from images.
		\item We already need the log pi for policy gradient, so it does not take extra computation.
		\item Matrix inverses can be computed asynchronously.
		\item Limitation: Straightforward for Feedforward nets and Convolutions. Less straightforward for RNNs with shared weights.
	\end{itemize}
	\item Can use Clipping Objective for Proximal Policy Optimization. (TO-DO)
	\item This is a bit better than TRPO on continuous control. Compatible with multi-output networks and RNNs.
\end{itemize}
\href{https://www.youtube.com/watch?v=xvRrgxcpaHY}{Lecture 6: Nuts and Bolts of Deep RL Experimentation}
\begin{itemize}[noitemsep,nolistsep]
	\item Approaching New Problems:
	\begin{itemize}[noitemsep,nolistsep]
		\item New Algorithm:
		\begin{itemize}[noitemsep,nolistsep]
			\item Run experiments quickly.
			\item Do Hyperparameter search.
			\item Interpret and visualize learning process: state visitation, value function (fitting), state distribution etc.
			\item Construct toy problems where your idea will be strongest and weakest, where you have a sense of what it should do
			\item Counterpoint: don't overfit algorithm to contrived problem
			\item Useful: medium-sized problems that you're intimately familiar with
		\end{itemize}
		\item new Task: Provide good input features, shape reward function
		\item POMDP Design
		\begin{itemize}[noitemsep,nolistsep]
			\item Visualize random policy: does it sometimes exhibit desired behavior?
			\item Plot time series for observations and rewards. Are they on a reasonable scale?
			\item Histogram observations and rewards.
		\end{itemize}
		\item Run Your Baseline: Don't expect them to work with default parameters.
		\item Recommended: Cross entropy method (Learning Tetris using the noisy cross-entropy method), \href{https://github.com/openai/baselines}{Well-tuned policy gradient or Q-learning methods}, \href{https://github.com/rll/rllab}{alternative}.
		\item Early in tuning process you may need a huge number of samples. It might need a "burn-in" period.
		\item Don't get deterred if you cannot published work to run directly.
	\end{itemize}
	\item Ongoing Development and Tuning:
	\begin{itemize}[noitemsep,nolistsep]
		\item Explore sensitivity to each parameter: If too sensitive, it doesn't really work you just got lucky.
		\item Look for health indicators: VF fit quality, policy entropy, Update size in output space and parameter space, Standard diagnostics for deep networks.
		\item If reusing code, regressions occur => run a battery of benchmarks occasionally.
		\item Always use different random seeds so that your algorithm is not influenced by one seed alone.
		\item Always try to simplify the algirithm. Different tricks basically do a similar thing (like in whitening).
		\item Favor simplicity in algorithm which normally leads to generalization.
		\item Automate your experiments!
	\end{itemize}
	\item General Tuning Strategies for RL:
	\begin{itemize}[noitemsep,nolistsep]
		\item Whitening:
		\begin{itemize}[noitemsep,nolistsep]
			\item If observations have unknown range, standardize them
			\item compute running estimate of mean and standard deviation.
			\item $x' = clip((x-\mu)/\sigma, -10, 10)$
			\item Rescale the rewards, but don't shift mean. That affects the agent.
			\item Standardize prediction targets (e.g. value function) the same way.
		\end{itemize}
		\item Generally important parameters:
		\begin{itemize}[noitemsep,nolistsep]
			\item Discount: gamma = 0.99 => ignore rewards delayed by more than 100 timesteps.
			\item Low gamma works well for well-shaped reward.
			\item with TD(lambda) methods, can get away with high gamma when lambda < 1
			\item Continuous timesteps (like games) are usually discretisized to time steps (like frameskips). Can the action frequency the agent has actually solve the problem?
			\item Also: look the random exploration: If you do the same action multiple times in a row you tend to explore further. Choose an interesting diskretisation
			\item Look at min/max/stdev of episode returns, along with means.
			\item Look at episode lengths: sometimes useful information: Solving problem faster, losing game slower
		\end{itemize}
	\end{itemize}
	\item Policy Gradient Strategies:
	\begin{itemize}[noitemsep,nolistsep]
		\item Premature drop in policy entropy => policy gets deterministic => no exploring => no learning (drops eventually but not super fast).
		\item Alleviate by using entropy bonus or KL penalty. 
		\item KL $KL[\pi_{old}(\cdot|s), \pi_{old}(\cdot|s)]$
		\item How to measure entropy: discrete: analytically, continuous policy: gaussian policy => compute differential entropy.
		\item Action-Space Entropy is what we are talking about. State-Space Entropy is to hard to calculate on anything nontrivial.
		\item KL = 0.01: Small update, 10: big update
		\item KL spike => drastic loss of performance.
		\item No learning progress might mean steps are too large
		\item explained variance = $\frac{1 - Var[empirical\ return - predicted\ value]}{Var[empirical\ return]}$
		\item Policy Initialization: Determines initial state visitation.
		\item Zero or tiny final layer, to maximize entropy
	\end{itemize}
	\item Q-Learning Strategies:
	\begin{itemize}[noitemsep,nolistsep]
		\item Optimize memory usage carefully: you'll need it for replay buffer.
		\item Learning rate schedules.
		\item Exploration schedules.
		\item DQN converges slowly
	\end{itemize}
	\item Miscellaneous Advice:
	\begin{itemize}[noitemsep,nolistsep]
		\item Don't get stuck on problems. Some algorithms do really well on some problems but bad on another.
		\item Techniques from supervised learning don't necessrily work in RL: batch norm, dropout, big networks.
	\end{itemize}
\end{itemize}


\section{Multi-Agent Systems}
\subsection{!MAS - An Introduction to Multi-Agent Systems - 2010}
\href{https://link.springer.com/chapter/10.1007/978-3-642-14435-6_1}{MAS - An Introduction to Multi-Agent Systems - 2010}
\\
\underline{Benefits of using MAS in large systems}
\begin{itemize}[noitemsep,nolistsep]
	\item Increase in the speed and efficiency of the operation due to parallel computation and asynchronous operation.
	\item Graceful degradation when one or more of the agent fail, thus increasing realibility and robustness of the system.
	\item Scalability and flexibility - Agents can be added as and when necessary.
	\item Cost Reduction: Individual agents cost much less than a centralized architecture
	\item Reusability: Agents with a modular structure can be easily replaced in other systems or be upgraded more easily than a monolithic sysetm.
\end{itemize}
\underline{Challenges of using MAS in large systems}
\begin{itemize}[noitemsep,nolistsep]
	\item environment: An agents action modify its own environment but also that of its neighbours. therefore they need to predict the action of the other agents so that they can reach a goal. This can be an unstable system. Environment dynamic: Is the effect caused by other agents or by the variation in the environment?
	\item perception: limited sensing range => each agent only has partial observability for the environment. Therefore the decisions reached might be sub-optimal.
	\item Abstraction: ???
	\item conflict resolution: lack of global view => conflict. therefore information on constraints, action preferences and goal prioritoes must be shared between agents. When to communicate what to which agent?
	\item Inference: Single-Agent: State-Action-Space can be mapped with trial and error. Multi-agent: each agent may or may not interact with each other. If they are heterogenous, they might even compete and have different goals. You need a fitting inference machanism
\end{itemize}
\underline{Classification: Internal Architecture}
\begin{itemize}[noitemsep,nolistsep]
	\item homogeneous: all agents have the same internal architecture (Local Goals, Sensor Capabilities, Internal states, Inference Mechanism and Possible Actions). In a typical distributed environment, overlap of sensory inputs is rarely present
	\item Heterogeneous: agents may differ in ability, structure and functionality. Because of the dynamics and location the actions chosen might differ between agents. their local goals may contradict the objective of other agents.
\end{itemize}
\underline{Classification: Agent Organization}
\begin{itemize}[noitemsep,nolistsep]
	\item hierarchical: typical: tree-structure. At different heights, different levels of autonomy. data from lower levels flow upwards. Control signal flows from high to low in the hierarchy.
	\begin{itemize}[noitemsep,nolistsep]
		\item simple: the decision making authority is a single agent of highest level. BUT: single point of Failure
		\item uniform: authority is distributed among the various agents, for better efficiency, fault tolerance, graceful degradation. Decisions made by agent with appropriate information. (MAS - TrafficControl - Neural Networks for Continuous Online Learning and Control - 2006)
	\end{itemize}
	\item holonic: fractal structure of several holons. Self-repeating. Used for large organizational behaviours in manufacturing and business. 
	\begin{itemize}[noitemsep,nolistsep]
		\item An agent that appears as a single entity might be composed of many sub-agents. They are not predetermined, but form through commitments.
		\item Each holon has a head agent that communicates with the environment or with other agents in the environment. It is selected either randomly, through a rotation policy, or selected by resource availability, communicaton capability.
		\item Holons can be nested to form Superholons.
		\item compare to tree: in Holons cross tree interactions and overlapping of holons is allowed.
		\item pro: abstraction good degree of freedom, good agent autonomy.
		\item contra: abstraction makes it difficult for other agents to predict the resulting actions of the holon.
	\end{itemize}
	\item coalitions: group of agents come together for a short time to increase utility or performance of the individual agents in a group. they cease to exist when the performance goal is achieved.
	\begin{itemize}[noitemsep,nolistsep]
		\item coalition may have either a flat or a hierarchical architecture.
		\item It may have an leading agent to act as a representative. 
		\item overlap is allowed. this increased complexity of computation of the negotiation strategy.
		\item You can have one coaltion with all agents => maximum performance of system. Impractical due to restraints on communication and resources.
		\item minimize amount of colations: because of the cost of creating and dissolving a colation group.
	\end{itemize}
	\item teams: agents work together to increase the overall performance of the group, rather than working as individual agents.
	\begin{itemize}[noitemsep,nolistsep]
		\item their interactions can be arbitrary and the goals and roals can vary with the performance of the group.
		\item large team size is not beneficial under all conditions. some compromises must be made.
		\item large teams offer a better visibility of the environment. but is slower computation wise. Learning-Performance Tradeoff.
		\item computation cost usually much greater than coalitions.
	\end{itemize}
\end{itemize}
\underline{Classification: Communication}
\begin{itemize}[noitemsep,nolistsep]
	\item local communication: agents directly communicate similar to message passing. there is no place to store information. creates distributed architecture. used in: (25),(37),(38).
	\item blackboards: a group of agents share a data repository which is provided for efficient storage.
	\begin{itemize}[noitemsep,nolistsep]
		\item can hold design data and control knowledge, accessable by the agents.
		\item control shell: notfies the agent when relevant data is available.
		\item single point of failure.
	\end{itemize}
	\item agent communication language (ACL): common framework for interaction and information sharing. (40).
	\begin{itemize}[noitemsep,nolistsep]
		\item procedural approach: modelled as a sharing of the precedural directives. Shared how an agent does a specific task or the entire working of the agent itself. Script Languages often used. Disadvantage: necessitiy of providing information on the recipient agent, which is in most cases partially known. Also how to merge the scripts into one executable. Not preferred method.
		\item declarative approach: sharing of statements for definitions, assumptions assertions, axioms etc. Short declarative statements as length increases probability of information corruption. Example: ARPA knowledge sharing effort.
		\item Best known inner languages: Knowledge Interchange Format. Information exchange is implicitly embedded in KIF. But the package size grows with the increase in embedded information. Solution: High-level Languages like KQML (Knowledge QUery and Manipulation Language)
	\end{itemize}
\end{itemize}
\underline{Classification: Decision making in Multi-Agent Systems}
\begin{itemize}[noitemsep,nolistsep]
	\item uncertainty: effects of a specific actions on the environment and dynamics because of the other agents.
	\item Methodology to try and find a joint action or equilibrium point which maximizes the reward of every agent.
	\item Typically modelled with game theory method. Strategic games:
	\begin{itemize}[noitemsep,nolistsep]
		\item a set of players (agents)
		\item Foreach player, there is a set of actions
		\item Foreach player, the prefeernces over a set of actions profiles
		\item payoff with the combination of action, a joint-action, that is assumed to be predefined.
		\item all actions are observable forall agents.
		\item make the assumption that all participating agents are rational.
	\end{itemize}
	\item Nash equilibrium: for a payoff matrix: An action profile (joint-action), where no player can do better by choosing one of the actions differently, given that the other player chose a specific action.
	\item there might be multiple nash equilibrium, so that there is no dominant solution. Here the coordination of MAS is needed to find a solution.
	\item Iterated Elimination Method: Strongly dominated actions are iteratively eliminated. This fails if there are no strictly dominated actions available.
\end{itemize}
\underline{Classification: Coordination}
\begin{itemize}[noitemsep,nolistsep]
	\item agents work in parallel, therefore they need to be coordinated or synchronize the actions to ensure stability of the system.
	\item other reasons: prevent chaos, meet global constraints, utilize distributed resources, prevent conflicts, improve efficiency.
	\item achievable with constraints on the joint actions or by using information collated from neighbouring agents. Used to find the equilibrium action.
	\item payoff matrix necessary might be difficult to determine. It increases expenentially in the number of agents and action choices.
	\item dividing the game into subgames: roles (permitted actions is reduced, good for distributed coordination or centralized coordination)
	\item Coordination via Protocol.
	\begin{itemize}[noitemsep,nolistsep]
		\item negotioation to arrive an approdiate solutions.
		\item Agents assume the role of manager (divide the problem) and contractor (who deals with the subproblems).
		\item The manager and contractor are working in a bidding system.
		\item Example: FIPA model
		\item disadvantage: assumption of the existence of an cooperative agent. It is very communication intensive
	\end{itemize}
	\item Coordination via Graphs: Problem is subdivided into easer problems. Assume the payoffs can be linear combinated from the local payoffs of the sub-games. THen just eliminate agents to find the optimal joint.
	\item Can also use belif models. Internal models of an agent on how he believes the environment works (needs to differentiate between environment and effects of other agents).
\end{itemize}
\underline{Classification: Learning}
\begin{itemize}[noitemsep,nolistsep]
	\item active learning: analysing the observations to creat a belief or internal model of the corresponding situated agent's environment. 
	\begin{itemize}[noitemsep,nolistsep]
		\item can be performed by using a deductive, inductive or probabilistic reasoning approach.
		\item deductive: inference to explain an instance or state-action sequence using his knowledge. It is deduced or inferred from the original knowledge it is nothing new. It could form new parts of the knowledge base. uncertainty is usually disregarded (not good for real-time)
		\item inductive: learning from observations of state-action pair. Good when environment can be presented in terms of some generalized statements. they use the correlation between observations and the action space.
		\item probabilistic: assumption: knowledge base or belief model can be represented as probabilities of occurrence of events. observations of the environment is used to predict the internal state of the agent. Good example: Bayesian learning. Difficult for MAS, as the joint probability scales poorly in the number of agents.
	\end{itemize}
	\item reactive learning: updating belief without having the actual knowledge of what needs to be learnt.
	\begin{itemize}[noitemsep,nolistsep]
		\item useful when the underlying model of the agent or the environment is not known clearly and are black boxes.
		\item can be ssen in agents which utilize connections systems such as NN.
		\item can use reactive multi-agent feed forward neural networks.
		\item they depend on the application domain and are therefore rarely employed in real world scenarios.
	\end{itemize}
	\item learning based on consequences:
	\begin{itemize}[noitemsep,nolistsep]
		\item learning methods based on evaluation of the goodness of selected action. like in reinforcement learning.
		\item programming the agents using reward and punishment scalar signals without specifying how the task is to be achieved.
		\item learnt through trial and error and interaction with the environment.
		\item usually used when action space is small and descret. Recent developments allow them to work in continious and large state-action space scenarios.
		\item An agent is usally represented as a Markov Decision Process.
		\item Expectaation operator optmal policy is the argmax of the Q-value, which uses the bellman equation. Bellman equation is solved iteratively.
		\item The solution is referred to as q-learning method.
		\item For MAS the reinforcement learning method has the problem of combinatorial explosion in the state-action pairs.
		\item The information must be passed between the agents for effective learning.
	\end{itemize}
\end{itemize}

\subsection{!Artificial Intelligence - A modern Approach}
\underline{Multiagent Planning (p.425)}
\begin{itemize}[noitemsep,nolistsep]
	\item each agent tries to achieve is own goals with the help or hindrance of others
	\item wide degree of problems with various degrees of \textbf{decomposition of the monolithic agent}.
	\item multiple concurrent effectors => \textbf{multieffector planning} (like type and speaking at the same time).
	\item effectors are physically decoupled => \textbf{multibody planning}.
	\item if relevant sensor information foreach body can be pooled centrally or in each body ~ like single-agent problem.
	\item When communication constraint does not allow that: \textbf{decentralized planning problem}. planning phase is centralized, but execution phase is at least partially decoupled.
	\item single entity is doing the planning: one goal, that every body shares.
	\item When bodies do their own planning, they may share identical goals.
	\item \textbf{multibody}: centralized planning and execution send to each.
	\item \textbf{multiagent}: decentralized local planning, with coordination needed so they do not do the same thing.
	\item Usage of \textbf{incentives} (like salaries) so that goals of the central-planner and the individual align.
\end{itemize}

\underline{Multiple simultaneous actions}
\begin{itemize}[noitemsep,nolistsep]
	\item \textbf{correct plan}: if executed by the actors, achieves the goal. Though multiagent might not agree to execute any particular plan.
	\item \textbf{joint action}: An Action for each actor defined => joint planning problem with branching factor b\^n (b = number of choices).
	\item if the actors are \textbf{loosely coupled} you can describe the system so that the problem complexity only scales linearly.
	\item standard approach: pretend the problems are completely decoupled and then fix up the interactions.
	\item \textbf{concurrent action list}: which actions must or most not be executed concurrently. (only one at a time)
\end{itemize}

\underline{Multiple agents: cooperation and coordination}
\begin{itemize}[noitemsep,nolistsep]
	\item each agent makes its own plan. Assume goals and knowledge base are shared.
	\item They \textbf{might choose different plans} and therefore collectively not achieve the common goal.
	\item \textbf{convention}: A constraint on the selection of joint plans. (cars: do not collide is achieved by “stay on the right side of the road”).
	\item widespread conventions: social laws.
	\item absence of convention: use communication to achieve common knowledge of a feasible joint plan.
	\item The agents can try to \textbf{recognize the plan other agents want to execute} and therefore use plan recognition to find the correct plan. This only works if it is unambiguously.
	\item an \textbf{ant} chooses its role according to the local conditions it observes.
	\item ants have a convention on the importance of roles.
	\item ants have some learning mechanism: a colony learns to make more successful and prudent actions over the course of its decades-long life, even though individual ants live only about a year.
	\item Another Example: \textbf{Boid}
	\item If all the boids execute their policies, the flock inhibits the emergent behavior of flying as a pseudorigid body with roughly constant density that does not disperse over time.
	\item \textbf{most difficult multiagent} problems involve both cooperation with members of one’s own team and competition against members of opposing teams, all without centralized control.
\end{itemize}

\subsection{!MAS - Sur - Multi-Agent Systems - A Survey - 2018}
\href{https://ieeexplore.ieee.org/abstract/document/8352646}{MAS - Sur - Multi-Agent Systems - A Survey - 2018}
\begin{itemize}[noitemsep,nolistsep]
	\item 2: Agent Introduction: 
	\begin{itemize}[noitemsep,nolistsep]
		\item Accessibility: Accuracy of the agents sensors to collect data from environment. Inaccessable: incomplete or noisy data.
		\item Determinism: Result of an action. Deterministic: Predictable. Non-Deterministic: Randomness or factors like the actions of all participants.
		\item Dynamism: the change that occurs in the environment that are independent of the actions taken by the agents. If it can only change with the action of agents it is static. Otherwise it is considered dynamic. When the environment changes dynamically the previously sensed information may not longer be accurate.
		\item Discrete action: finite set of actions. Continuous Action: practically unlimited amount of actions.
	\end{itemize}
	\item 3: MAS
	\begin{itemize}[noitemsep,nolistsep]
		\item Model MAS as a graph of agents. The Edge indicates communication.
		\item The decision of an agent might change the structure of the graph.
		\item Agent might need time to find and communicate that provides a certain service. This overhead can be midigated for large-scale MAS by \textbf{middle agents}.
		\item middle agents have a list of services that all agents provide. It can give this information to a searching agent so that he knows which other agent gives a certain service.
		\item Middle Agent Types: 
		\begin{itemize}[noitemsep,nolistsep]
			\item Facilitator: Always a intermediate between the requester and the service. The facilitator becomes a bottleneck and a single point of failure. Multiple collaborative facilitators can be employed. But they need to synchronize and balance their loads. 
			\item Mediator: They just mediate the communication between two agents and after that the two agents communicate directly.
		\end{itemize}
		\item MAS Features:
		\begin{itemize}[noitemsep,nolistsep]
			\item Leadership: a leader, an agent that defines goals and tasks for th eother agents based on one global goal. MAS can be leaderless or leader-follow. In leaderless MAS each agent autonomously decides it's action. In leader-follow their actions are given by the leader. Leader is predefined or chosen by the agents. Leader may move and agents need to track it's position to get in communication range.
			\item Decision Function: linear MAS: decision of an agent is proportional to the sensed parameters from environment. non-linear MAS the decision nonlinear in the sensors due to some non-linearity of the decision proccess.
			\item Heterogenity:
			\item Delay Consideration: Delays in e.g. communication. MAS is delayed or without delay. Most real-world application might feature some delays.
			\item Topology: Position and relations of the agents that can be static or dynamic.
			\item Data Transmission Frequency: Data sharing is either time-triggered (update-rate) or event-triggered.
			\item Mobility: static agent has always the same position, mobile agent can move in the environment.
		\end{itemize}
		\item MAS comparison to OOP and expert systems: ???
	\end{itemize}
	\item 4. Applications:
	\begin{itemize}[noitemsep,nolistsep]
		\item Computer networks:
		\begin{itemize}[noitemsep,nolistsep]
			\item Cloud Computing:
			\item Social Networking:
			\item Security:
			\item Routing:
		\end{itemize}
		\item Agents in robotics:
		\item Agents for modeling complex systems:
		\item Agents in city and built environments:
		\item Agents in smart grids:
	\end{itemize}
	\item 5. Challenges:
	\begin{itemize}[noitemsep,nolistsep]
		\item Coordination Control:
		\item the action perfomed by each agent affects the environment and thus the other agents.
		\item coordination control refers to managing agents to collaboratively reach their goals.
		\begin{itemize}[noitemsep,nolistsep]
			\item \textbf{Consensus:} A global agreement over a perticular feature of interest.
			\item Table 3: Kinds of MAS features and their consensus approaches.
			\item [38]: agents disagreement reduces over time until a consensus is reached. average-consensus: consensus over average of the initial state of the feature. Finding parameters so that each agent can reach consesus might not be possible so an average can be employed.
			\item Flocking: collective behavior of agents to reach a group objective. In nature: ant conoly, bees, fish, penguins. See Boid Algorithm for that. Needs rules like cohesio, seperation and alignment.
			\item Consensus has two sub-challenges:
			\item Tracking: with leader: agents should reach consensus over leader position. Agents keep connection to leader.
			\item Containment: Multiple leaders. the leaders limit all possible position of the agents because of required communication range.
			\item \textbf{Controllability:} Steering the MAS to a specific state using regulations. there is certainty in reaching a particular state by following specific steps. Dynamism and determinism in the environment can change the controllability. Both cases introduces delay. In literature controllability is largely achieved in a centralized manner. 
			\item \textbf{Synchronization:} actions each agent performs are aligned in time with other agents. This is a problem of Heterogenity as for different features that need to be synced uses the slowest one. Heterogeneous synchronization known as partial-state or output synchronization.
			\item \textbf{Connectivity:} Sometimes permanent connections are required but mobility of the agents and enviroment noise or a limited view of the topology can make this challenging. MAS can be connected or connectionless in reference if they are permanent.
			\item \textbf{Formation:} Structure that is maintained for a specific period of time, known as formation challenge.
		\end{itemize}
		\item \textbf{Learning:} Learning complexity increases with processing and communication overhead, dynamic environments, MAS topology, protection against malicious agents and scalability of learning methods.
		\item  agents can share their knowledge and achieve collaborative learning [128]. 
		\item Main Learning methods: RL and Genetic Programming (GP). 5, 122, 126, 127
		\item \textbf{Fault Detection:} Detecting and isolating faulty agents. Fault Detection and Isolation (FDI). Usually centralized, but are suboptimal for large-scale problems. Because of single point of failure and overwhelming one agent with requests.
		\item Details: [130], [131], [136]
		\item Types: with/without Embedded Residual Generator (ERG). ERG uses new environment inputs aswell as previous FDI results.
		\item Problems: Mostly focused on homogeneous agents, most methods require high resources, Little work on agent isolation, most solutions are centralized. Survey on FDI [6]
		\item \textbf{Task Allocation:} allocation of tasks to the agents considering the costs, time and overheads. centralized or decentralized [137]. [134]: dynamic clusters.
		\item Allocation is usally using metrics like the agent's talent and agent's position.
		\item \textbf{Localization:} Limited view => locating a particular agent can be a problem. most methods are centralized. 
		\item Distributed systems: share location info with neighbors and after some iteration every agent knows where every agent is. This can add an additional delay on information propagation.
		\item Localizing dynamic agents becomes more problematic. You might need to estimate the positon [142]
		\item Agent Organization:
		\begin{itemize}[noitemsep,nolistsep]
			\item \textbf{Flat:} (already done)
			\item \textbf{Hierarchical:} (already done)
			\item \textbf{Holonic:} (already done)
			\item \textbf{Coalition:} (already done)
			\item \textbf{Team:} (already done)
			\item \textbf{Matrix:} Each agent is managed by at least two head agents (managers). Effective where agents are controlled by more than eone leader.
			\item \textbf{Congregation:} agents in a location form a congregation to achieve their requirements that they cannot achieve alone. Each agent can leave or join them freely (only part of one at a time). The fulfillment of the requirement depends on the other agents in the congregation. [139]
		\end{itemize}
		\item Security: Agents use information they get from other agents to learn this makes them vulnerable. Without a central trused authority verification becomes challenging. Mobility might make agents be able to spread misinformation.
		\item Security compenents: Authentication, Authorization, Integrity, availability and Confidentiality.
	\end{itemize}
	\item 6. Communication:
	\begin{itemize}[noitemsep,nolistsep]
		\item speach at: some utterance of verbs or sentences that change the physical environment.
		\item message passing: done in: MAS - An Introduction to Multi-Agent Systems - 2010
		\item blackboard: done in: MAS - An Introduction to Multi-Agent Systems - 2010
		\item message semantics important so that each agent has the same interpretation. Hard in heterogeneous agent. An Agent Communicate Language adresses that. donne in: MAS - An Introduction to Multi-Agent Systems - 2010
	\end{itemize}
	\item 7. Modeling Environements:
	\begin{itemize}[noitemsep,nolistsep]
		\item Java Agent Development framework (JADE), GAMA, Matlab, Mathematical analysis.
	\end{itemize}
\end{itemize}

\subsection{MAS - App - Neural Networks for Continuous Online Learning and Control - 2006}
\href{https://ieeexplore.ieee.org/abstract/document/4012019}{MAS - App - Neural Networks for Continuous Online Learning and Control - 2006}

\subsection{MAS - Base - The Multiagent Planning Problem - 2016}
\href{https://www.hindawi.com/journals/complexity/2017/3813912/}{MAS - Base - The Multiagent Planning Problem - 2016}

\subsection{MAS - Con - Distributed Cooperative Control and Communication for Multi-agent Systems - 2021}
\href{https://link.springer.com/book/10.1007%2F978-981-33-6718-0}{MAS - Con - Distributed Cooperative Control and Communication for Multi-agent Systems - 2021}

\subsection{MAS - Con - PRIMA 2020 Principles and Practice of Multi-Agent Systems - 2021}
\href{https://link.springer.com/book/10.1007%2F978-3-030-69322-0}{MAS - Con - PRIMA 2020 Principles and Practice of Multi-Agent Systems - 2021}

\subsection{MAS - Con - Swarm Intelligence - 2010}
\href{https://link.springer.com/book/10.1007/978-3-642-15461-4}{MAS - Con - Swarm Intelligence - 2010}

\subsection{MAS - Con - Swarm Intelligence - 2012}
\href{https://link.springer.com/book/10.1007/978-3-642-32650-9}{MAS - Con - Swarm Intelligence - 2012}

\subsection{MAS - Con - Swarm Intelligence - 2014}
\href{https://link.springer.com/book/10.1007/978-3-319-09952-1}{MAS - Con - Swarm Intelligence - 2014}

\subsection{MAS - Con - Swarm Intelligence - 2016}
\href{https://link.springer.com/book/10.1007/978-3-319-44427-7}{MAS - Con - Swarm Intelligence - 2016}

\subsection{MAS - Con - Swarm Intelligence - 2018}
\href{https://link.springer.com/book/10.1007/978-3-030-00533-7}{MAS - Con - Swarm Intelligence - 2018}

\subsection{MAS - Con - Swarm Intelligence - 2020}
\href{https://link.springer.com/book/10.1007/978-3-030-60376-2}{MAS - Con - Swarm Intelligence - 2020}

\subsection{MAS - Evo - Co-evolutionary Multi-agent System with Predator-Prey Mechanism for Multi-objective Optimization - 2007}
\href{https://link.springer.com/chapter/10.1007/978-3-540-71618-1_8}{MAS - Evo - Co-evolutionary Multi-agent System with Predator-Prey Mechanism for Multi-objective Optimization - 2007}

\subsection{MAS - Het - Multiagent Systems A Survey from a Machine Learning Perspective - 2000}
\href{https://link.springer.com/article/10.1023/A:1008942012299}{MAS - Het - Multiagent Systems A Survey from a Machine Learning Perspective - 2000}

\subsection{MAS - Hie - Hierarchical Control in a Multiagent System - 2007}
\href{https://ieeexplore.ieee.org/abstract/document/4427756}{MAS - Hie - Hierarchical Control in a Multiagent System - 2007}

\subsection{MAS - Hie - Holonic - A Taxonomy of Autonomy in Multiagent Organisation - 2003}
\href{https://link.springer.com/chapter/10.1007/978-3-540-25928-2_6}{MAS - Hie - Holonic - A Taxonomy of Autonomy in Multiagent Organisation - 2003}



\subsection{MAS - Sur - A survey of multi-agent organizational paradigms - 2004}
\href{https://dl.acm.org/doi/abs/10.1017/S0269888905000317}{MAS - Sur - A survey of multi-agent organizational paradigms - 2004}

\subsection{MAS - Tra - Transfer Learning for Multi-agent Coordination - 2011}
\href{https://www.scitepress.org/Papers/2011/31856/}{MAS - Tra - Transfer Learning for Multi-agent Coordination - 2011}

\subsection{MAS - Tra - Transfer learning in multi-agent systems through paralllel transfer - 2013}
\href{https://ulir.ul.ie/handle/10344/3305}{MAS - Tra - Transfer learning in multi-agent systems through paralllel transfer - 2013}

\section{Game Theory}
\subsection{Artificial Intelligence - A modern Approach}
\underline{Game Theory (p.666)}
\begin{itemize}[noitemsep,nolistsep]
	\item perfect information = fully observable, imperfect information = partially observable.
	\item single-move games:
	\begin{itemize}[noitemsep,nolistsep]
		\item only one action can be chosen per game.
		\item \textbf{Payoff Matrix}: For 2 players it shows what action-tuple gives which reward. Also known as strategic/normal form.
		\item strategy = policy, pure strategy = deterministic policy, mixed strategy = probabilistic policy.
		\item \textbf{strategy profile}: assignment of strategy to each player. the game's aoutcome is then a numeric value for each player.
		\item \textbf{solution}: each player adopts a rational strategy.
		\item \textbf{dominant strategy}: a policy that is always the best.
		\item s \textbf{strongly dominates} s' for player p if the outcome of s is always better then s' for any strategy of any other player.
		\item s \textbf{weakly dominates} s' for player p if the outcome of s is atleast once better then s' and as good as s' for any strategy of any other player.
		\item Outcome is \textbf{pareto optimal} if there is no other outcome that all players would prefer.
		\item Outcome is \textbf{pareto dominated} by another outcome if all players would prefer the other outcome.
		\item Every player has a dominant strategy, then all the strategies are called an \textbf{dominant strategy equilibrium}. Which is generally formed if no player can benefit by switching strategies.
		\item equilibrium = local optimum. Every game has at least one equilibrium an Nash equilibrium.
		\item dominant strategy equilibrium is a nash equilibrium. Some games have nash equilibria but no dominant strategies.
		\item repeated game: multiple runs.
		\item multiple acceptable solutions: use the unique pareto-optimal nash equilibrium if it exists. Every game has at least one.
		\item coordination game: communication so players can negotiate before a game the soltuons they take to be mutually beneficial.
		\item zero-sum game: the sum of the payoffs for all players is zero or a constant. To solve these take one player as the maximizer => maximin technique.
		\item zero-sum games have maximin nash equilibria. Every zero-sum game has a maximin equilbrium when you allow mixed strategies.
		\item approach for finding equilibria in non-zero-sum games:
		\begin{itemize}[noitemsep,nolistsep]
			\item 1. Enumerate all possible subset of actions that might form mixed strategies. This is exponential in the number of action.
			\item 2. Foreach strategy enumerated in 1. check to see if it is an equilibrium.
		\end{itemize}
	\end{itemize}
	\item repeated games
	\begin{itemize}[noitemsep,nolistsep]
		\item repeated game: players face the same choices repeatedly, but each tim ewith knowledge of the history ofa ll player's previous choices.
		\item So the game is played multiple round. But the last one has nothing to influence so it used the dominant strategy for single-move game. Then the second to last has nothing to influence => induction => just play the same as single-move => not optimal.
		\item Use a chance so the players do not know when the game will end.
		\item if the agents cannot store the entire history they do not know when the game will end.
	\end{itemize}
	\item sequential games
	\begin{itemize}[noitemsep,nolistsep]
		\item sequence of turns that need not be all the same. Represented by a game tree called the extensive form.
		\item non-deterministic actions can be created by having the player's action be deterministic and then another action that is randomly chosen.
		\item There could be a chance player that acts randomly to introduce distributions.
		\item perfect recall: each player remembers all their own previous actions.
		\item extensive form allows to always find solutions beause it represents the belief states of all players at once. Which is important if you strategy depends on the other players strategies.
		\item extensive games can be converted to a normal-form game to solve it. By having using all possible state history combinations for the other player in the payoff matrix (does not scale well). This can usally be solved with linear programming.
		\item alpha-beta pruning works good for large game trees but does not work well for imperfect information.
		\item Alternative: sequence form: Is linear in the soze of the tree. it repsresnts not strategies in a node but paths through the tree which scales in the amount of possible endstates.
		\item use feature spaces as abstractions of a game to create a smaller tree.
		\item just using the equilibrium strategy gives you the perfect solution if the other players also use the equilibrium strategy. If the other player makes mistake you need to capitalize on that.
	\end{itemize}
	\item game theory does not deal with with continuous states and actions. 
	\item cournot competition is an extension that can halb in the contiuous space.
	\item game theory assumes the game is known. if the actions are not known beforehand or the other players are not fully rational.
	\item This can be olved with a Bayes-Nash equilibrium that expresses a player's belief about the other player's likely strategies.
\end{itemize}

\underline{Stochastic Games (p.177)}
This is more about game trees and alpha-beta pruning so technically not applicable for my work.
\underline{Partial Observable Games (p.180)}
This is more about game trees and alpha-beta pruning so technically not applicable for my work.

\section{Multi-Agent Reinforcement Learning}
\subsection{MARL - A Comprehensive Survey of Multiagent Reinforcement Learning - 2008}
\href{https://ieeexplore.ieee.org/abstract/document/4445757}{MARL - A Comprehensive Survey of Multiagent Reinforcement Learning - 2008}
\\
\underline{Benefits}
\begin{itemize}[noitemsep,nolistsep]
	\item can be parallelized.
	\item can use experience sharing via communication, or with a teacher-learner relationship.
	\item Failure of one agent can be covered by other agents.
	\item insertion of new agents => scaleable.
	\item MARL Complexity: Exponential in number of agents.
	\item exploration (new knowledge) - exploitation (current knowledge) - Tradeoff.
	\item They explore about the environment and other agents.
	\item need for coordination.
\end{itemize}
\underline{Application Domains}
\begin{itemize}[noitemsep,nolistsep]
	\item simulation better than real-life (better scalability and robustness).
	\item Distributed Control: for controlling processes (for larger industry plants).
	\begin{itemize}[noitemsep,nolistsep]
		\item avenue for future work.
		\item used for traffic, power or sensory networks.
		\item could also be used for pendulum systems.
	\end{itemize}
	\item Robotic Teams (Multirobot):
	\begin{itemize}[noitemsep,nolistsep]
		\item simulated 2D space. 
		\item navigation: Reach a goal with obstacles. Area sweeping (retrival of objects (also cooperative)).
		\item pursuit: Capture a prey robot.
	\end{itemize}
	\item Automated Trading: Exchange goods on electronic markets with negotiation and auctions.
	\item Resource Management: Cooperatie team to manage resources or as clients. (routing, load balancing).
\end{itemize}
\underline{Practicallity and Future works}
\begin{itemize}[noitemsep,nolistsep]
	\item Scalability Problem: Q-functions do not scale well with the size of the state-action space.
	\begin{itemize}[noitemsep,nolistsep]
		\item Approximation needed: for discrete large state-action spaces, for continous states and discrete actions or continious state and action.
		\item Heuristic in nature and only work in a narrow set of problems.
		\item Could use theoretical results on single-agent approximate RL.
		\item also could use discovery and exploitation of the decentralized, modular structure of the multiagent task.
	\end{itemize}
	\item MARL without prior knowledge is very slow.
	\begin{itemize}[noitemsep,nolistsep]
		\item Need humans to teach the agent.
		\item shaping: first simple task then scale them.
		\item could use reflex behavior.
		\item Knowledge about the task structure.
	\end{itemize}
	\item Incomplete, uncertain state measurements could be handled with partiall observability techniques (Markov decision process).
	\item Multiagent Goals needs a stable learning process for the environment and an adaption for the dynamics of other agents.
	\item using game-theory-based analysis to apply to the dynamics of the environment.
\end{itemize}

\subsection{MARL - AC - Networked Multi-Agent Reinforcement Learning in Continuous Spaces - 2018}
\href{https://ieeexplore.ieee.org/abstract/document/8619581}{MARL - AC - Networked Multi-Agent Reinforcement Learning in Continuous Spaces - 2018}

\subsection{MARL - AC - Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environment - 2017}
\href{https://arxiv.org/abs/1706.02275}{MARL - AC - Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environment - 2017}

\subsection{MARL - AC - Actor-Attention-Critic for Multi-Agent Reinforcement Learning - 2019}
\href{http://proceedings.mlr.press/v97/iqbal19a.html}{MARL - AC - Actor-Attention-Critic for Multi-Agent Reinforcement Learning - 2019}

\subsection{MARL - Base - Multiagent Reinforcement Learning - Theoretical Framework and an Algorithm - 1998}
\href{https://dl.acm.org/doi/abs/10.5555/645527.657296}{MARL - Base - Multiagent Reinforcement Learning - Theoretical Framework and an Algorithm - 1998}

\subsection{MARL - Base - Deep Reinforcement Learning for Robot Swarms - 2019 - KIT}
\href{https://publikationen.bibliothek.kit.edu/1000118251}{MARL - Base - Deep Reinforcement Learning for Robot Swarms - 2019 - KIT}

\subsection{MARL - Base - PettingZoo - Gym for Multi-Agent Reinforcement Learning - 2020}
\href{https://arxiv.org/abs/2009.14471}{MARL - Base - PettingZoo - Gym for Multi-Agent Reinforcement Learning - 2020}

\subsection{MARL - Base - Multi-agent reinforcement learning weighting and partitioning - 1999}
\href{https://www.sciencedirect.com/science/article/pii/S0893608099000246}{MARL - Base - Multi-agent reinforcement learning weighting and partitioning - 1999}

\subsection{MARL - Com - Learning to Communicate with Deep Multi-Agent Reinforcement Learning - 2016}
\href{https://arxiv.org/abs/1605.06676}{MARL - Com - Learning to Communicate with Deep Multi-Agent Reinforcement Learning - 2016}

\subsection{MARL - Com - Coordinating multi-agent reinforcement learning with limited communication - 2013}
\href{https://dl.acm.org/doi/abs/10.5555/2484920.2485093}{MARL - Com - Coordinating multi-agent reinforcement learning with limited communication - 2013}

\subsection{MARL - Het - LIIR - Learning Individual Intrinsic Reward inMulti-Agent Reinforcement Learning - 2019}
\href{https://proceedings.neurips.cc/paper/2019/hash/07a9d3fed4c5ea6b17e80258dee231fa-Abstract.html}{MARL - Het - LIIR - Learning Individual Intrinsic Reward inMulti-Agent Reinforcement Learning - 2019}

\subsection{MARL - Het - An approach to the pursuit problem on a heterogeneous multiagent system using reinforcement learning - 2002}
\href{https://www.sciencedirect.com/science/article/abs/pii/S092188900300040X}{MARL - Het - An approach to the pursuit problem on a heterogeneous multiagent system using reinforcement learning - 2002}

\subsection{MARL - Hie - Hierarchical multi-agent reinforcement learning - 2006}
\href{https://link.springer.com/article/10.1007/s10458-006-7035-4}{MARL - Hie - Hierarchical multi-agent reinforcement learning - 2006}

\subsection{MARL - MO - Reward shaping for knowledge-based multi-objective multi-agent reinforcement learning - 2017}
\href{https://www.cambridge.org/core/journals/knowledge-engineering-review/article/reward-shaping-for-knowledgebased-multiobjective-multiagent-reinforcement-learning/75F1507F7CAC7C6625F87AE7CD344D52}{MARL - MO - Reward shaping for knowledge-based multi-objective multi-agent reinforcement learning - 2017}

\subsection{MARL - Role - ROMA Multi-Agent Reinforcement Learning with Emergent Roles - 2020}
\href{http://proceedings.mlr.press/v119/wang20f.html}{MARL - Role - ROMA Multi-Agent Reinforcement Learning with Emergent Roles - 2020}

\subsection{MARL - Sca - GAMA - Graph Attention Multi-agent reinforcement learning algorithm for cooperation - 2020}
\href{https://link.springer.com/article/10.1007/s10489-020-01755-8}{MARL - Sca - GAMA - Graph Attention Multi-agent reinforcement learning algorithm for cooperation - 2020}
\subsection{MARL - Sca - Heuristically-Accelerated Multiagent - 2014}
\href{https://ieeexplore.ieee.org/abstract/document/6502216}{MMARL - Sca - Heuristically-Accelerated Multiagent - 2014}



\subsection{MARL - Sca - Plan-based reward shaping for multi-agent reinforcement learning - 2016}
\href{https://www.cambridge.org/core/journals/knowledge-engineering-review/article/planbased-reward-shaping-for-multiagent-reinforcement-learning/B173D25B1006B755667616C3A3EB3BE5}{MARL - Sca - Plan-based reward shaping for multi-agent reinforcement learning - 2016}

\subsection{MARL - Sca - Multi-Agent Reinforcement Learning Using Linear Fuzzy Model Applied to Cooperative Mobile Robots - 2018}
\href{https://www.mdpi.com/2073-8994/10/10/461}{MARL - Sca - Multi-Agent Reinforcement Learning Using Linear Fuzzy Model Applied to Cooperative Mobile Robots - 2018}

\subsection{MARL - Sca - Stabilising Experience Replay for Deep Multi-Agent Reinforcement Learning - 2017}
\href{http://proceedings.mlr.press/v70/foerster17b.html}{MARL - Sca - Stabilising Experience Replay for Deep Multi-Agent Reinforcement Learning - 2017}

\subsection{MARL - Sca - Mean Field Multi-Agent Reinforcement Learning - 2018}
\href{http://proceedings.mlr.press/v80/yang18d.html}{MARL - Sca - Mean Field Multi-Agent Reinforcement Learning - 2018}

\subsection{MARL - Sca - A modular approach to multi-agent reinforcement learning - 2005}
\href{https://link.springer.com/chapter/10.1007/3-540-62934-3_39}{MARL - Sca - A modular approach to multi-agent reinforcement learning - 2005}

\subsection{MARL - Sur - Multi-Agent Reinforcement Learning - a critical survey - 2003}
\href{http://ai.stanford.edu/people/shoham/www%20papers/MALearning_ACriticalSurvey_2003_0516.pdf}{MARL - Sur - Multi-Agent Reinforcement Learning - a critical survey - 2003}

\subsection{MARL - Sur - Multi-Agent Reinforcement Learning A Selective Overview of Theories and Algorithms - 2021}
\href{https://arxiv.org/abs/1911.10635}{MARL - Sur - Multi-Agent Reinforcement Learning A Selective Overview of Theories and Algorithms - 2021}

\subsection{MARL - Sur - Multi-Agent Reinforcement Learning A Report on Challenges and Approaches - 2018}
\href{https://arxiv.org/abs/1807.09427}{MARL - Sur - Multi-Agent Reinforcement Learning A Report on Challenges and Approaches - 2018}

\subsection{MARL - Sur - A Review of Cooperative Multi-Agent Deep Reinforcement Learning - 2019}
\href{https://arxiv.org/abs/1908.03963}{MARL - Sur - A Review of Cooperative Multi-Agent Deep Reinforcement Learning - 2019}

\subsection{MARL - Sur - A Survey on Transfer Learning for Multiagent Reinforcement Learning Systems  - 2019}
\href{https://www.jair.org/index.php/jair/article/view/11396}{MARL - Sur - A Survey on Transfer Learning for Multiagent Reinforcement Learning Systems  - 2019}

\subsection{MARL - Tra - Transfer Learning in Multi-agent ReinforcementLearning Domains - 2011}
\href{https://link.springer.com/chapter/10.1007/978-3-642-29946-9_25}{MARL - Tra - Transfer Learning in Multi-agent ReinforcementLearning Domains - 2011}

\subsection{MARL - Tra - Parallel Transfer Learning in Multi-Agent Systems What, when and how to transfer - 2019}
\href{https://ieeexplore.ieee.org/abstract/document/8851784}{MARL - Tra - Parallel Transfer Learning in Multi-Agent Systems What, when and how to transfer - 2019}

\subsection{MARL - Tra - Transfer among Agents An Efficient Multiagent Transfer Learning Framework - 2020}
\href{https://arxiv.org/abs/2002.08030}{MARL - Tra - Transfer among Agents An Efficient Multiagent Transfer Learning Framework - 2020}

\subsection{MARL - Tra - Agents teaching agents a survey on inter-agent transfer learning - 2019}
\href{https://link.springer.com/article/10.1007/s10458-019-09430-0}{MARL - Tra - Agents teaching agents a survey on inter-agent transfer learning - 2019}

\section{Graph Neural Networks}
\subsection{!Theoretical Foundations of Graph Neural Networks - 2021}
\href{https://www.youtube.com/watch?v=uF53xsT7mjc}{Theoretical Foundations of Graph Neural Networks - 2021}
\begin{itemize}[noitemsep,nolistsep]
	\item Goal: Exact same results for two \textbf{isomorphic graphs} (graphs that are the same the nodes are just arranged differently).
	\item Nodes: $x_i \in \mathbb{R}^k$ (features of node), feature matrix $\mathbf{X} = (x_1,...x_n)^T \in \mathbb{R}^{k \times n}$
	\item By stacking the nodes in the matrix you have already ordered them (result should not depend on this).
	\item Operations that change the nod order: permutation matrices. They ahve exactly one 1 in every row and column, and zeroes everywhere examples. Left Multiplied: permute the rows. $P_{(2,4,1,3)}$: The numbers indicate where the 1 in the row is.
	\item \textbf{Permutation Invariance}: f is permutation invariant iff: $\forall P \in Permutation: f(PX) = f(X)$. Example: Deep Sets Model $f(X) = \phi(\sum_{i \in V} \psi(x_i))$. This is true for the entire data-set.
	\item \textbf{Permutation equivariance}: for identification on the node level. Seek functions that don't change the node order. f is permutation equivariant iff: $\forall P \in Permutation: f(PX) = Pf(X)$.
	\item \textbf{equivariance}: each node's row is unchanged by f. So foreach node we could define: $\forall i: h_i = \psi(x_i).$, the latent features h-i. Stacking h yields: $H=f(x)$. The functions are applied indipendently to each node.
	\item Stacking equivarent functions with an invariant tail: $f(X) = \phi(\bigoplus_{i \in V}\psi(x_i)). \bigoplus\ is\ permutationinvariant\ aggregator\ (sum,\ avg,\ max)$.
	\item \textbf{Learning on Graphs}:
	\begin{itemize}[noitemsep,nolistsep]
		\item Represent Edges with adjacency matrix A: $a_{ij} = \begin{cases} 1& (i,j) \in E \\ 0 & otherwise \end{cases}$. Edge features could be added aswell. permutation x-variance still holds.
		\item xvariance on graphs: To not change edges: permutate rows and columns. Permutate with $PAP^T$.
		\item \textbf{Invariance}: $f(PX,PAP^T) = f(X,A)\ (A = Edges,\ X = Nodes)$
		\item \textbf{Exvariance}: $f(PX,PAP^T) = Pf(X,A)\ (A = Edges,\ X = Nodes)$
		\item Neighbourhoods: Node i, it's 1-hop neighbors are defined as: $\mathcal{N}_i = \{j: (i,j) \in E \vee (j,i) \in E\}$. (Non-directed edges, node i is in it's own neighbourhood).
		\item Multiset of features in the neighbourhood: $X_{\mathcal{N}_i} = \{\{ x_j : j \in \mathcal{N}_i \}\}$. With a local function g as operating over this multiset: $g(x_i, X_{N_i})$
		\item Construct perm-equi function f(X,A) by applying g over all neighbourhoods: $f(\textbf{X,A}) = \begin{pmatrix} g(x_1,X_{\mathcal{N}_1}) \\ g(x_2,X_{\mathcal{N}_1}) \\ \vdots \\ g(x_n,X_{\mathcal{N}_n}) \end{pmatrix}$. g should not depend on the order of the neighbourhood, it should be permu-invari.
		\item Once you have the latent-Graph via the GNN you can use them in a Node-classification,  Graph-classification, or Link-prediction task.
	\end{itemize}
	\item Message Passing in Graphs.
	\begin{itemize}[noitemsep,nolistsep]
		\item GNN Layer: Construct f(X,A) via the local function g (known as diffusion, propagation or message passing). F is refered ta as a GNN layer.
		\item How to define g? Active research!
		\item Classification thre flavoulrs of CNN:
		\item Convolutional GNN:
		\begin{itemize}[noitemsep,nolistsep]
			\item constants $c_{ij}$. How much does Node i value the features of nodes j. They are coefficients for weighted combinations. The weights usually depend on A.
			\item $h_i = \phi (x_i, \bigoplus_{j \in \mathcal{N}_i} c_{i,j}\psi(x_j))$. 
			\item Examples: ChebyNet, GCN (Graph Convolutional Network), SGC (Simplified Graph Convolutional Networks)
			\item useful for homophilous graphs (similar edges) and scales well.
		\end{itemize}
		\item Attentional GNN:
		\begin{itemize}[noitemsep,nolistsep]
			\item neighbour features aggregated with implicit weights (via attention a). This weights are learnable.
			\item $h_i = \phi (x_i, \bigoplus_{j \in \mathcal{N}_i} a(x_i,x_j)\psi(x_j))$. 
			\item Examples: MoNet, GAT (Graph Attention Network), GaAN (Gated Attention Network).
			\item useful as a middle ground with respect to capacity and scale. Edges are not strict homophily, but you compute sclarar value in each edge.
		\end{itemize}
		\item Message Passing GNN:
		\begin{itemize}[noitemsep,nolistsep]
			\item sender and receiver work together to compute arbitrary vectors ("messages") to be sent across edges.
			\item $h_i = \phi (x_i, \bigoplus_{j \in \mathcal{N}_i} \psi(x_i, x_j)). \psi(x_i, x_j)) = m_{ij}$.  
			\item Examples: Interaction Networks, MPNN (Message Passing Neural Networks), GraphNets
			\item most generic GNN. May have scalability or learnability issues. Ideal for reasoning.
		\end{itemize}
	\end{itemize}
	\item Node embedding techniques:
	\begin{itemize}[noitemsep,nolistsep]
		\item embedding: Finding an Encoding, so that x-i are now the latent features of h-i.
		\item a good representation should preserve the graph structure. This leads to the unsupervised objective: $optimise\ h_i\ and\ h_j\ to\ be\ nearby\ iff: (i,j) \in E$. They predict if there is an edge between the nodes.
		\item Can use binary cross-entropy loss: $\sum_{(i,j) \in E}log \sigma(h_i^Th_j) + \sum_{(i,j) \notin E}log (1 - \sigma(h_i^Th_j))$
	\end{itemize}
	\item local objective emulate Convolutional GNNs. Neighbouring nodes tend to highly overlap in n-step neighborhoods. A conv-GNN enforces similar features for neighbouring nodes by design.
	\item GNN and Natural Language Processing (NLP) correspond alot (nodes similar to words).
	\item Common assumption if you have no information about how the graph should look like: Assume a complete graph and then let the network infer the actual relations.
	\item Transformers: are fully connected attentional GNNs.
	\item Spectral GNNs:
	\begin{itemize}[noitemsep,nolistsep]
		\item Time Sequences can be imageind as a cyclical grid graph with a convolution over it. A node is a time-step and the convolution looks at the time step and it's immediate neighbors.
		\item You don't need to know the convolutional operation if you know the eigenvalues with respect to the fourier basis (36:13)
		\item convolutional GNN: $c_{ij} = (p_k(L))_{ij}$. Use a polynomial function p-k for the Laplacian matrix L = D - A. D being the Degree matrix. p-k is necessary to make the eigenvalue decomposition easier.
		\item This means there is no spectral GNN and spatial GNN as they can be transformed into each other.
	\end{itemize}
	\item Probabilistic Graphical Models:
	\begin{itemize}[noitemsep,nolistsep]
		\item Nodes are random variables and edges are dependencies between their distributions. THis is an Probabilistic graphical Model (PGMs). This helps you decompose a joint probability distribution.
		\item Can use Markov Random Fields (MRF) to decompose the joint into a product of edge potentials.
		\item Mean-field inference.
		\item PGM corresponds to a message passing GNN.
	\end{itemize}
	\item how powerful are GNNs?
	\begin{itemize}[noitemsep,nolistsep]
		\item untrained GNNs work well, as they are similar to random hashes. (Weisfeiler-Lemann Test). Also called 1-WL test.
		\item Though some instances the isomorphism test fails.
		\item GNNs can only be as powerful as the 1-WL test.
		\item Can make the strongers by analysing failer cases.
		\item Continous Features: Sums may not destinguish parts of the graph (2+2 = 4+0).
	\end{itemize}
	\item \href{https://youtu.be/uF53xsT7mjc?t=2943}{curr}
\end{itemize}

\subsection{Probabilistic Deep Learning Book}
\href{https://probml.github.io/pml-book/book1.html}{Probabilistic Deep Learning Book}
\begin{itemize}[noitemsep,nolistsep]
	\item Message passing GNNs
	\item Spectral Graph Convolutions:
	\item Spatial Graph Convolutions:
	\item Non-euclidean graph Convoltuons:
\end{itemize}

\subsection{!Geometric Deep Learning: The Erlangen Programme of ML}
\href{https://www.youtube.com/watch?v=w6Pw4MOzMuo}{Geometric Deep Learning: The Erlangen Programme of ML Youtube}
\begin{itemize}[noitemsep,nolistsep]
	\item Zoo of Deep Learning Architecture for different kinds of data: CNN, GNN, Transformer, Deep Sets, RNN
	\item Perceptron neural network: Just 2 layer. Let's you approximate any function arbitrarly exakt. Universal Approximation theorem.
	\item Number of samples grows exponentially $O(\epsilon^{-d})\ samples$: Curse of dimensionality.
	\item Example with Image processing: If you just put the data in a vector, then shifting the image (a number) needs to be learned. The NN needs to learn shift invariance.
	\item Better: CNNs. Use the Weight of nearby pixels. Solved the curse of dimensionality.
	\item We want to work with networks like molecules and social networks that don't change their meaning when reordered. But they do if you just squish them into a simple vector.
	\item Using geometric priors (???). Symmetry in the input signal defined in a domain is preserfed through a group representation (shift operator). The geometric structure therefore imposes structure on the class of functios to learn. 
	\item \textbf{Invariant function:} $f(p(g)x)=f(x)$, (p(g) is the shift).
	\item So for image classification an invariant might be the actual position of the cat in an image. This would be called shift invariance, that is already embedded in the structure and does not need to be learned with the NN.
	\item \textbf{Equivarient function:} $f(p(g)x) = p(g)f(x)$: The function has the same input and output structure. The output could be a pixel-wise mask. The output should transformed like the Input.
	\item \textbf{Scale seperation:} Scale down the Input with a coarse graining P. We can thann use the scaled input $\tilde{x}$ with a coarse scaled function $\tilde{f}$. Function f is locally stable if it can be approximated by $f \approx \tilde{f} \circ P$.
	\item Normally f would interact with long-range aspects, but we can focus the function on a particular Range with coarse graining to focus on parts of the function.
	\item can use multiple equivariant layers with a single invariant layer as a single output. Can create a hierarchy by using the coarsing procedure that can take the form of local pooling in NN implementations.
	\item These can work well for Structures like Grids, Groups, Graphs, Geodesics and Gauges (5G of Geometric Deep Learning).
	\item \textbf{Deep Learning Techniques}:
	\begin{itemize}[noitemsep,nolistsep]
		\item The implementaion of these principles can lead to Perceptrons (Function regularity), CNNs (Translation), Group-Cnns (Translation + Rotation), GNNs (Permutation), DeepSets/Transformers (Permutation), Intrinsic CNNs (Isometry / Gauge choice).
	\end{itemize}
	\item Nodes in a GNN can have d-dimensional features $x_i$.
	\item Graphs are not ordered, but by using Feature matrix $X$ and an Adjacency matrix $A$ we impose an ordering of the nodes. 
	\item Changing the Ordering needs to perumtate the Matrices: $PAP^T$ and $PX$. the rows and columns will be permuted.
	\item The output might have to be unneffected by the ordering of the input: \textbf{permutation invariant}: $f(PX, PAP^T) = f(X,A)$.
	\item If we want to make node-wide predictions we need \textbf{permutation equivariance}:  $f(PX, PAP^T) = Pf(X,A)$.
	\item Create a feature vector, where the first data is a node and the other ones are all the neighbors: $\phi(x_i, X_{N_i}) X_{N_i} are\ the\ neighbors$. This must be permutation invariant. If we stack those phi into a feature matrix $F(X,A) = (\phi(x_1, X_{N_1}), \phi(x_i, X_{N_i}), \dots \phi(x_n, X_{N_n}))$ F is permuation equivariant.
	\item When $\phi$ is injective: The NN is equivalent to the Weisfeiler-Lehmann test. To Test if a Graph is isomorphic. This test can only show if two graphs are not isomorphic. But cannot show that it is isomorphic.
	\item Types of GNNs:
	\begin{itemize}[noitemsep,nolistsep]
		\item Form for the local aggreation function: $f(x_i) = \phi(x_i, aggr_{j \in N_i} \psi(x_j)).$ aggr is permutation invariant aggregation operator (like summ). phi and psi are learnable functions. psi transforms the neighbor features and phi updates the features of node i by the aggregated features of its neighbors.
		\item \textbf{Convolutional:} $f(x_i) = \phi(x_i, aggr_{j \in N_i} c_{ij} \psi(x_j)).$ Weight the features that with $c_{ij}$ that only depends on the structure of the graph. The importance of node j to the reresentation of node i.
		\item \textbf{Attentional:}  $f(x_i) = \phi(x_i, aggr_{j \in N_i} a(x_i,x_j) \psi(x_j)).$ The aggregation coefficient now depend on the features itselves. 
		\item \textbf{Message Passing:} $f(x_i) = \phi(x_i, aggr_{j \in N_i} \psi(x_i,x_j)).$. A kind of attentional flavor.
	\end{itemize}
	\item GNN Structure: Multiple Permutation-equivariant layers and function Tha than get aggregated and pooled to one output $\phi()$ pooling. Some architectures use some form of local pooling coarsening that can be learnable.
	\item Cases:
	\begin{itemize}[noitemsep,nolistsep]
		\item Graph with no edges: Set. Also unordered. Easiest approach: process each element individually: $\phi(x_i)$. This translates to a permutation equivariant function over the set. Architecture known as DeepSets or PointNets
		\item Fully connected graph: convolutional variety does not work. as the weights would the same over all nodes. Use Attentional. This is simular as a transformer that is used in language processing.
		\item \textbf{Graph positional encoding}: So that the position is fixed $\phi(x_i, aggr_{j \in N_i} \psi(x_i, x_j, p_i)).$ This architecture is called graph substructure network. A way to introduce inductive bias: like in chemistry 5-6-sided-rings are the most prominent structures. 
		\item \textbf{Decouple computational graph}: The Graph you do computations on is not the same graph as the one yuo get es input. Can do graph sampling (scaling issues), rewiring, multi-hop filters (aggregation on  the neighbors of the neighbors). 
		\item \textbf{Dynamic Graph CNN}: Can learn the graph that we want to do computations on, called \textbf{latent graph learning}. This can be related to manifold learning or nonlinear dimensionality reduction. The Datapoints while on high dimension are actually more strongly correlated in a shape that has lower dimensions.
		\item \textbf{Convolution}: grid = line of nodes connected to only two neighbors. The last one can be connected to the first one, a ring graph. Fixed neighborhoods. Even ordering is fixed. $f(x_i) = \phi(x_{i-1},x_i,x_{i+1})$. Using a linear function (weighted sum) you will get a classical convolution. In a matrix form you will get the circulant matrix that has the shared weights of CNNs. Circulant Matrixes commutes with shift. In other words convolution is shift-equivariant. Convolution automatically emerges from translation symmetry of GNNs. Furthermore the Eigenvector. 
		\item There is a relation between convolution in the spatial domain and in the frequency domain donated by the fourier matrices. This is called the convolution theorem
		\item \textbf{Groups}: ??? Something about Learning on Manifolds. 24:00 - 30:49
	\end{itemize}
\end{itemize}

\subsection{!GNN - Sur - A Comprehensive Survey on Graph Neural Networks - 2019}
\href{https://arxiv.org/abs/1901.00596}{GNN - Sur - A Comprehensive Survey on Graph Neural Networks - 2019}
\begin{itemize}[noitemsep,nolistsep]
	\item Definitions:
	\begin{itemize}[noitemsep,nolistsep]
		\item Graph: Node features $x_j$, edge features $x^e_{v,u}$.
		\item \textbf{Spatial-Temporal Graph}: an attributed graph where the node attributes change dynamically over time $G^{(t)} = (V,E,X^{(t)}),\ X^{(t)} \in \mathbb{R}^{n \times d}$
	\end{itemize}
	\item Taxonomy:
	\begin{itemize}[noitemsep,nolistsep]
		\item TABLE OF TAXONOMY AND PUBLICATIONS USING THEM (p.4)
		\item \textbf{Recurrent graph neural networks (RecGNNs)}: learn node representatin with recurrent neural architectures. A node constantly exchanges mesages with its neighbors until stable equilbrium is reached.
		\item \textbf{Convolutional graph neural networks (ConvGNNs)}: generalize convolution opreation. Aggregate its own featues with the neighbors features. ConvGNNs stack multiple convolutional layers to extract high-level node representations.
		\item \textbf{Graph autoencodes (GAEs)}: Encode node/graphs into a latent vector space and reconstruct graph data from the encode information. Generate these graphs.
		\item \textbf{Spatial-temporal graph neural networks (STGNNs)}: learn hidden patterns from spatial-temporal graphs. Consider spatial dependency.
	\end{itemize}
	\item Frameworks:
	\begin{itemize}[noitemsep,nolistsep]
		\item \textbf{Node-level output}: relates to node regression and node classification tasks. Using RecGNNs and ConvGNNS and a sofmax layer as the output.
		\item \textbf{Edge-level output}: relates to edge classification and link prediction tasks. 
		\item \textbf{Graph-level output}: outputs relate to the graph classification task.
		\item \textbf{Semi-supervised learning for node-level classification}: Network with partial nodes being labeled, ConvGNNs can learn what the other labels should be. Stack GConv and softmax output.
		\item \textbf{Supervised learning for graph-level classification}: Predict the class labels for the entire graph. Can be realized with Graph convolutional layers, graph pooling layers and readout layers.
		\item \textbf{Unsupervised learning for graph embeddings}: No class labels are available. These algorithms exploit edge-level informations
	\end{itemize}
	\item Recurrent Graph Neural Networks:
	\begin{itemize}[noitemsep,nolistsep]
		\item \textbf{GNN}:
		\begin{itemize}[noitemsep,nolistsep]
			\item Apply same set of parameters recurrently over nodes in a graph to extract high-level node representations. Can handle different types of graphs (cyclic, acyclic, direct, undirected).
			\item Update $h_v^{(t)} = \sum_{u \in N(v)} f(x_v, x^e_{(v,u)}, x_u h_u^{(t-1)})$. h are the hidden layers. $h_v^{(0)}$ is initialized randomly.
			\item The sum allows the GNN to be applicable to all nodes. To converge f must be a contraction mapping, which shrinks the diestance between two points after projecting them into a latent space.
		\end{itemize}
		\item \textbf{Gated Graph Neural network (GGNN)}:
		\begin{itemize}[noitemsep,nolistsep]
			\item Uses a gated recurrent unit (GRU). This does not need to constrain parameters to ensure convergence.
			\item Update: $h_v^{(t)} = GRU(h_v^{(t-1)}, \sum_{u \in N(v)} \mathbf{W} h_u^{(t-1)} ),\  h_v^{(0)} = x_v$.
			\item GGNN uses back-propagation through time in contrast to GNN and GraphESN. This can be problematic for large graphs.
		\end{itemize}
		\item \textbf{Stochastic Steady-state Embedding (SSE)}:
		\begin{itemize}[noitemsep,nolistsep]
			\item scales well to large graph. Node's hidden states are updated recurrently in a stochastic and asynchronous fashion. It alternateively samples a batch of nodes for state update and a batch of nodes for gradient computation.
			\item Update: $h_v^{(t)} = (1 - \alpha)h_v^{(t-1)} + \alpha \mathbf{W}_1 \sigma(\mathbf{W}_2 [x_v, \sum_{u \in N(v)} [h_u^{(t-1)}, x_u]])$. $h_v^{(0)}$ is initialized randomly.
		\end{itemize}
	\end{itemize}
	\item Convolutional Graph Neural Networks:
	\begin{itemize}[noitemsep,nolistsep]
		\item \textbf{ConvGNNs}: Usaes a fixed number of layers with different weights for graph convolutions.
		\item \textbf{Spectral-based ConvGNNs}: 
		\begin{itemize}[noitemsep,nolistsep]
			\item Define graph convolutions by introducing filter from the perspective of graph signal processing, where convolution is interpretaed as removing noise.
			\item TO-DO! p.7-8
		\end{itemize}
		\item \textbf{Spatial-based ConvGNNs}: 
		\begin{itemize}[noitemsep,nolistsep]
			\item Define graph convolutions by information propagation.
			\item TO-DO! p.8-10
		\end{itemize}	
		\item \textbf{Graph Pooling Modules}:
		\begin{itemize}[noitemsep,nolistsep]
			\item Directly using the node features can be computational challenging so we need to down-sample them. This operation can be called pooling operation or readout operation (different naming conventions). These use graph coarsening algorithms.
			\item mean/max/sum pooling is the most primitive but effective way to implement down-sampling.
		\end{itemize}
		\item \textbf{Discussion of Theoretical Aspects:}
		\begin{itemize}[noitemsep,nolistsep]
			\item \textbf{shape of receptive field}: It is the set of nodes that contribute to the determination of one nodes final node representation. When compositing multiple spatial graph convolutional layers, the receptive field of a node grows one step ahead towards its distant neighbors each time. It can be proven that a ConvGNN is able to extract global information by stacking local graph convolution layers.
			\item \textbf{VC Dimension}: Measure of model complexity. The largest number of points tha can be shattered by a model.p model parameters and n nodes it follows that the VC Dimesion is $O(p^4n^2)$ for sigmoid or tanget hyperbolicy activation and $O(p^4n^2)$ for piecewise polynomial activation function.
			\item \textbf{Graph isomorphism}: See Geometric Deep Learning: The Erlangen Programme of ML
			\item \textbf{Equivariance and invariance}: Geometric Deep Learning: The Erlangen Programme of ML
			\item \textbf{Universal approximation}: The universal approximation capability (the theorem) of GNNs hs seldem been studied. 
		\end{itemize}
	\end{itemize}
	\item Graph autoencoders:
	\begin{itemize}[noitemsep,nolistsep]
		\item Network Embedding:
		\begin{itemize}[noitemsep,nolistsep]
			\item TO-DO: p.12-14
		\end{itemize}
		\item Graph Generation:
		\begin{itemize}[noitemsep,nolistsep]
			\item TO-DO: p.14
		\end{itemize}
	\end{itemize}
	\item Spatial-temporal Graph Neural Networks:
	\begin{itemize}[noitemsep,nolistsep]
		\item TO-DO: p.15
	\end{itemize}
\end{itemize}

\subsection{GNN - Base - Learning Decentralized Controllers for Robot Swarms with Graph Neural Networks - 2019}
\href{https://arxiv.org/abs/1903.10527}{GNN - Base - Learning Decentralized Controllers for Robot Swarms with Graph Neural Networks - 2019}

\subsection{GNN - Arch - Exploiting Edge Features in Graph Neural Networks - 2018}
\href{https://arxiv.org/abs/1809.02709}{GNN - Arch - Exploiting Edge Features in Graph Neural Networks - 2018}

\subsection{GNN - Base - Graph Convolutional Reinforcement Learning - 2018}
\href{https://arxiv.org/abs/1810.09202}{GNN - Base - Graph Convolutional Reinforcement Learning - 2018}

\subsection{GNN - App - Optimizing Large-Scale Fleet Management on a Road Network using Multi-Agent Deep Reinforcement Learning with Graph Neural Network - 2020}
\href{https://arxiv.org/abs/2011.06175}{GNN - App - Optimizing Large-Scale Fleet Management on a Road Network using Multi-Agent Deep Reinforcement Learning with Graph Neural Network - 2020}

\subsection{GNN - Deep Multi-Agent Reinforcement Learning with Relevance Graphs - 2018}
\href{https://arxiv.org/abs/1811.12557}{GNN - Deep Multi-Agent Reinforcement Learning with Relevance Graphs - 2018}

\subsection{GNN - Deep Implicit Coordination Graphs for Multi-agent Reinforcement Learning- 2020}
\href{https://arxiv.org/abs/2006.11438}{GNN - Deep Implicit Coordination Graphs for Multi-agent Reinforcement Learning- 2020}

\subsection{GNN - Multi-Agent Game Abstraction via Graph Attention Neural Network  - 2020}
\href{https://ojs.aaai.org/index.php/AAAI/article/view/6211}{GNN - Multi-Agent Game Abstraction via Graph Attention Neural Network  - 2020}

\subsection{GNN - Scaling Up Multiagent Reinforcement Learning for Robotic Systems Learn an Adaptive Sparse Communication Graph - 2020}
\href{https://arxiv.org/abs/2003.01040}{GNN - Scaling Up Multiagent Reinforcement Learning for Robotic Systems Learn an Adaptive Sparse Communication Graph - 2020}

\subsection{GNN - Graphcomm A Graph Neural Network Based Method for Multi-Agent Reinforcement Learning - 2021}
\href{https://ieeexplore.ieee.org/abstract/document/9413716}{GNN - Graphcomm A Graph Neural Network Based Method for Multi-Agent Reinforcement Learning - 2021}

\subsection{GNN - Towards Heterogeneous Multi-Agent Reinforcement Learning with Graph Neural Networks - 2020}
\href{https://arxiv.org/abs/2009.13161}{GNN - Towards Heterogeneous Multi-Agent Reinforcement Learning with Graph Neural Networks - 2020}

\subsection{GNN - The Emergence of Adversarial Communication in Multi-Agent Reinforcement Learning - 2020}
\href{https://arxiv.org/abs/2008.02616}{GNN - The Emergence of Adversarial Communication in Multi-Agent Reinforcement Learning - 2020}

\subsection{GNN - Multi-Agent Deep Reinforcement Learning using Attentive Graph Neural Architectures for Real-Time Strategy Games - 2021}
\href{https://arxiv.org/abs/2105.10211}{GNN - Multi-Agent Deep Reinforcement Learning using Attentive Graph Neural Architectures for Real-Time Strategy Games - 2021}

\subsection{GNN - Global-Localized Agent Graph Convolution for Multi-Agent Reinforcement Learning - 2021}
\href{https://ieeexplore.ieee.org/abstract/document/9414993}{GNN - Global-Localized Agent Graph Convolution for Multi-Agent Reinforcement Learning - 2021}

\subsection{GNN - Specializing Inter-Agent Communication in Heterogeneous Multi-Agent Reinforcement Learning using Agent Class Information - 2020}
\href{https://arxiv.org/abs/2012.07617}{GNN - Specializing Inter-Agent Communication in Heterogeneous Multi-Agent Reinforcement Learning using Agent Class Information - 2020}

\subsection{GNN - Collaborative Multiagent Reinforcement Learning by Payoff Propagation - 2006}
\href{https://dl.acm.org/doi/abs/10.5555/1248547.1248612}{GNN - Collaborative Multiagent Reinforcement Learning by Payoff Propagation - 2006}

\subsection{Write up for GCNs - 2016}
\href{https://tkipf.github.io/graph-convolutional-networks/}{Write up for GCNs - 2016}


