% !TeX spellcheck = en_US
% !TeX encoding = UTF-8
% !TeX root = ../thesis.tex

\chapter{Introduction}
% pages: 0.9-1.2 ~ 369-715 words, ~62-120 words per paragraph
Any system that requires multiple agents to work together or against each other can be modelled as a Multi-Agent System (MAS). They are able to achieve more for a common goal, than a single agent could. An Agent usually has a limited sensory perception range, so cooperation is required. Observations can be shared to improve performance of the group. These agents can organize themselves and achieve more complex behavior together. They can improve the reliability and robustness of systems, as single failures can be compensated. Additionally, a MAS allows for flexibility in the number of participants. Agents can be added to the group as and when necessary. \par

Multi-Agent Systems have several real-world applications. For example, in \citet{MARLTraffic2020} the authors defined a traffic control system using MAS. They created a grid of traffic lights, where each of them is an agent. Then the agents learned to optimize the total waiting times at the intersections. Similarly, robot swarms can also be used to solve pathfinding problems. For example, indoor and maze-like scenarios are discussed in \citet{SwarmPathFinding2013}. The structure of the environment causes the communication to be severely limited, which does not pose a problem for the MAS System. These Swarms are used to simulate physical systems of liquids, solids and deformable materials \citep{GNS2020}.
\par

Reinforcement Learning (RL) was originally designed for environments with a single agent. More advancement has come from the field of deep reinforcement learning. Because of those, it has become possible to adapt single-agent RL to Multi-Agent Systems. This is called Multi-Agent Reinforcement Learning (MARL). We will focus on simple homogeneous agents that will cooperate to solve a common task. For example, MARL has been used to solve complex strategic and tactical problems found in videogames. More specifically research can be found that focuses on Real-Time Strategy games \citep{RTSMARL2021}. \par

With more and more agents the needs for a more sophisticated structure to process observation data is needed in MARL. We want to explore the viability of using Graph Neural Networks (GNNs) as a processing step for Multi-Agent Reinforcement Learning. GNNs allow for learning of Neural Networks with graphs inputs. Graphs are a natural and fitting representation for the communication between agents. They also allow for data processing between agents that are spatially adjacent, which fits with many real-world applications. \par

Our Approach is based on the previously published work found in \citet{RobinRuede2021}. It already describes a GNN for processing observation data. We improve on that by implementing multiple GNN passes inspired by \citet{graphconvolutionMARL}. We want to run our experiments in continuous environment, which is why we chose not to use the learning approach found in \citet{graphconvolutionMARL}, but to the same algorithm as \citet{RobinRuede2021}. We want to show that multiple GNN passes can be a benefit to the training results.\par


This thesis is structured like this: Firstly we will go over the necessary preliminaries and notation needed to understand this thesis in \Cref{ch:Preliminaries}. This includes Reinforcement Learning basics in the single-agent case, then addresses how it is applied in multi-agent tasks and is followed by a brief overview of Graph Neural Networks. In \Cref{ch:Related Work} related work will be discussed. The subsequent \Cref{ch:Architecture} is focused on the details of our proposed model. It will describe our policy architecture respect to both homogeneous and heterogeneous Graph Neural Networks. Our experiments, including the general setup and the specific tasks that we trained our mode on, are described in \Cref{ch:Experiments}. The goal of our experiments, their interpretation and evaluation of our results can be found in \Cref{ch:Evaluation}. We will complete this thesis by distilling the conclusions about our findings and provide future avenues for improvements in \Cref{ch:Conclusion}. 