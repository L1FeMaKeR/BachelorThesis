% !TeX spellcheck = en_US
% !TeX encoding = UTF-8
% !TeX root = ../thesis.tex

\chapter{Preliminaries}
\label{ch:Preliminaries}
% pages: 3.6-4.8
% baseline: Bachelor degree CS without any assumptions about electives.

\section{Notation}

% Environments
\newcommand{\agent}{a}
\newcommand{\actions}{A}
\newcommand{\action}{act}
\newcommand{\policy}{pi}
\newcommand{\observation}{o}
\newcommand{\observations}{O}
% RL
\newcommand{\advantage}{A}
\newcommand{\reward}{r}
\newcommand{\state}{s}
\newcommand{\states}{S}
% GNN
\newcommand{\graph}{G}
\newcommand{\nodes}{V}
\newcommand{\node}{v}
\newcommand{\nodefeatures}{X_v}
\newcommand{\nodefeature}{x_v}
\newcommand{\edges}{E}
\newcommand{\edge}{e}
\newcommand{\edgefeatures}{X_e}
\newcommand{\edgefeature}{x_e}
\newcommand{\globalfeatures}{X_g}
\newcommand{\globalfeature}{x_g}
\newcommand{\neighborhood}{N}

\begin{table}[ht!]
	\caption{Overview of mathematical, RL and GNN notation}
	\vspace*{0.5cm}
	\centering
	\begin{tabular}{ll}
		\toprule
		Symbol & Description \\
		\midrule
		$\oplus$ or $\otimes$ & Aggregation \\
		\midrule
		$\agent$ & Agent \\
		$\action$ & Action \\
		$\pi$ & Policy \\
		$\reward$ & Reward \\
		$\state$ & State \\
		$\observation$ & Observation \\
		\midrule
		$\nodefeature$ & Node-features \\
		$\edgefeature$ & Edge-features \\
		$\globalfeatures$ & Global-features \\
		$\mathcal{\neighborhood}_i$ & Neighborhood for agent i \\
		\bottomrule
	\end{tabular}
	\label{tab:macros}
\end{table}


\section{Reinforcement Learning}
In Reinforcement Learning (RL), an agent interacts with an environment to iteratively learn about it's task. Every timestep it uses an action $a$ and the environment generates a reward $\reward$, which is used to update the strategy of the agent, called policy $\pi$. $\pi$ can be deterministic $\pi(s) = a$, or probabilistic $\pi(a|s) = \mathbb{P}[A_t=a|S_t=s]$. Rewards can be dense (outputed at avery step), or sparse (outputed irregularly). It also generates an observation $\observation$. Is the environment fully observable, $\observation$ is the current environment state $S^e$. Otherwise, called partially observable, $\observation$ is the agents belief of the environment or the agent state $S^a$. This is used by $\pi$ to generate a new action for the next time-step. \par

Single-Agent Reinforcement Learning in fully observable environments are formally described using a Markov Decision Process (MDP). It relies on the Markov property of the Markov chain. It defines that the current state is enough to infer future states through a transition probablitity. So the history of previous states is not needed. Therefore it can be described by the 4-tuple ($\state, \actions, P, R$), where:
\begin{itemize}[noitemsep,nolistsep]
	\item $\states$ is a set of states called the state space.
	\item $\actions$ is a set of actions called action space.
	\item $P_{ss'}$ is the state transition function. For a given action $a$ and state $\state$ it outputs the probability of landing in state $s'$.
	\item $R_{ss'}$ is the expected immediate reward function. It returns the reward for transitioning from state $\state$ to state $s'$.
\end{itemize}

If the environment is partially observable an MPD can be expanded to a partially observable MPD (POMDP). The policy now takes the observation as input and not the actual environment state. The observation is based on given agent and includes subjective partial information. This is a tuple ($\states, \actions, \observations, P, R, Z$), where we have in addition to MDPs:
\begin{itemize}[noitemsep,nolistsep]
	\item $O$ a finite set of observations.
	\item $Z_{s'o}^a$ an observation function which gives us the probability of observation $\observation$ from the agent's perspective for a new environment state $s'$.
\end{itemize}

To evaluate a policy we are able to use two functions, the state-value function and action-value function. The state-value function $V_\pi(s)$ tells us the expected future reward of a given start state $s$ and a policy $\pi$. This is the sum of the rewards we will get following the policy starting from $s$. The sum is further discounted by the discount rate $\gamma \in [0,1)$. It is used to discount rewards that are futher in the future:

\begin{equation}
    V_\pi(s) = \mathbb{E}_\pi \left [ \sum_{t=0}^\infty \gamma^t r_t | S_t = s\right ] \nonumber
\end{equation} \par

On the other hand we have the action-value function $Q_\pi(s,a)$, also known as Q-function. It tells us the discounted expected future reward for a policy $\pi$, an state $\state$ and an action $a$. This describes how good it is to be in a given state and to use a given action:

\begin{equation}
    Q_\pi(s,a) = \mathbb{E}_\pi [\sum_{t=0}^\infty \gamma^t r_t | S_t = s, A_t = a] \nonumber
\end{equation} \par

So an optimal policy $\pi^*$ maximizes the value-function and the action-value-function. For any nontrivial task like continuous environments that have alot of states, a direct representation of $\pi$ is not practical. The action-state space explodes in size and therefore an approximation is required. \par

For optimization, we use policy gradient methods, which uses gradient descent to find $\pi^*$. In Actor-Critic methods we optimize two participants. The actor which controls how the agent behaves is based on the policy $\pi$. The critic measures how good the current policy is by using the value-function. While learning, the actor tries to improve the policy by using the advice from the critic, which learns how to correctly critique the policy of the actor. \par

In order to reduce variance when training we can use the advantage function $A^\pi(s,a)$. It describes how much extra reward the agent could get by taking a given action:

\begin{equation}
    A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s)\nonumber
\end{equation} 

This is called Advantage Actor Critic (A2C). Common Actor-Critic algorithms include Proximal Policy Optimization (PPO) \Citet{SchulmanWDRK17}, Trust Region Policy Optimization (TRPO) \Citet{TRPO2015}, Trust Region Layers (PG-TRL) \Citet{otto2021differentiable}.



\section{Multi-Agent Reinforcement Learning}
% Talk about Kinds of MAS Environments, Centralized/Decentralized Control, Homogeneous and Heterogeneous Agents, Hierarchical Structures, coalition, teams
% \href{https://link.springer.com/chapter/10.1007/978-3-642-14435-6_1}{MAS - An Introduction to Multi-Agent Systems - 2010} or is this part of related_works?
In this thesis we explore Multi-Agent Reinforcement Learning (MARL) or Swarm Reinforcement Learning (SwarmRL). In a Multi-Agent System (MAS) multiple agents influence the environment. The actions for all agents at a given timestep is called the joint actions $A = \langle a_1,...,a_n\rangle$, where $a_i$ is the action of agent i. Each agent has it's own observation and we therefore have a joint observation $O = \langle o_1,...,o_n\rangle$. There are a few extensions to POMDPs for multi-agent environments, for example decentralized POMPDs (Dec-POMDP). The main difference to POMPDs can be seen in the joint observations and joint actions:

\begin{itemize}[noitemsep,nolistsep]
	\item $I$ is the set of n agents.
	\item $\states$ is a set of states.
	\item $\actions$ is a set of joint actions.
	\item $P_{ss'}$ is the state transition function. For a given joint action $a$ and state $\state$ it gives the probability of landing in state $s'$.
	\item $O$ is a set of joint observations.
	\item $Z_{s'o}^a$ is an observation function which gives us the probability of an observation $\observation$ from an agent $\agent$'s perspective for a new environment state $s'$.
	\item $R_{ss'}$ is the expected immediate reward function for all agents. It returns the reward for transitioning from state $\state$ to state $s'$.
\end{itemize}



\section{Message Passing GNN}
In Graph Neural Networks (GNNs), we want to learn functions that run on graphs as inputs. The function should have the same result for two isomorphic graphs. Edges are represented using an adjacency matrix A, where:

\begin{equation}
    a_{ij} = \begin{cases} 1& (i,j) \in E \\ 0 & otherwise \end{cases} \nonumber
\end{equation}

Nodes and edges have features, called node-features $X_v$ and edge-features $X_e$. To preserve isomorphic graphs, we want the function $f$ of our neural network to be permuation equivariant: $f(PX_v,PAP^T) = Pf(X_v,A)$ for a permuation $P$. In order to do that we define a function $g(x_{v_i}, \mathcal{N}_i)$, that uses a node-feature and the neighborhood of a node $\mathcal{N}_i$. It is called the local aggregation function. $g$ should not depend of the order of nodes in the neighborhood, in other words it should be permutation invariant $g(x_{v_i}, P\mathcal{N}_i) = g(x_{v_i}, \mathcal{N}_i)$. We can then construct $f$ using $g$ so that $f$ achieves permutation equivariance:

\begin{equation}
    f(X_v,A) = \begin{pmatrix} g(x_{v_1},\mathcal{N}_1) \\ g(x_{v_2},\mathcal{N}_2) \\ \vdots \\ g(x_{v_n},\mathcal{N}_n) \end{pmatrix} \nonumber
\end{equation}

$f$ is called a single Graph Neural Network layer or a single GNN hop. By applying this function multiple times to the graph, information can flow between multiple neighborhoods, called multiple GNN hops. A node is able to get information about the neighbors of his neighbor. The local aggregation function $g$ can have one of three flavors:

\begin{equation}
    \begin{aligned}
        \textup{Convolutional:}\ & g(x_i) = \phi(x_{v_i}, \oplus_{j \in \mathcal{N}_i}\ c_{ij}\ \psi(x_{v_j})).\\
        \textup{Attentional:}\ & g(x_i) = \phi(x_{v_i}, \oplus_{j \in \mathcal{N}_i}\ a(x_i,x_j)\ \psi(x_{v_j})).\\
        \textup{Message-Passing:}\ & g(x_i) = \phi(x_{v_i}, \oplus_{j \in \mathcal{N}_i}\ \psi(x_{v_i}, x_{v_j})). \\
    \end{aligned}
    \nonumber
\end{equation}

In Convolutional GNNs we use constants $c_{ij}$ for each edge which states how much Node $j$ influences the features of Node $i$. This turns the neighborhood aggregation into a weighted sum. So it is only dependend on the structure of the graph. Attentional GNNs are similar, but the weights are now a function $a(x_i,x_j)$ of Node $i$ and Node $j$ that is learnable. In Message-passing GNNs we have no weights, but the neighbor feature transformation function $\psi$ now takes both node features into account $\psi(x_{v_i}, x_{v_j})$. This way the sender and receiver work together to create messages that are used across an edge.



\section{Neural Networks}
The Neural Network is the basis for every kind of approximate function learning. Each neuron can hold a real value. Layers are mulitple neurons in a row, where neurons in a given layer are used as inputs for the neurons of the next layer. The new value $y$ is a weighted  sum, with weights $w$, of the neurons of the previous layer it is connected to plus a bias $b$. This is describes a linear function. The result is then put through an activation function $\phi$ to control value ranges:

\begin{equation}
    y = \phi(w^T x + b) \nonumber
\end{equation} 

If each of the neurons of the previous layer is connected to each node of the next layer we call that fully connected. The first layer is the input of the function and the last layer is the output. A Multi-Layer-Perceptron has multiple fully connected layers.