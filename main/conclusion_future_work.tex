% !TeX spellcheck = en_US
% !TeX encoding = UTF-8
% !TeX root = ../thesis.tex

\chapter{Conclusion and Future Work}
\label{ch:Conclusion}
% pages: 0.9-1.2


\section{Conclusion}
Generally, we have shown the overall validity of our approach and architecture. Furthermore, under different constraints multiple homogeneous GNN hops can improve upon the results. Firstly, when having a small latent dimension. Here, more hops did help and were more efficient than increasing the latent dimension. This suggests a higher observation efficiency than without using GNNs. Secondly, in the ablation tests, the benefit was also much higher. In observation constrained environment the architecture performed better, if the setup was reasonable solvable. If it wasn't, more hops did not improve the result. Here in low latent dimension and low observation radius, the benefit was most pronounced. Which leads to the conclusion that the high processing capabilities of mulitple hops did improve upon learning.

\section{Future Work}
More can be done to expand on the work already finished in this bachelor thesis.\par

Firstly, while our results showed promising results, additional experiments would be needed to show the limits and real benefits of our approach. Some of the experiments may produce different results if they ran longer, like a latent dimension of 8. Also, tighter observation radii could use more iterations. Especially kNN did not produce real results. Furthermore, some of the parameters our architecture is capable of we were not able to test. This includes the different neighbor aggregation types for heterogeneous graphs, or the node-wise values and graph-wise value functions. Likewise working results for Pursuit are needed to show our architecture in the heterogeneous case.

All of the experiments in this paper only considered using Proximal Policy Optimization (PPO) \citet{SchulmanWDRK17}, as it is a very common baseline training algorithm.
However, recent research shows, that other methods might lead to better results for Multi-Agent Reinforcement Learning.
Specifically Trust Region Layers (PG-TRL) \citet{otto2021differentiable} is an alternative to PPO. The results show that it is at least on par with PPO, while requiring less code-level optimizations. It is noted that in experiments with sparser rewards, the better exploration of TRL improves the results significantly.
\citet{RobinRuede2021} explores Trust Region Layers (PG-TRL) \citep{otto2021differentiable} for multi-agent tasks. The author explains, that given a multi-agent cooperative task, the agents are rewarded as a group, which makes the reward more sparse. Therefore, creating correlation of a single agent's action and the group reward is harder. The hyper parameters used for TRL were based on searches for PPO and no extensive testing for TRL was done. Even then TRL was able to perform similar to PPO.\\
Creating further experiments based on the architecture established in this thesis would very likely benefit from TRL. \par

Furthermore \citet{RobinRuede2021} also used more complex multi-agent task than we used in our experiments. In Box Clustering there are agents and multiple boxes. Each box is assigned to one cluster. The goal is to move the boxes, so that the distance between the boxes in each cluster is minimal. Optimal solutions will require that the agents work together to move the boxes and that they split the work between them. In his thesis his approach worked well for two clusters of boxes but fell apart with three clusters. Then the agents were only able to move one cluster correctly. Only after increasing the batch size and environment steps per training steps tenfold, the agents were able to consider more than 2 clusters. As explained above, our approach is structurally similar to \citet{RobinRuede2021} as both can be described with message-passing of GNNs. It was shown that more complex tasks, especially with tight communication ranges, benefited hugely from multiple message passing hops. So, one can assume that we would be able to solve Box Clustering better.\par

As this thesis is an extension of basic ideas found in \citet{RobinRuede2021}, both thesis are designed to work for a single group of homogeneous agents. It is stated in the architecture description, that in heterogeneous graphs the policy training method can only use one node type. It is always trained on the agent node features. It would be possible to parameterize the node type it trains on. In team-based tasks you can have multiple competing groups of agents. Our architecture could support two policies that can be trained on different node types and therefore groups of agents. \par

\iffalse
a
Data pre processing:
Wobei ein MLP Encoder bei uns auch was ist, was man ausprobieren kann. Theoretisch ist das nicht notwendig, aber praktisch spart man sich damit vielleicht ein oder zwei GNN Blöcke. Das und andere initiale Encodings kannst du gerne als Future work reintun. Dazu gibt es einen ganzen Satz Paper, die ihren Input bspw. gelernt, mittels Sinuiden oder Random Fourier Features encoden, bevor das eigentliche Netz anfängt. Wenn du magst schick ich dir zwei drei Referenzen, die du in dem Kontext zitieren kannst

https://arxiv.org/pdf/2003.08934.pdf Macht positional encoding für "synthesizing novel views of complex scenes". Figure 4 zeigt ein Beispiel, wie die Methode mit und ohne Encoding aussieht.
https://arxiv.org/pdf/2006.09661.pdf Nutzen sinus-Aktivierungen für implicit representations. Das ist bspw. sowas, wie ein Bild als Funktion zu Enkodieren ("B(x,y)=(r,g,b)") und das in den Gewichten eines neuronalen Netzes zu lernen
https://arxiv.org/abs/2006.10739 Zeigt, dass ein Fourier Feature Mapping dazu führt, dass Netze viel besser darin werden, Details in örtlich benachbarten Bereichen auf Bildern zu differenzieren.

\fi

\section{Acknowledgements}
The authors acknowledge support by the state of Baden-Württemberg through bwHPC.