% !TeX spellcheck = en_US
% !TeX encoding = UTF-8
% !TeX root = ../thesis.tex

\chapter{Conclusion and Future Work}
\label{ch:Conclusion}
% pages: 0.9-1.2

\section{Conclusion}
% 109 (David), 195 (Niklar), 229 (Robin) Words
% What comparison we made on different tasks, What we observed. What have we shown of results. Basically a summary of "evaluation".
\begin{itemize}[noitemsep,nolistsep]
	\item harsher culling => more layers better. partially compensate, but interpreted data about neighbor's neighbors.
	\item more hops, better but has limit (more complex, more hops).
	\item randomized: good abstractions.
	\item heterogeneous: concat(aggr())?
	\item GNN Structure: undirected vs directed.
	\item node-wise or graph-wise value function.
\end{itemize}


\section{Future Work}
More can be done to expand on the work already finished in this bachelor thesis.\par

All of the experiments in this paper only considered using Proximal Policy Optimization (PPO) \citet{SchulmanWDRK17}, as it is a very common baseline training algorithm.
However recent research shows that other methods might lead to better results for mult-iagent Reinforcement Learning.
Specifically Trust Region Layers (PG-TRL) \citet{otto2021differentiable} is an alternative that is able to be atleast on par with PPO, while requiring less code-level optimizations. It is noted that in experiments using sparser rewards the fact that TRL has better exploration over PPO improves the results significantly. Though the base paper only explores single-agent problems.\\
\citet{RobinRuede2021} explores Trust Region Layers (PG-TRL) \citet{otto2021differentiable} for multi-agent tasks. The author explains that given an multi-agent cooperative task the agents are rewarded as a group, which makes the reward more sparse. Therefore creating correlation of a single agents action and the group reward is harder. The hyper parameters used for TRL were based on searches for PPO and no extensive testing for TRL was done. Even then TRL was able to perform similar to PPO.\\
Creating further experiments based on the architecture established in this thesis would very likely benefit from TRL. \par

Furthermore \citet{RobinRuede2021} also used more complex multi-agent task than we used in our experiments. In Box Clustering there are agents and multiple boxes. Each box is assigned to one cluster. The goal is to move the boxes, so that the distance between the boxes in a given cluster is minimal. Optimal solutions will require that the agents work together to move the boxes and that they split the work between them. In his thesis his approach worked well for two clusters of boxes, but fell appart with 3 clusters. Here the agents were only able to move one cluster correctly. Only after increasing the batch size and environment steps per training steps tenfold the agents were able to consider more than 2 clusters. As explained above, our approach is structuraly similar to \citet{RobinRuede2021} as both can be described with message-passing of GNNs. It was shown that more complex tasks, especially with tight communication ranges, benefitet hugely from multiple message passing hops. So one can assume that we would be able to solve Box Clustering better.\par

As this thesis is an extension of basic ideas found in \citet{RobinRuede2021}, both thesis are designed to work for a single group of homogeneous agents. As stated in the architecture description, in heterogeneous graphs the policy training method can only use one node type. It is always trained on the agent node features. It would be possible to parameterize the node type it trains on. In team-based tasks you can have multiple competing groups of agents. Our architecture could support two policies that can be trained on different node types and therefore groups of agents. \par

TODO!!!!!
Personalized rewards or responsibility assignment. Might be easier to learn, because it can better correlate which actions where how good. Need to change critic, as it currently only uses the aggregate of the values. Needs to use each value itself for the critic? Look at some papers and how they do that. This can also help in supporting heterogeneous agents. Agents different strengths and weaknesses => can better understand how to use them if they are not aggregated. because actions are already used as non aggregated. (PAPER!)

\section{Acknowledgements}
The authors acknowledge support by the state of Baden-WÃ¼rttemberg through bwHPC.