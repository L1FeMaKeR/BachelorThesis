% !TeX spellcheck = en_US
% !TeX encoding = UTF-8
% !TeX root = ../thesis.tex

\chapter{Conclusion and Future Work}
\label{ch:Conclusion}
% pages: 0.9-1.2

\section{Conclusion}
% 109 (David), 195 (Niklar), 229 (Robin) Words
% What comparison we made on different tasks, What we observed. What have we shown of results. Basically a summary of "evaluation".
\begin{itemize}[noitemsep,nolistsep]
	\item harsher culling => more layers better. partially compensate, but interpreted data about neighbor's neighbors.
	\item more hops, better but has limit (more complex, more hops).
	\item randomized: good abstractions.
	\item heterogeneous: concat(aggr())?
	\item GNN Structure: undirected vs directed.
	\item node-wise or graph-wise value function.
\end{itemize}


\section{Future Work}
More can be done to expand on the work already finished in this bachelor thesis.\par

All of the experiments in this paper only considered using Proximal Policy Optimization (PPO) \citet{SchulmanWDRK17}, as it is a very common baseline training algorithm.
However recent research shows that other methods might lead to better results for Multi-Agent Reinforcement Learning.
Specifically Trust Region Layers (PG-TRL) \citet{otto2021differentiable} is an alternative that is able to be atleast on par with PPO, while requiring less code-level optimizations. It is noted that in experiments using sparser rewards the fact that TRL has better exploration over PPO improves the results significantly. Though the base paper only explores single-agent problems.
\citet{RobinRuede2021} explores Trust Region Layers (PG-TRL) \citet{otto2021differentiable} for multi-agent tasks. The author explains that given an multi-agent cooperative task the agents are rewarded as a group, which makes the reward more sparse. Therefore creating correlation of a single agent's action and the group reward is harder. The hyper parameters used for TRL were based on searches for PPO and no extensive testing for TRL was done. Even then TRL was able to perform similar to PPO.\\
Creating further experiments based on the architecture established in this thesis would very likely benefit from TRL. \par

Furthermore \citet{RobinRuede2021} also used more complex multi-agent task than we used in our experiments. In Box Clustering there are agents and multiple boxes. Each box is assigned to one cluster. The goal is to move the boxes, so that the distance between the boxes in a given cluster is minimal. Optimal solutions will require that the agents work together to move the boxes and that they split the work between them. In his thesis his approach worked well for two clusters of boxes, but fell appart with three clusters. Then the agents were only able to move one cluster correctly. Only after increasing the batch size and environment steps per training steps tenfold, the agents were able to consider more than 2 clusters. As explained above, our approach is structurally similar to \citet{RobinRuede2021} as both can be described with message-passing of GNNs. It was shown that more complex tasks, especially with tight communication ranges, benefitet hugely from multiple message passing hops. So one can assume that we would be able to solve Box Clustering better.\par

As this thesis is an extension of basic ideas found in \citet{RobinRuede2021}, both thesis are designed to work for a single group of homogeneous agents. As stated in the architecture description, in heterogeneous graphs the policy training method can only use one node type. It is always trained on the agent node features. It would be possible to parameterize the node type it trains on. In team-based tasks you can have multiple competing groups of agents. Our architecture could support two policies that can be trained on different node types and therefore groups of agents. \par

\iffalse
Data pre processing:
Wobei ein MLP Encoder bei uns auch was ist, was man ausprobieren kann. Theoretisch ist das nicht notwendig, aber praktisch spart man sich damit vielleicht ein oder zwei GNN Blöcke. Das und andere initiale Encodings kannst du gerne als Future work reintun. Dazu gibt es einen ganzen Satz Paper, die ihren Input bspw. gelernt, mittels Sinuiden oder Random Fourier Features encoden, bevor das eigentliche Netz anfängt. Wenn du magst schick ich dir zwei drei Referenzen, die du in dem Kontext zitieren kannst

https://arxiv.org/pdf/2003.08934.pdf Macht positional encoding für "synthesizing novel views of complex scenes". Figure 4 zeigt ein Beispiel, wie die Methode mit und ohne Encoding aussieht.
https://arxiv.org/pdf/2006.09661.pdf Nutzen sinus-Aktivierungen für implicit representations. Das ist bspw. sowas, wie ein Bild als Funktion zu Enkodieren ("B(x,y)=(r,g,b)") und das in den Gewichten eines neuronalen Netzes zu lernen
https://arxiv.org/abs/2006.10739 Zeigt, dass ein Fourier Feature Mapping dazu führt, dass Netze viel besser darin werden, Details in örtlich benachbarten Bereichen auf Bildern zu differenzieren.

\fi

\section{Acknowledgements}
The authors acknowledge support by the state of Baden-Württemberg through bwHPC.