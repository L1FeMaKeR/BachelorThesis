@article{RobinRuede2021,
    title = {Bayesian and Attentive Aggregation for Multi-Agent Deep Reinforcement Learning},
    author = {Ruede, Robin and Neumann, Gerhard and Asfour, Tamim and Huettenrauch, Maximilian},
    abstract = {Multi-agent reinforcement learning (MARL) is an emerging field in reinforcement learning with real world applications such as unmanned aerial vehicles, search-and-rescue, and warehouse organization. There are many different approaches for applying the methods used for single-agent reinforcement learning to MARL. In this work, we survey different learning methods and environment properties and then focus on a problem that persists through most variants of MARL: How should one agent use the information gathered from a large and varying number of observations of the world in order to make decisions? We focus on three different methods for aggregating observations and compare them regarding their training performance and sample eﬀiciency. We introduce a policy architecture for aggregation based on Bayesian conditioning and compare it to mean aggregation and attentive aggregation used in related work. We show the performance of the different methods on a set of cooperative tasks that can scale to a large number of agents, including tasks that have other objects in the world that need to be observed by the agents in order to solve the task. We optimize the hyperparameters to be able to show which parameters lead to the best results for each of the methods. In addition, we compare different variants of Bayesian aggregation and compare the recently introduced Trust Region Layers learning method to the commonly used Proximal Policy Optimization.},
    year = {2021},
    url = {https://phiresky.github.io/masters-thesis/manuscript.pdf},
    keywords = {Multi-Agent Reinforcement Learning},
    journal={Autonomous Learning Robotics (ALR)}
}

@inproceedings{otto2021differentiable,
title={Differentiable Trust Region Layers for Deep Reinforcement Learning},
author={Fabian Otto and Philipp Becker and Vien Anh Ngo and Hanna Carolin Maria Ziesche and Gerhard Neumann},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=qYZD-AO1Vn}
}

@article{SchulmanWDRK17,
  author    = {John Schulman and
               Filip Wolski and
               Prafulla Dhariwal and
               Alec Radford and
               Oleg Klimov},
  title     = {Proximal Policy Optimization Algorithms},
  journal   = {CoRR},
  volume    = {abs/1707.06347},
  year      = {2017},
  url       = {http://arxiv.org/abs/1707.06347},
  eprinttype = {arXiv},
  eprint    = {1707.06347},
  timestamp = {Mon, 13 Aug 2018 16:47:34 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/SchulmanWDRK17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{ZHOU201664,
title = {Cooperative pursuit with Voronoi partitions},
journal = {Automatica},
volume = {72},
pages = {64-72},
year = {2016},
issn = {0005-1098},
doi = {https://doi.org/10.1016/j.automatica.2016.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S0005109816301911},
author = {Zhengyuan Zhou and Wei Zhang and Jerry Ding and Haomiao Huang and Dušan M. Stipanović and Claire J. Tomlin},
keywords = {Pursuit–evasion games, Voronoi, Cooperative pursuit},
abstract = {This work considers a pursuit–evasion game in which a number of pursuers are attempting to capture a single evader. Cooperation among multiple agents can be difficult to achieve, as it may require the selection of actions in the joint input space of all agents. This work presents a decentralized, real-time algorithm for cooperative pursuit of a single evader by multiple pursuers in bounded, simply-connected planar domains. The algorithm is based on minimizing the area of the generalized Voronoi partition of the evader. The pursuers share state information but compute their inputs independently. No assumptions are made about the evader’s control strategies other than requiring the evader control inputs to conform to a speed limit. Proof of guaranteed capture is shown when the domain is convex and the players’ motion models are kinematic. Simulation results are presented showing the efficiency and effectiveness of this strategy.}
}

@unpublished{DAVIS2021,
  author = {Freymuth, Niklas},
  note = {Part of the research of the ALR research group at the Karlsruher Institute of Technology (KIT)},
  title  = {DAVIS Project},
  year   = {2021}
}

@article{PPOHacks2020,
  author    = {Logan Engstrom and
               Andrew Ilyas and
               Shibani Santurkar and
               Dimitris Tsipras and
               Firdaus Janoos and
               Larry Rudolph and
               Aleksander Madry},
  title     = {Implementation Matters in Deep Policy Gradients: {A} Case Study on
               {PPO} and {TRPO}},
  journal   = {CoRR},
  volume    = {abs/2005.12729},
  year      = {2020},
  url       = {https://arxiv.org/abs/2005.12729},
  eprinttype = {arXiv},
  eprint    = {2005.12729},
  timestamp = {Thu, 28 May 2020 17:38:09 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2005-12729.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{MARLTraffic2020,
author={Chu, Tianshu and Wang, Jie and Codecà, Lara and Li, Zhaojian},
journal={IEEE Transactions on Intelligent Transportation Systems}, 
title={Multi-Agent Deep Reinforcement Learning for Large-Scale Traffic Signal Control}, 
year={2020},
volume={21},
number={3},
pages={1086-1095},
doi={10.1109/TITS.2019.2901791}}

@INPROCEEDINGS{SwarmPathFinding2013,
author={Aurangzeb, M. and Lewis, F. L. and Huber, M.},
booktitle={2013 10th IEEE International Conference on Control and Automation (ICCA)}, 
title={Efficient, swarm-based path finding in unknown graphs using reinforcement learning}, 
year={2013},
volume={},
number={},
pages={870-877},
doi={10.1109/ICCA.2013.6564940}}

@article{RTSMARL2021,
title = {Hierarchical control of multi-agent reinforcement learning team in real-time strategy (RTS) games},
journal = {Expert Systems with Applications},
volume = {186},
pages = {115707},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.115707},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421010897},
author = {Weigui Jair Zhou and Budhitama Subagdja and Ah-Hwee Tan and Darren Wee-Sze Ong},
keywords = {Hierarchical control, Self-organizing neural networks, Reinforcement learning, Real-time strategy games},
abstract = {Coordinated control of multi-agent teams is an important task in many real-time strategy (RTS) games. In most prior work, micromanagement is the commonly used strategy whereby individual agents operate independently and make their own combat decisions. On the other extreme, some employ a macromanagement strategy whereby all agents are controlled by a single decision model. In this paper, we propose a hierarchical command and control architecture, consisting of a single high-level and multiple low-level reinforcement learning agents operating in a dynamic environment. This hierarchical model enables the low-level unit agents to make individual decisions while taking commands from the high-level commander agent. Compared with prior approaches, the proposed model provides the benefits of both flexibility and coordinated control. The performance of such hierarchical control model is demonstrated through empirical experiments in a real-time strategy game known as StarCraft: Brood War (SCBW).}
}

