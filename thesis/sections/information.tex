%% LaTeX2e class for student theses
%% sections/content.tex
%% 
%% Karlsruhe Institute of Technology
%% Institute for Program Structures and Data Organization
%% Chair for Software Design and Quality (SDQ)
%%
%% Dr.-Ing. Erik Burger
%% burger@kit.edu
%%
%% Version 1.3.5, 2020-06-26

\chapter{Information to sort}
\label{ch:Info}

\section{Books}
\href{http://aiwisdom.com/ai_genrerts.html}{AI Game Programming Wisdom (1-4)}

\section{MAS - A Comprehensive Survey of Multiagent Reinforcement Learning - 2008}
\href{https://ieeexplore.ieee.org/abstract/document/4445757}{MAS - A Comprehensive Survey of Multiagent Reinforcement Learning - 2008}
\underline{Benefits}
\begin{itemize}[noitemsep,nolistsep]
	\item can be parallelized.
	\item can use experience sharing via communication, or with a teacher-learner relationship.
	\item Failure of one agent can be covered by other agents.
	\item insertion of new agents => scaleable.
	\item MARL Complexity: Exponential in number of agents.
	\item exploration (new knowledge) - exploitation (current knowledge) - Tradeoff.
	\item They explore about the environment and other agents.
	\item need for coordination.
\end{itemize}
\underline{Application Domains}
\begin{itemize}[noitemsep,nolistsep]
	\item simulation better than real-life (better scalability and robustness).
	\item Distributed Control: for controlling processes (for larger industry plants).
	\begin{itemize}[noitemsep,nolistsep]
		\item avenue for future work.
		\item used for traffic, power or sensory networks.
		\item could also be used for pendulum systems.
	\end{itemize}
	\item Robotic Teams (Multirobot):
	\begin{itemize}[noitemsep,nolistsep]
		\item simulated 2D space. 
		\item navigation: Reach a goal with obstacles. Area sweeping (retrival of objects (also cooperative)).
		\item pursuit: Capture a prey robot.
	\end{itemize}
	\item Automated Trading: Exchange goods on electronic markets with negotiation and auctions.
	\item Resource Management: Cooperatie team to manage resources or as clients. (routing, load balancing).
\end{itemize}
\underline{Practicallity and Future works}
\begin{itemize}[noitemsep,nolistsep]
	\item Scalability Problem: Q-functions do not scale well with the size of the state-action space.
	\begin{itemize}[noitemsep,nolistsep]
		\item Approximation needed: for discrete large state-action spaces, for continous states and discrete actions or continious state and action.
		\item Heuristic in nature and only work in a narrow set of problems.
		\item Could use theoretical results on single-agent approximate RL.
		\item also could use discovery and exploitation of the decentralized, modular structure of the multiagent task.
	\end{itemize}
	\item MARL without prior knowledge is very slow.
	\begin{itemize}[noitemsep,nolistsep]
		\item Need humans to teach the agent.
		\item shaping: first simple task then scale them.
		\item could use reflex behavior.
		\item Knowledge about the task structure.
	\end{itemize}
	\item Incomplete, uncertain state measurements could be handled with partiall observability techniques (Markov decision process).
	\item Multiagent Goals needs a stable learning process for the environment and an adaption for the dynamics of other agents.
	\item using game-theory-based analysis to apply to the dynamics of the environment.
\end{itemize}

\section{Artificial Intelligence - A modern Approach}
\subsection{Agents and Environments}
p.34
\begin{itemize}[noitemsep,nolistsep]
	\item \textbf{agent}: anything that perceives its \textbf{environment} through \textbf{sensors} and acting upon that environment using \textbf{actuators}.
	\item \textbf{percept}: agent’s perceptual inputs at any given instance. Percept sequence is a complete history of perception.
	\item agents choice of action decided upon the history of perception, but not anything it has not perceived.
	\item its behavior is described by the \textbf{agent function}, which is internally implemented by the \textbf{agent program}.
\end{itemize}

\subsection{Rational Agent}
p.36
\begin{itemize}[noitemsep,nolistsep]
	\item \textbf{rational agent}: it does the correct thing. Correctness is determined by a performance measure, which is determined by the changed environment states.
	\item design \textbf{performance measures} according to what one actually wants in the environment, rather than according to how one thinks the agent should behave.
	\item rational depends on:
	\begin{itemize}
		\item the performance measure that defines the criterion of success
		\item the agent’s prior knowledge of the environment.
		\item The actions that the agent can perform.
		\item The agent’s percept sequence of data.
	\end{itemize}
	\item depending on the measures the agent might be rational or not. 
	\item an \textbf{omniscient agent} knows the actual outcome of its actions and can act accordingly, but this is impossible in reality.
	\item rationality maximizes expected performance, while perfection (omniscient) maximizes actual performance.
	\item agents can do actions in order to modify future percepts, called \textbf{information gathering, or exploration}.
	\item rational agents learn as much as possible from what it perceives.
	\item his knowledge can be augmented and modified as it gains experience.
	\item if the agent relies on the prior knowledge of its designer rather than on its own percepts, we say that the agent lacks \textbf{autonomy}.
	\item it should learn what it can to compensate for partial or incorrect prior knowledge.
	\item give it some initial knowledge and the ability to learn, so it will become independent of its prior knowledge.
\end{itemize}

\subsection{Nature of Environments}
p.40
\begin{itemize}[noitemsep,nolistsep]
	\item \textbf{task environments}: the “problems” to which rational agents are the “solutions”.
	\item Describe the task environment in the following aspects P(Performance measure), E(Environment), A(Actuators), S(Sensors).
	\item \textbf{fully observable}: the agent’s sensors give it access to the complete state of the environment. All aspects that are relevant to the choice of actions
	\item \textbf{partially observable}: otherwise. Because of missing sensors or noise.
	\item no sensors: unobservable
	\item single-agent environments and multi-agent environments.
	\item multi-agent can be either competitive (chess) or cooperative (avoiding collisions maximizes performance).
	\item \textbf{communication} emerges as a rational behavior in multiagent environments.
	\item randomized behavior is rational because it avoids the pitfalls of predictability.
	\item \textbf{Deterministic}: next state of environment is completely determined by the current state and the action executed by the agent, otherwise it is \textbf{stochastic}.
	\item you can ignore uncertainty that arises purely from the actions of other agents in a multiagent environment.
	\item If the environment is partial observable, it could appear to be stochastic, which implies quantifiable outcomes in terms of probabilities.
	\item an environment is \textbf{uncertain} if it is not fully observable or not deterministic. 
	\item \textbf{episodic}: the agent’s experience is divided into atomic episodes. In each the agent receives a percept and performs a single episode. The next episode does not depend on the actions taken in previous episodes, otherwise it is \textbf{sequential}.
	\item When the environment can change while the agent is deliberating, then the environment is \textbf{dynamic} for that agent otherwise it is \textbf{static}.
	\item if the environment itself does not change with the passage of time but the agent’s performance score does, then we say the environment is \textbf{semi dynamic}.
	\item \textbf{discrete/continuous} applies to the state of the environment, to the way time is handled, and to the percepts and actions of the agents.
	\item \textbf{known vs. unknown}: refers to the agent’s state of knowledge about the “laws of physics” of the environment. Known environment, the outcomes for all actions are given, otherwise the agent needs to learn how it works. An environment can be known, but partially observable (solitaire: I know the rules but still unable to see the cards that have not yet been turned over)
	\item hardest case: partially observable, multiagent, stochastic, sequential, dynamic, continuous, and unknown
	\item \textbf{environment class}: multiple environment scenarios to train it for multiple situations.
	\item you can create an \textbf{environment generator}, that selects environments in which to run the agent.
\end{itemize}

\subsection{Structure of Agents}
p.46
\begin{itemize}[noitemsep,nolistsep]
	\item agent = architecture (computing device) + program (agent program).
	\item agent programs take the current percept as input and return an action to the actuators.
	\item agent program takes the current percept, agent function which takes the entire percept history.
	\item \textbf{table driven agent}: Uses a table of actions indexed by percept sequences. This table grows way to fast and is therefore not practical.
\end{itemize}

\underline{Simple reflex agents:}
\begin{itemize}[noitemsep,nolistsep]
	\item \textbf{simple reflex agents}: Select the actions on the basis of the current percept, ignoring the rest of the history.
	\item \textbf{condition-action-rule}: these agents create actions in a specific condition (if-then). These connections can be seen as reflexes.
	\item uses an \textbf{interpret-input} function as well as a \textbf{rule-match} function.
	\item they need the environment to be fully observable. They could run into infinite loops.
	\item you can mitigate this by using randomization for the actions. Which is non-rational for single agent environments.
\end{itemize}

\underline{Model-based reflex agents:}
\begin{itemize}[noitemsep,nolistsep]
	\item keep track of the part of the world an agent cannot see now. It maintains some sort of \textbf{internal state} that depends on the percept history.
	\item agents needs to know how the world evolves independently of the agent and how the agent’s own actions affect the world.
	\item with this it creates a \textbf{model} of the world hence it is called model-based agent.
	\item it needs to update this state given sensor data.
	\item this model is a \textbf{best guess} and does not determine the entire current state of the environment exactly.
\end{itemize}

\underline{Goal-based agents:}
\begin{itemize}[noitemsep,nolistsep]
	\item an agent needs some sort of \textbf{goal information} that describes situations that are desirable. This can also be combined with the model.
	\item Usually agents need to do multiple actions to fulfill a goal which requires \textbf{search} and \textbf{planning}.
	\item this also involves consideration of the future.
	\item the goal-based agent’s behavior can be easily changed to go to a different destination by using a goal where a reflex agent needs completely now rules.
\end{itemize}

\underline{Utility-based agents:}
\begin{itemize}[noitemsep,nolistsep]
	\item goals provide a crude binary distinction between good and bad states.
	\item use an internal \textbf{utility function} to create a performance measure.
	\item if the external performance measure and the internal utility function agree, the agent will act rationally.
	\item if you have conflicting goals the utility function can specify the appropriate \textbf{tradeoff}.
	\item if multiple goals cannot be achieved with certainty, utility provides a way to determine the \textbf{likelihood} of success.
	\item a rational utility-based agent chooses the action that \textbf{maximizes the expected utility}.
	\item any rational agent must behave as if it possesses a utility function whose expected value it tries to maximize.
	\item a utility-based agent must model and keep track of its environment.
\end{itemize}

\underline{Learning Agents:}
\begin{itemize}[noitemsep,nolistsep]
	\item it allows the agent to operate in initially unknown environments and to become more competent than its initial knowledge alone might allow.
	\item 4 conceptual components: \textbf{learning element} (responsible for improvements), \textbf{performance element} (select external action), \textbf{critic} (gives feedback to change the learning element), \textbf{problem generator} (suggesting actions that lead to new and informative experiences).
	\item critic tells the learning element how well the agent is doing given a performance standard. It tells the agent which percepts are good and which are bad.
	\item problem generator allows for exploration and suboptimal actions to discover better actions in the long run.
	\item learning element: simplest case: learning directly from the percept sequence.
	\item the \textbf{performance standard} distinguishes part of the incoming percept as a reward or penalty that provides direct feedback on the quality of the agent’s behavior.
\end{itemize}

\underline{How the components of agent programs work:}
\begin{itemize}[noitemsep,nolistsep]
	\item \textbf{atomic representation}: Each state of the world is indivisible. Algorithms like search and game-playing, Hidden Markov models and Markov decision models work like this.
	\item \textbf{factored representation}: splits up each state of a fixed set of variables or attributes which each can have a value. Used in constraint satisfaction algorithms, propositional logic, planning, Bayesian networks.
	\item \textbf{structured representation}: here the different states have connections to each other. Used in relational databases, first-order logic, first-order probability models, knowledge-based learning and natural language understanding.
	\item more complex representations are more \textbf{expressive} and can capture everything more concise.
\end{itemize}

\subsection{Multiagent Planning}
p.425
\begin{itemize}[noitemsep,nolistsep]
	\item each agent tries to achieve is own goals with the help or hindrance of others
	\item wide degree of problems with various degrees of \textbf{decomposition of the monolithic agent}.
	\item multiple concurrent effectors => \textbf{multieffector planning} (like type and speaking at the same time).
	\item effectors are physically decoupled => \textbf{multibody planning}.
	\item if relevant sensor information foreach body can be pooled centrally or in each body ~ like single-agent problem.
	\item When communication constraint does not allow that: \textbf{decentralized planning problem}. planning phase is centralized, but execution phase is at least partially decoupled.
	\item single entity is doing the planning: one goal, that every body shares.
	\item When bodies do their own planning, they may share identical goals.
	\item \textbf{multibody}: centralized planning and execution send to each.
	\item \textbf{multiagent}: decentralized local planning, with coordination needed so they do not do the same thing.
	\item Usage of \textbf{incentives} (like salaries) so that goals of the central-planner and the individual align.
\end{itemize}

\underline{Multiple simultaneous actions:}
\begin{itemize}[noitemsep,nolistsep]
	\item \textbf{correct plan}: if executed by the actors, achieves the goal. Though multiagent might not agree to execute any particular plan.
	\item \textbf{joint action}: An Action for each actor defined => joint planning problem with branching factor b\^n (b = number of choices).
	\item if the actors are \textbf{loosely coupled} you can describe the system so that the problem complexity only scales linearly.
	\item standard approach: pretend the problems are completely decoupled and then fix up the interactions.
	\item \textbf{concurrent action list}: which actions must or most not be executed concurrently. (only one at a time)
\end{itemize}


\underline{Multiple agents: cooperation and coordination }
\begin{itemize}[noitemsep,nolistsep]
	\item each agent makes its own plan. Assume goals and knowledge base are shared.
	\item They \textbf{might choose different plans} and therefore collectively not achieve the common goal.
	\item \textbf{convention}: A constraint on the selection of joint plans. (cars: do not collide is achieved by “stay on the right side of the road”).
	\item widespread conventions: social laws.
	\item absence of convention: use communication to achieve common knowledge of a feasible joint plan.
	\item The agents can try to \textbf{recognize the plan other agents want to execute} and therefore use plan recognition to find the correct plan. This only works if it is unambiguously.
	\item an \textbf{ant} chooses its role according to the local conditions it observes.
	\item ants have a convention on the importance of roles.
	\item ants have some learning mechanism: a colony learns to make more successful and prudent actions over the course of its decades-long life, even though individual ants live only about a year.
	\item Another Example: \textbf{Boid}
	\item If all the boids execute their policies, the flock inhibits the emergent behavior of flying as a pseudorigid body with roughly constant density that does not disperse over time.
	\item \textbf{most difficult multiagent} problems involve both cooperation with members of one’s own team and competition against members of opposing teams, all without centralized control.
\end{itemize}

\subsection{Game Theory}
p.666

\subsection{Mechanism Design for Multiple Agents}
p.679

\subsection{Adversarial Search}
p.182

\subsection{Probabilistic Reasoning over Time}
p.587

\subsection{Reinforcement Learning}
p.830

\subsection{Planning Uncertain Movements (Potential Fields)}
p.993

\section{RTSAI - A Review of Real-Time Strategy Game AI - 2014}
\subsection{Tactical Decision making}
\begin{itemize}[noitemsep,nolistsep]
	\item In Practice: FSM mostly used.
	\item Tactical and micromanagement decisions can be seperated from other aspects of the game.
	\item learning domain knowledge is feasible
	\item Less focus in research.
\end{itemize}
\underline{Reinforcement Learning}
\begin{itemize}[noitemsep,nolistsep]
	\item Reinforcement Learning can be used. 
	\item requires clever state abstraction to learn effectively.
	\item RL not used for Strategic aspects as the problem space is larger and the reward function has delays.
	\item Used to learn an expected reward value for a particular actions.
	\item Using a hierarchical approach, where a unit is controlled individually with a higher-level group control affecting each individual's decision would decrease the complexity and make approaches more viable.
\end{itemize}
\underline{GameTree Search}
\begin{itemize}[noitemsep,nolistsep]
	\item Much Simplified scenario with a known deck of strategies and counters.
	\item Only then can a search algorithm work with skipping uninteresting states and the moves only be described with pairs and counters. (Nash).
	\item alpha-beta search / prunning.
	\item These simplification do not work well into a real game because of the lack of unit collisions and accerlations. Really bad at hit-and-run maneuver.
	\item For StarCraft: The Search will only allow up to eight units per side in a 2player battle before it is too slow.
\end{itemize}
\underline{Monte Carlo Planning}
\begin{itemize}[noitemsep,nolistsep]
	\item sample decision space using randomly generated plans to find successful ones.
	\item Monte Calro tree search may be effective at strategic level decision making for RTS Games.
	\item Using upper confidence bounds applied to trees (UCT). Plays game randomly out to a terminal state.
\end{itemize}
\underline{Other Models}
\begin{itemize}[noitemsep,nolistsep]
	\item using objectives, opportunities and threats in a bayesian model learned with RL. Paired with hierarchical FSM for different behavior sets for different scenarios.
	\item Case-Based Reasoning: selects most similar case for reuse, that only is executed when it is able to be executed. Also used reactionary cases (reflexes)
	\item Neuroevolution: Evolutionary Algorithms for Neural Networks: Each Unit has its own neural network. (rtNEAT). Environmental sources and actions as outputs. How well does it scale for more units?
\end{itemize}

\subsection{Strategic Decision making}
\begin{itemize}[noitemsep,nolistsep]
	\item In Research: Planning Systems. They determine a sequence of actions to be taken.
	\item Challenging, because of incomplete information.
	\item Goal: Humanlike level of competence and reduce development efffort for the industry for scripting methods.
	\item Main Techniques used: case-based planning (CBP), goal-drive autonomy (GDA) and hierarchical planning.
	\item Kohan 2: Kings of War: Fun AI with Goals and priorities that resulted in a fun opponent. Also easy to update.
\end{itemize}
\underline{Case-Based Planning}
\begin{itemize}[noitemsep,nolistsep]
	\item finds similar past situations from which to draw potential solutions to the current situation.
	\item for CBP: Solutions are a set of potential plans or subplans thate are likely to be effective.
	\item They exhibit poor reactivity at the strategic level and excessive reactivity at the action level.
	\item Can abstract the states into a state lattice of possible orders (build orders) combined with small set of features and abstracted actions.
	\item Approaches with using game logs and behaviors, goals and alive conditions.
	\item During plan execution it may be modified in order to adapt to unforeseen events. Examples use decision tree models. Decision tree predicts the high-level situation which determines the attrributes and their weights for the case selection. The Tree is learnt.
	\item can be combined with fuzzy sets to abstract state information and abstracting the regions of the map and their state with choke points, military presence, combat intensity, lost units and amounts of each friendly and enemy unit type.
	\item needs replays (some examples could achieve a 60\% winrate after one replay)
\end{itemize}
\underline{hierarchical Planning}
\begin{itemize}[noitemsep,nolistsep]
	\item breaking up the problem hierarchically reduces the complexity of the problem, but create new issues in coordination between the different levels 
	\item hierarchical plans maps well to the hierarchy of goals and subgoals in typical RTS games.
	\item Hierarchical Task Networks (HTN): contains tasks, their ordering and methods for achieving them. high level tasks can be decomposed up until concrete actions.
	\item They allow the agent to react dynamically to problems. 
	\item There has not been much work in learning HTNs for RTS only for simpler domains.
	\item can also use an active behavior tree which has paralle, sequentiall and conditional behaviors and goals in a tree structure. Here the tree is expanded during the execution by selecting behaviors. 
	\item Can combine this with other methods like hierarchical CBP.
\end{itemize}
\underline{Behavior Trees}
\begin{itemize}[noitemsep,nolistsep]
	\item a hierarchy of decision and action nodes, used alot by game designers.
	\item they can be edited using visual tools, which make the more accessable to non-programmers.
	\item as they are hierarchical they can cover a wide range of behavior. 
	\item They have been used to enable direct control of a case-based-planner's behavior: Combining Expert Knowledge and Learning from Demonstration in Real-Time Strategy Games. (seems nice)
\end{itemize}
\underline{Goal-Driven Autonomy}
\begin{itemize}[noitemsep,nolistsep]
	\item An agent reasons about its goals, identifies when they need to be updated and changes or adds to them as needed for subsequent planning and execution.
	\item Adresses the high- and low-level reactivity problem experienced by CBP by reasoning about and reacting to why a goal is succeding or failing.
	\item unexpected situation: record it as a discrepancy, generate an explanantion and form new goals to revise the plan.
	\item There is a CBR and RL Version of GDA that allows it to learn new domain knowledge which is not needed to be fed to it.
\end{itemize}


\subsection{Open Research Areas}
\underline{Game AI in Industry}
\begin{itemize}[noitemsep,nolistsep]
	\item In Practice: Industry uses older and simpler approaches.
	\item papers reason that AI Research will be useful for development by reducing the work involved in creating AI opponents, or by allowing game developers to create better AI opponents.
	\item Planetary Annihilations uses flow fields for path-finding and neural networks for squad unit control.
	\item industry triesl to create challenging but defeatable opponents that are fun to play against.
	\item the game is more fun when reasonably challenging.
	\item the agent tries to win at all cost, but the game industry wants an agent that looses in a more human-like way. it is fun to finding and taking advantage of opportunities and opponent's mistakes.
	\item usually classic AI techniques are enough to make a fun AI. It can cheat.
	\item the old techniques are predictable, reliable and easy to test and debug. Academic AI are seen as difficult to customize, tune, or tweak.
	\item could gain alot by making the AI more Humanlike: capable of cooperation, using surprises, deception, distractions and coordinated attacks, planning effective strategies and changing streategies to become less predictable. (Scott 2002)
	\item Being predictable and exploitable in the same fashion over multiple games: not fun.
	\item AI can make mistakes: They need to seem plausable in the context of the game. Something a human would do.
	\item Replicate Human PLaystyle (Turtle-ing), which gives the AI more presonality. This could be trained and learned from a human player (from demonstration and offline). Copy how player play the game.
	\item More work to be done how to make complex systems easier to tweak and customize. Produce specific behavior while retaining learning or reasoning capabilities.
\end{itemize}

\underline{Multiscale AI}
\begin{itemize}[noitemsep,nolistsep]
	\item currently bots require multiple abstractions and reasoining mechanics to work in concert. They usually seperate ways of handling tactical and strategic level decision making. Seperately maging resources, construction and reconnaissance.
	\item each module interacts with each other, so straightforward hierarchy of command is difficult, as high legel systems require direct knowledge of other systems.
	\item This is called multiscale AI problems.
	\item Some approaches use high-level commanders and sub-commanders. Commanders further down the hieararchy are increasingly focused on a particular task, but have lesss information about the overall game state, so therefore must rely on their parents to make them act appropriately in the bigger picture. They can use information from their parent in the hieararchy, but they are unable to react and coordinate with other low-level contreollers directly for cooperative actions.
\end{itemize}

\subsection{AI Techniques that can be used}
\begin{table}[h!]
	\centering
	\begin{tabular}{ c c }
		Tactical Decision Making & Strategic Decision Making and Plan Recognition\\
		%\toprule
		Reinforcement Learning & Case-Based Planning\\
		Game-Tree Search & Heararchical Planning\\
		Bayesian Models & Behavior Trees\\
		Case-Based Reasoning & Goal-Driven Autonomy\\
		Neural Networks &  State Space Planning\\
		& Evolutionary Algorithms\\
		& Cognitive Architectures\\
		& Deductive Reasoning\\
		& Probabilistic Reasoning\\
		& Case-Based Reasoning\\
	\end{tabular}	
\end{table}


\section{Ant Colony Optimization}
\subsection{Wikipedia Article}
\href{https://en.wikipedia.org/wiki/Ant_colony_optimization_algorithms}{Ant Colony Optimization Algorithm, Wikipedia}
\begin{itemize}[noitemsep,nolistsep]
	\item is used for solving computational problems which can be reduced to finding good paths through graphs.
	\item artificial ants locate optimal soluions by moving through a parameter space represneting all possible solutions.
	\item they record their positions and the quality of their solutions for later iterations to find better solutions (pheromones).
\end{itemize}

\section{UNSORTED}
Gordon 2000: Ants at Work.
\\
Gordon 2007: Control without hierarchy. Nature.
\\\textbf{Links:}\\
\\
\href{https://www.youtube.com/watch?v=X-iSQQgOd1A}{Ant Simulation Video 1}
\\
\href{https://www.youtube.com/watch?v=81GQNPJip2Y}{Ant Simulation Video 2}
\\
\href{https://www.youtube.com/watch?v=bqtqltqcQhw}{Boids Video}
\\
\href{https://en.wikipedia.org/wiki/Distributed_artificial_intelligence}{Distributed Artificial Intelligence, Wikipedia}
\\
\href{https://en.wikipedia.org/wiki/Multi-agent_learning}{Multi-agent learning, Wikipedia}
\\
\href{https://en.wikipedia.org/wiki/Bees_algorithm}{Bees algorithm, Wikipedia}
\\
\href{https://en.wikipedia.org/wiki/Swarm_intelligence}{Swarm Intelligence, Wikipedia}

\section{References \& Papers}
\subsection{Ant Colony Optimization (ACO)}
\href{https://www.sciencedirect.com/science/article/pii/S0888613X02000919}{ACO - Ant Colony Optimization for learning Bayesian network - 2002}

\subsection{Multiagent Systems (MAS)}
\href{https://ieeexplore.ieee.org/abstract/document/8352646}{MAS - Multi-Agent Systems - A Survey - 2018}
\\
\href{https://link.springer.com/book/10.1007%2F978-981-33-6718-0}{MAS - Distributed Cooperative Control and Communication for Multi-agent Systems - 2021}
\\
\href{https://link.springer.com/book/10.1007%2F978-3-030-69322-0}{MAS - PRIMA 2020 Principles and Practice of Multi-Agent Systems - 2021}
\\
\href{https://www.hindawi.com/journals/complexity/2017/3813912/}{MAS - The Multiagent Planning Problem - 2016}
\\
\href{https://link.springer.com/book/10.1007/978-3-642-15461-4}{MAS - Swarm Intelligence - 2010}
\\
\href{https://link.springer.com/book/10.1007/978-3-642-32650-9}{MAS - Swarm Intelligence - 2012}
\\
\href{https://link.springer.com/book/10.1007/978-3-319-09952-1}{MAS - Swarm Intelligence - 2014}
\\
\href{https://link.springer.com/book/10.1007/978-3-319-44427-7}{MAS - Swarm Intelligence - 2016}
\\
\href{https://link.springer.com/book/10.1007/978-3-030-00533-7}{MAS - Swarm Intelligence - 2018}
\\
\href{https://link.springer.com/book/10.1007/978-3-030-60376-2}{MAS - Swarm Intelligence - 2020}
\\
\href{https://www.diva-portal.org/smash/record.jsf?pid=diva2%3A805249&dswid=2610}{MAS - Using Multi-Agent Potential Fields in Real-Time Strategy Games - 2008}
\\
\href{https://dl.acm.org/doi/abs/10.5555/3306127.3331766}{MAS - Evaluating the Effectiveness of Multi-Agent Organisational Paradigms in a Real-Time Strategy Environment, Engineering Multiagent Systems Track - 2019}
\\
\href{https://dl.acm.org/doi/10.5555/3306127.3332052}{MAS - The StarCraft Multi-Agent Challenge - 2019}
\\
\href{https://dl.acm.org/doi/abs/10.1145/2371316.2371324}{MAS - Neuroevolution based multi-agent system for micromanagement in real-time strategy games - 2012}
\\
\href{https://ieeexplore.ieee.org/document/5035621}{MAS - Dealing with fog of war in a Real Time Strategy game environment - 2008}
\begin{itemize}[noitemsep,nolistsep]
	\item p.3: The Initial set of charges were found using trial and error => can use learning for that.
	\item using multi-agent potential fields for unit movements.
\end{itemize}

\href{https://link.springer.com/article/10.1007/s10458-006-7035-4}{MAS - Hierarchical multi-agent reinforcement learning - 2006}


\subsection{RTS AI}

\href{https://ojs.aaai.org/index.php/aimagazine/article/view/2478}{RTSAI - A Review of Real-Time Strategy Game AI - 2014}
\\
\href{https://dl.acm.org/doi/10.1145/3297156.3297188}{RTSAI - Artificial Intelligence Techniques on Real-time Strategy Games - 2018}
\\
\href{https://ieeexplore.ieee.org/abstract/document/7860394}{RTSAI - Informed Monte Carlo Tree Search for Real-Time Strategy games - 2016}
\\
\href{https://link.springer.com/chapter/10.1007/978-3-642-23291-6_15}{RTSAI - Case based Reasoning Research (Palmer) - 2011 - CBP - BT}

\subsection{RTS Learning}

\href{https://www.hindawi.com/journals/ijcgt/2009/129075/}{RTSLearn - Combining AI Methods for Learning Bots in a Real-Time Strategy Game - 2008}
\\
\href{https://ojs.aaai.org/index.php/AIIDE/article/view/12922}{RTSLearn - Combining Strategic Learning and Tactical Search in RTS Games - 2017}
\\
\href{https://ieeexplore.ieee.org/abstract/document/6374186}{RTSLearn - Imitative Learning for RTS - 2012}
\\
\href{https://ieeexplore.ieee.org/abstract/document/8490409}{RTSLearn - Deep RTS A Game Environment for Deep Reinforcement Learning in Real-Time Strategy Games - 2018}
\\
\href{https://ieeexplore.ieee.org/abstract/document/8490369}{RTSLearn - Learning Map-Independent Evaluation Functions for Real-Time Strategy Games - 2018}
\\
\href{https://ojs.aaai.org/index.php/AIIDE/article/view/12433}{RTSLearn - Learning Probabilistic Behavior Models in Real-Time Strategy Games - 2011}
\\
\href{https://link.springer.com/article/10.1007/s10994-006-8919-x}{RTSLearn - Machine Learning and Games - 2006}
\\
\href{https://www.sciencedirect.com/science/article/pii/S187705091501354X}{RTSLearn - Real Time Strategy Games: A Reinforcement Learning Approach - 2015}
\\
\href{https://arxiv.org/abs/1611.00625}{RTSLearn - Torchcraft - A Library for Machine Learning Research on Real-Time Strategy Games - 2016}
