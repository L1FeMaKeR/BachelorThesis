%% LaTeX2e class for student theses
%% sections/content.tex
%% 
%% Karlsruhe Institute of Technology
%% Institute for Program Structures and Data Organization
%% Chair for Software Design and Quality (SDQ)
%%
%% Dr.-Ing. Erik Burger
%% burger@kit.edu
%%
%% Version 1.3.5, 2020-06-26

\chapter{Information to sort}
\label{ch:Info}

\section{MARL - A Comprehensive Survey of Multiagent Reinforcement Learning - 2008}
\href{https://ieeexplore.ieee.org/abstract/document/4445757}{MARL - A Comprehensive Survey of Multiagent Reinforcement Learning - 2008}
\\
\underline{Benefits}
\begin{itemize}[noitemsep,nolistsep]
	\item can be parallelized.
	\item can use experience sharing via communication, or with a teacher-learner relationship.
	\item Failure of one agent can be covered by other agents.
	\item insertion of new agents => scaleable.
	\item MARL Complexity: Exponential in number of agents.
	\item exploration (new knowledge) - exploitation (current knowledge) - Tradeoff.
	\item They explore about the environment and other agents.
	\item need for coordination.
\end{itemize}
\underline{Application Domains}
\begin{itemize}[noitemsep,nolistsep]
	\item simulation better than real-life (better scalability and robustness).
	\item Distributed Control: for controlling processes (for larger industry plants).
	\begin{itemize}[noitemsep,nolistsep]
		\item avenue for future work.
		\item used for traffic, power or sensory networks.
		\item could also be used for pendulum systems.
	\end{itemize}
	\item Robotic Teams (Multirobot):
	\begin{itemize}[noitemsep,nolistsep]
		\item simulated 2D space. 
		\item navigation: Reach a goal with obstacles. Area sweeping (retrival of objects (also cooperative)).
		\item pursuit: Capture a prey robot.
	\end{itemize}
	\item Automated Trading: Exchange goods on electronic markets with negotiation and auctions.
	\item Resource Management: Cooperatie team to manage resources or as clients. (routing, load balancing).
\end{itemize}
\underline{Practicallity and Future works}
\begin{itemize}[noitemsep,nolistsep]
	\item Scalability Problem: Q-functions do not scale well with the size of the state-action space.
	\begin{itemize}[noitemsep,nolistsep]
		\item Approximation needed: for discrete large state-action spaces, for continous states and discrete actions or continious state and action.
		\item Heuristic in nature and only work in a narrow set of problems.
		\item Could use theoretical results on single-agent approximate RL.
		\item also could use discovery and exploitation of the decentralized, modular structure of the multiagent task.
	\end{itemize}
	\item MARL without prior knowledge is very slow.
	\begin{itemize}[noitemsep,nolistsep]
		\item Need humans to teach the agent.
		\item shaping: first simple task then scale them.
		\item could use reflex behavior.
		\item Knowledge about the task structure.
	\end{itemize}
	\item Incomplete, uncertain state measurements could be handled with partiall observability techniques (Markov decision process).
	\item Multiagent Goals needs a stable learning process for the environment and an adaption for the dynamics of other agents.
	\item using game-theory-based analysis to apply to the dynamics of the environment.
\end{itemize}

\section{!MAS - An Introduction to Multi-Agent Systems - 2010}
\href{https://link.springer.com/chapter/10.1007/978-3-642-14435-6_1}{MAS - An Introduction to Multi-Agent Systems - 2010}
\\
\underline{Benefits of using MAS in large systems}
\begin{itemize}[noitemsep,nolistsep]
	\item Increase in the speed and efficiency of the operation due to parallel computation and asynchronous operation.
	\item Graceful degradation whone one or more of the agent fail, thus increasing realibility and robustness of the system.
	\item Scalability and flexibility - Agents can be added as and when necessary.
	\item Cost Reduction: Individual agents cost much less than a centralized architecture
	\item Reusability: Agents with a modular structure can be easily replaced in other systems or be upgraded more easily than a monolithic sysetm.
\end{itemize}
\underline{Challenges of using MAS in large systems}
\begin{itemize}[noitemsep,nolistsep]
	\item environment: An agents action modify its own environment but also that of its neighbours. therefore they need to predict the action of the other agents so that they can reach a goal. This can be an unstable system. Environment dynamic: Is the effect caused by other agents or by the variation in the environment?
	\item perception: limited sensing range => each agent only has partial observability for the environment. Therefore the decisions reached might be sub-optimal.
	\item Abstraction: ???
	\item conflict resolution: lack of global view => conflict. therefore information on constraints, action preferences and goal prioritoes must be shared between agents. When to communicate what to which agent?
	\item Inference: Single-Agent: State-Action-Space can be mapped with trial and error. Multi-agent: each agent may or may not interact with each other. If they are heterogenous, they might even compete and have different goals. You need a fitting inference machanism
\end{itemize}
\subsection{Classification of MAS}
\underline{Internal Architecture}
\begin{itemize}[noitemsep,nolistsep]
	\item homogeneous: all agents have the same internal architecture (Local Goals, Sensor Capabilities, Internal states, Inference Mechanism and Possible Actions). In a typical distributed environment, overlap of sensory inputs is rarely present
	\item Heterogeneous: agents may differ in ability, structure and functionality. Because of the dynamics and location the actions chosen might differ between agents. their local goals may contradict the objective of other agents.
\end{itemize}
\underline{Agent Organization}
\begin{itemize}[noitemsep,nolistsep]
	\item hierarchical: typical: tree-structure. At different heights, different levels of autonomy. data from lower levels flow upwards. Control signal flows from high to low in the hierarchy.
	\begin{itemize}[noitemsep,nolistsep]
		\item simple: the decision making authority is a single agent of highest level. BUT: single point of Failure
		\item uniform: authority is distributed among the various agents, for better efficiency, fault tolerance, graceful degradation. Decisions made by agent with appropriate information. (MAS - TrafficControl - Neural Networks for Continuous Online Learning and Control - 2006)
	\end{itemize}
	\item holonic: fractal structure of several holons. Self-repeating. Used for large organizational behaviours in manufacturing and business. 
	\begin{itemize}[noitemsep,nolistsep]
		\item An agent that appears as a single entity might be composed of many sub-agents. They are not predetermined, but form through commitments.
		\item Each holon has a head agent that communicates with the environment or with other agents in the environment. It is selected either randomly, through a rotation policy, or selected by resource availability, communicaton capability.
		\item Holons can be nested to form Superholons.
		\item compare to tree: in Holons cross tree interactions and overlapping of holons is allowed.
		\item pro: abstraction good degree of freedom, good agent autonomy.
		\item contra: abstraction makes it difficult for other agents to predict the resulting actions of the holon.
	\end{itemize}
	\item coalitions: group of agents come together for a short time to increase utility or performance of the individual agents in a group. they cease to exist when the performance goal is achieved.
	\begin{itemize}[noitemsep,nolistsep]
		\item coalition may have either a flat or a hierarchical architecture.
		\item It may have an leading agent to act as a representative. 
		\item overlap is allowed. this increased complexity of computation of the negotiation strategy.
		\item You can have one coaltion with all agents => maximum performance of system. Impractical due to restraints on communication and resources.
		\item minimize amount of colations: because of the cost of creating and dissolving a colation group.
	\end{itemize}
	\item teams: agents work together to increase the overall performance of the group, rather than working as individual agents.
	\begin{itemize}[noitemsep,nolistsep]
		\item their interactions can be arbitrary and the goals and roals can vary with the performance of the group.
		\item large team size is not beneficial under all conditions. some compromises must be made.
		\item large teams offer a better visibility of the environment. but is slower computation wise. Learning-Performance Tradeoff.
		\item computation cost usually much greater than coalitions.
	\end{itemize}
\end{itemize}
\underline{Communication}
\begin{itemize}[noitemsep,nolistsep]
	\item local communication: agents directly communicate similar to message passing. there is no place to store information. creates distributed architecture. used in: (25),(37),(38).
	\item blackboards: a group of agents share a data repository which is provided for efficient storage.
	\begin{itemize}[noitemsep,nolistsep]
		\item can hold design data and control knowledge, accessable by the agents.
		\item control shell: notfies the agent when relevant data is available.
		\item single point of failure.
	\end{itemize}
	\item agent communication language (ACL): common framework for interaction and information sharing. (40).
	\begin{itemize}[noitemsep,nolistsep]
		\item procedural approach: modelled as a sharing of the precedural directives. Shared how an agent does a specific task or the entire working of the agent itself. Script Languages often used. Disadvantage: necessitiy of providing information on the recipient agent, which is in most cases partially known. Also how to merge the scripts into one executable. Not preferred method.
		\item declarative approach: sharing of statements for definitions, assumptions assertions, axioms etc. Short declarative statements as length increases probability of information corruption. Example: ARPA knowledge sharing effort.
		\item Best known inner languages: Knowledge Interchange Format. Information exchange is implicitly embedded in KIF. But the package size grows with the increase in embedded information. Solution: High-level Languages like KQML (Knowledge QUery and Manipulation Language)
	\end{itemize}
\end{itemize}
\underline{Decision making in Multi-Agent Systems}
\begin{itemize}[noitemsep,nolistsep]
	\item undercainty: effects of a specific actions on the environment and dynamics because of the other agents.
	\item Methodology to try and find a joint action or equilibrium point which maximizes the reward of every agent.
	\item Typically modelled with game theory method. Strategic games:
	\begin{itemize}[noitemsep,nolistsep]
		\item a set of players (agents)
		\item Foreach player, there is a set of actions
		\item Foreach player, the prefeernces over a set of actions profiles
		\item payoff with the combination of action, a joint-action, that is assumed to be predefined.
		\item all actions are observable forall agents.
		\item make the assumption that all participating agents are rational.
	\end{itemize}
	\item Nash equilibrium: for a payoff matrix: An action profile (joint-action), where no player can do better by choosing one of the actions differently, given that the other player chose a specific action.
	\item there might be multiple nash equilibrium, so that there is no dominant solution. Here the coordination of MAS is needed to find a solution.
	\item Iterated Elimination Method: Strongly dominated actions are iteratively eliminated. This fails if there are no strictly dominated actions available.
\end{itemize}
\underline{Coordination}
\begin{itemize}[noitemsep,nolistsep]
	\item agents work in parallel, therefore they need to be coordinated or synchronize the actions to ensure stability of the system.
	\item other reasons: prevent chaos, meet global constraints, utilize distributed resources, prevent conflicts, improve efficiency.
	\item achievable with constraints on the joint actions or by using informatil collated from neighbouring agents. Used to find the equilibrium action.
	\item payoff matrix necessary might be difficult to determine. It increases expenentially in the number of agents and action choices.
	\item dividing the game into subgames: roles (permitted actions is reduced, good for distributed coordination or centralized coordination)
	\item Coordination via Protocol.
	\begin{itemize}[noitemsep,nolistsep]
		\item negotioation to arrive an approdiate solutions.
		\item Agents assume the role of manager (divide the problem) and contractor (who deals with the subproblems).
		\item The manager and contractor are working in a bidding system.
		\item Example: FIPA model
		\item disadvantage: assumption of the existence of an cooperative agent. It is very communication intensive
	\end{itemize}
	\item Coordination via Graphs: Problem is subdivided into easer problems. Assume the payoffs can be linear combinated from the local payoffs of the sub-games. THen just eliminate agents to find the optimal joint.
	\item Can also use belif models. Internal models of an agent on how he believes the environment works (needs to differentiate between environment and effects of other agents).
\end{itemize}
\underline{Learning}
\begin{itemize}[noitemsep,nolistsep]
	\item active learning: analysing the observations to creat a belief or internal model of the corresponding situated agent's environment. 
	\begin{itemize}[noitemsep,nolistsep]
		\item can be performed by using a deductive, inductive or probabilistic reasoning approach.
		\item deductive: inference to explain an instance or state-action sequence using his knowledge. It is deduced or inferred from the original knowledge it is nothing new. It could form new parts of the knowledge base. uncertainty is usually disregarded (not good for real-time)
		\item inductive: learning from observations of state-action pair. Good when environment can be presented in terms of some generalized statements. they use the correlation between observations and the action space.
		\item probabilistic: assumption: knowledge base or belief model can be represented as probabilities of occurrence of events. observations of the environment is used to predict the internal state of the agent. Good example: Bayesian learning. Difficult for MAS, as the joint probability scales poorly in the number of agents.
	\end{itemize}
	\item reactive learning: updating belief without having the actual knowledge of what needs to be learnt.
	\begin{itemize}[noitemsep,nolistsep]
		\item useful when the underlying model of the agent or the environment is not known clearly and are black boxes.
		\item can be ssen in agents which utilize connections systems such as NN.
		\item can use reactive multi-agent feed forward neural networks.
		\item they depend on the application domain and are therefore rarely employed in real world scenarios.
	\end{itemize}
	\item learning based on consequences:
	\begin{itemize}[noitemsep,nolistsep]
		\item learning methods based on evaluation of the goodness of selected action. like in reinforcement learning.
		\item programming the agents using reward and punishment scalar signals without specifying how the task is to be achieved.
		\item learnt through trial and error and interaction with the environment.
		\item usually used when action space is small and descret. Recent developments allow them to work in continious and large state-action space scenarios.
		\item An agent is usally represented as a Markov Decision Process.
		\item Expectaation operator optmal policy is the argmax of the Q-value, which uses the bellman equation. Bellman equation is solved iteratively.
		\item The solution is referred to as q-learning method.
		\item For MAS the reinforcement learning method has the problem of combinatorial explosion in the state-action pairs.
		\item The information must be passed between the agents for effective learning.
	\end{itemize}
\end{itemize}

\section{Artificial Intelligence - A modern Approach}
\subsection{Agents and Environments}
p.34
\begin{itemize}[noitemsep,nolistsep]
	\item \textbf{agent}: anything that perceives its \textbf{environment} through \textbf{sensors} and acting upon that environment using \textbf{actuators}.
	\item \textbf{percept}: agent’s perceptual inputs at any given instance. Percept sequence is a complete history of perception.
	\item agents choice of action decided upon the history of perception, but not anything it has not perceived.
	\item its behavior is described by the \textbf{agent function}, which is internally implemented by the \textbf{agent program}.
\end{itemize}

\subsection{Rational Agent}
p.36
\begin{itemize}[noitemsep,nolistsep]
	\item \textbf{rational agent}: it does the correct thing. Correctness is determined by a performance measure, which is determined by the changed environment states.
	\item design \textbf{performance measures} according to what one actually wants in the environment, rather than according to how one thinks the agent should behave.
	\item rational depends on:
	\begin{itemize}
		\item the performance measure that defines the criterion of success
		\item the agent’s prior knowledge of the environment.
		\item The actions that the agent can perform.
		\item The agent’s percept sequence of data.
	\end{itemize}
	\item depending on the measures the agent might be rational or not. 
	\item an \textbf{omniscient agent} knows the actual outcome of its actions and can act accordingly, but this is impossible in reality.
	\item rationality maximizes expected performance, while perfection (omniscient) maximizes actual performance.
	\item agents can do actions in order to modify future percepts, called \textbf{information gathering, or exploration}.
	\item rational agents learn as much as possible from what it perceives.
	\item his knowledge can be augmented and modified as it gains experience.
	\item if the agent relies on the prior knowledge of its designer rather than on its own percepts, we say that the agent lacks \textbf{autonomy}.
	\item it should learn what it can to compensate for partial or incorrect prior knowledge.
	\item give it some initial knowledge and the ability to learn, so it will become independent of its prior knowledge.
\end{itemize}

\subsection{Nature of Environments}
p.40
\begin{itemize}[noitemsep,nolistsep]
	\item \textbf{task environments}: the “problems” to which rational agents are the “solutions”.
	\item Describe the task environment in the following aspects P(Performance measure), E(Environment), A(Actuators), S(Sensors).
	\item \textbf{fully observable}: the agent’s sensors give it access to the complete state of the environment. All aspects that are relevant to the choice of actions
	\item \textbf{partially observable}: otherwise. Because of missing sensors or noise.
	\item no sensors: unobservable
	\item single-agent environments and multi-agent environments.
	\item multi-agent can be either competitive (chess) or cooperative (avoiding collisions maximizes performance).
	\item \textbf{communication} emerges as a rational behavior in multiagent environments.
	\item randomized behavior is rational because it avoids the pitfalls of predictability.
	\item \textbf{Deterministic}: next state of environment is completely determined by the current state and the action executed by the agent, otherwise it is \textbf{stochastic}.
	\item you can ignore uncertainty that arises purely from the actions of other agents in a multiagent environment.
	\item If the environment is partial observable, it could appear to be stochastic, which implies quantifiable outcomes in terms of probabilities.
	\item an environment is \textbf{uncertain} if it is not fully observable or not deterministic. 
	\item \textbf{episodic}: the agent’s experience is divided into atomic episodes. In each the agent receives a percept and performs a single episode. The next episode does not depend on the actions taken in previous episodes, otherwise it is \textbf{sequential}.
	\item When the environment can change while the agent is deliberating, then the environment is \textbf{dynamic} for that agent otherwise it is \textbf{static}.
	\item if the environment itself does not change with the passage of time but the agent’s performance score does, then we say the environment is \textbf{semi dynamic}.
	\item \textbf{discrete/continuous} applies to the state of the environment, to the way time is handled, and to the percepts and actions of the agents.
	\item \textbf{known vs. unknown}: refers to the agent’s state of knowledge about the “laws of physics” of the environment. Known environment, the outcomes for all actions are given, otherwise the agent needs to learn how it works. An environment can be known, but partially observable (solitaire: I know the rules but still unable to see the cards that have not yet been turned over)
	\item hardest case: partially observable, multiagent, stochastic, sequential, dynamic, continuous, and unknown
	\item \textbf{environment class}: multiple environment scenarios to train it for multiple situations.
	\item you can create an \textbf{environment generator}, that selects environments in which to run the agent.
\end{itemize}

\subsection{Structure of Agents}
p.46
\begin{itemize}[noitemsep,nolistsep]
	\item agent = architecture (computing device) + program (agent program).
	\item agent programs take the current percept as input and return an action to the actuators.
	\item agent program takes the current percept, agent function which takes the entire percept history.
	\item \textbf{table driven agent}: Uses a table of actions indexed by percept sequences. This table grows way to fast and is therefore not practical.
\end{itemize}

\underline{Simple reflex agents:}
\begin{itemize}[noitemsep,nolistsep]
	\item \textbf{simple reflex agents}: Select the actions on the basis of the current percept, ignoring the rest of the history.
	\item \textbf{condition-action-rule}: these agents create actions in a specific condition (if-then). These connections can be seen as reflexes.
	\item uses an \textbf{interpret-input} function as well as a \textbf{rule-match} function.
	\item they need the environment to be fully observable. They could run into infinite loops.
	\item you can mitigate this by using randomization for the actions. Which is non-rational for single agent environments.
\end{itemize}

\underline{Model-based reflex agents:}
\begin{itemize}[noitemsep,nolistsep]
	\item keep track of the part of the world an agent cannot see now. It maintains some sort of \textbf{internal state} that depends on the percept history.
	\item agents needs to know how the world evolves independently of the agent and how the agent’s own actions affect the world.
	\item with this it creates a \textbf{model} of the world hence it is called model-based agent.
	\item it needs to update this state given sensor data.
	\item this model is a \textbf{best guess} and does not determine the entire current state of the environment exactly.
\end{itemize}

\underline{Goal-based agents:}
\begin{itemize}[noitemsep,nolistsep]
	\item an agent needs some sort of \textbf{goal information} that describes situations that are desirable. This can also be combined with the model.
	\item Usually agents need to do multiple actions to fulfill a goal which requires \textbf{search} and \textbf{planning}.
	\item this also involves consideration of the future.
	\item the goal-based agent’s behavior can be easily changed to go to a different destination by using a goal where a reflex agent needs completely now rules.
\end{itemize}

\underline{Utility-based agents:}
\begin{itemize}[noitemsep,nolistsep]
	\item goals provide a crude binary distinction between good and bad states.
	\item use an internal \textbf{utility function} to create a performance measure.
	\item if the external performance measure and the internal utility function agree, the agent will act rationally.
	\item if you have conflicting goals the utility function can specify the appropriate \textbf{tradeoff}.
	\item if multiple goals cannot be achieved with certainty, utility provides a way to determine the \textbf{likelihood} of success.
	\item a rational utility-based agent chooses the action that \textbf{maximizes the expected utility}.
	\item any rational agent must behave as if it possesses a utility function whose expected value it tries to maximize.
	\item a utility-based agent must model and keep track of its environment.
\end{itemize}

\underline{Learning Agents:}
\begin{itemize}[noitemsep,nolistsep]
	\item it allows the agent to operate in initially unknown environments and to become more competent than its initial knowledge alone might allow.
	\item 4 conceptual components: \textbf{learning element} (responsible for improvements), \textbf{performance element} (select external action), \textbf{critic} (gives feedback to change the learning element), \textbf{problem generator} (suggesting actions that lead to new and informative experiences).
	\item critic tells the learning element how well the agent is doing given a performance standard. It tells the agent which percepts are good and which are bad.
	\item problem generator allows for exploration and suboptimal actions to discover better actions in the long run.
	\item learning element: simplest case: learning directly from the percept sequence.
	\item the \textbf{performance standard} distinguishes part of the incoming percept as a reward or penalty that provides direct feedback on the quality of the agent’s behavior.
\end{itemize}

\underline{How the components of agent programs work:}
\begin{itemize}[noitemsep,nolistsep]
	\item \textbf{atomic representation}: Each state of the world is indivisible. Algorithms like search and game-playing, Hidden Markov models and Markov decision models work like this.
	\item \textbf{factored representation}: splits up each state of a fixed set of variables or attributes which each can have a value. Used in constraint satisfaction algorithms, propositional logic, planning, Bayesian networks.
	\item \textbf{structured representation}: here the different states have connections to each other. Used in relational databases, first-order logic, first-order probability models, knowledge-based learning and natural language understanding.
	\item more complex representations are more \textbf{expressive} and can capture everything more concise.
\end{itemize}

\subsection{Multiagent Planning}
p.425
\begin{itemize}[noitemsep,nolistsep]
	\item each agent tries to achieve is own goals with the help or hindrance of others
	\item wide degree of problems with various degrees of \textbf{decomposition of the monolithic agent}.
	\item multiple concurrent effectors => \textbf{multieffector planning} (like type and speaking at the same time).
	\item effectors are physically decoupled => \textbf{multibody planning}.
	\item if relevant sensor information foreach body can be pooled centrally or in each body ~ like single-agent problem.
	\item When communication constraint does not allow that: \textbf{decentralized planning problem}. planning phase is centralized, but execution phase is at least partially decoupled.
	\item single entity is doing the planning: one goal, that every body shares.
	\item When bodies do their own planning, they may share identical goals.
	\item \textbf{multibody}: centralized planning and execution send to each.
	\item \textbf{multiagent}: decentralized local planning, with coordination needed so they do not do the same thing.
	\item Usage of \textbf{incentives} (like salaries) so that goals of the central-planner and the individual align.
\end{itemize}

\underline{Multiple simultaneous actions:}
\begin{itemize}[noitemsep,nolistsep]
	\item \textbf{correct plan}: if executed by the actors, achieves the goal. Though multiagent might not agree to execute any particular plan.
	\item \textbf{joint action}: An Action for each actor defined => joint planning problem with branching factor b\^n (b = number of choices).
	\item if the actors are \textbf{loosely coupled} you can describe the system so that the problem complexity only scales linearly.
	\item standard approach: pretend the problems are completely decoupled and then fix up the interactions.
	\item \textbf{concurrent action list}: which actions must or most not be executed concurrently. (only one at a time)
\end{itemize}


\underline{Multiple agents: cooperation and coordination }
\begin{itemize}[noitemsep,nolistsep]
	\item each agent makes its own plan. Assume goals and knowledge base are shared.
	\item They \textbf{might choose different plans} and therefore collectively not achieve the common goal.
	\item \textbf{convention}: A constraint on the selection of joint plans. (cars: do not collide is achieved by “stay on the right side of the road”).
	\item widespread conventions: social laws.
	\item absence of convention: use communication to achieve common knowledge of a feasible joint plan.
	\item The agents can try to \textbf{recognize the plan other agents want to execute} and therefore use plan recognition to find the correct plan. This only works if it is unambiguously.
	\item an \textbf{ant} chooses its role according to the local conditions it observes.
	\item ants have a convention on the importance of roles.
	\item ants have some learning mechanism: a colony learns to make more successful and prudent actions over the course of its decades-long life, even though individual ants live only about a year.
	\item Another Example: \textbf{Boid}
	\item If all the boids execute their policies, the flock inhibits the emergent behavior of flying as a pseudorigid body with roughly constant density that does not disperse over time.
	\item \textbf{most difficult multiagent} problems involve both cooperation with members of one’s own team and competition against members of opposing teams, all without centralized control.
\end{itemize}

\subsection{Game Theory}
p.666

\subsection{Mechanism Design for Multiple Agents}
p.679

\subsection{Adversarial Search}
p.182

\subsection{Probabilistic Reasoning over Time}
p.587

\subsection{Reinforcement Learning}
p.830

\subsection{Planning Uncertain Movements (Potential Fields)}
p.993


\section{Ant Colony Optimization}
\subsection{Wikipedia Article}
\href{https://en.wikipedia.org/wiki/Ant_colony_optimization_algorithms}{Ant Colony Optimization Algorithm, Wikipedia}
\begin{itemize}[noitemsep,nolistsep]
	\item is used for solving computational problems which can be reduced to finding good paths through graphs.
	\item artificial ants locate optimal soluions by moving through a parameter space represneting all possible solutions.
	\item they record their positions and the quality of their solutions for later iterations to find better solutions (pheromones).
\end{itemize}

\section{UNSORTED}
Gordon 2000: Ants at Work.
\\
Gordon 2007: Control without hierarchy. Nature.
\\\textbf{Links:}\\
\\
\href{https://www.youtube.com/watch?v=X-iSQQgOd1A}{Ant Simulation Video 1}
\\
\href{https://www.youtube.com/watch?v=81GQNPJip2Y}{Ant Simulation Video 2}
\\
\href{https://www.youtube.com/watch?v=bqtqltqcQhw}{Boids Video}
\\
\href{https://en.wikipedia.org/wiki/Distributed_artificial_intelligence}{Distributed Artificial Intelligence, Wikipedia}
\\
\href{https://en.wikipedia.org/wiki/Multi-agent_learning}{Multi-agent learning, Wikipedia}
\\
\href{https://en.wikipedia.org/wiki/Bees_algorithm}{Bees algorithm, Wikipedia}
\\
\href{https://en.wikipedia.org/wiki/Swarm_intelligence}{Swarm Intelligence, Wikipedia}


\section{References \& Papers}
\subsection{Ant Colony Optimization (ACO)}
\href{https://www.sciencedirect.com/science/article/pii/S0888613X02000919}{ACO - Ant Colony Optimization for learning Bayesian network - 2002}


\subsection{Multi Agent Reinforcement Learning (MARL)}
\href{https://dl.acm.org/doi/abs/10.5555/645527.657296}{MARL - Multiagent Reinforcement Learning - Theoretical Framework and an Algorithm - 1998}
\\
\href{https://publikationen.bibliothek.kit.edu/1000118251}{MARL - Deep Reinforcement Learning for Robot Swarms - 2019 - KIT}
\\
\href{https://link.springer.com/article/10.1007/s10458-006-7035-4}{MARL - Hierarchical multi-agent reinforcement learning - 2006}
\\
\href{https://arxiv.org/abs/1605.06676}{MARL - Learning to Communicate with Deep Multi-Agent Reinforcement Learning - 2016}
\\
\href{https://www.cambridge.org/core/journals/knowledge-engineering-review/article/reward-shaping-for-knowledgebased-multiobjective-multiagent-reinforcement-learning/75F1507F7CAC7C6625F87AE7CD344D52}{MARL - Reward shaping for knowledge-based multi-objective multi-agent reinforcement learning - 2017}
\\
\href{https://link.springer.com/article/10.1007/s10489-020-01755-8}{MARL - GAMA - Graph Attention Multi-agent reinforcement learning algorithm for cooperation - 2020}
\\
\href{https://www.cambridge.org/core/journals/knowledge-engineering-review/article/planbased-reward-shaping-for-multiagent-reinforcement-learning/B173D25B1006B755667616C3A3EB3BE5}{MARL - Plan-based reward shaping for multi-agent reinforcement learning - 2016}
\\
\href{https://www.mdpi.com/2073-8994/10/10/461}{MARL - Multi-Agent Reinforcement Learning Using Linear Fuzzy Model Applied to Cooperative Mobile Robots - 2018}
\\
\href{http://ai.stanford.edu/people/shoham/www%20papers/MALearning_ACriticalSurvey_2003_0516.pdf}{MARL - Multi-Agent Reinforcement Learning - a critical survey - 2003}
\\
\href{http://proceedings.mlr.press/v70/foerster17b.html}{MARL - Stabilising Experience Replay for Deep Multi-Agent Reinforcement Learning - 2017}
\\
\href{http://proceedings.mlr.press/v80/yang18d.html}{MARL - Mean Field Multi-Agent Reinforcement Learning - 2018}
\\
\href{https://arxiv.org/abs/1911.10635}{MARL - Multi-Agent Reinforcement Learning A Selective Overview of Theories and Algorithms - 2021}
\\
\href{https://dl.acm.org/doi/abs/10.5555/2484920.2485093}{MARL - Coordinating multi-agent reinforcement learning with limited communication - 2013}
\\
\href{https://arxiv.org/abs/1807.09427}{MARL - Multi-Agent Reinforcement Learning A Report on Challenges and Approaches - 2018}
\\
\href{https://proceedings.neurips.cc/paper/2019/hash/07a9d3fed4c5ea6b17e80258dee231fa-Abstract.html}{MARL - LIIR - Learning Individual Intrinsic Reward inMulti-Agent Reinforcement Learning - 2019}
\\
\href{https://arxiv.org/abs/2009.14471}{MARL - PettingZoo - Gym for Multi-Agent Reinforcement Learning - 2020}
\\
\href{https://ieeexplore.ieee.org/abstract/document/8619581}{MARL - Networked Multi-Agent Reinforcement Learning in Continuous Spaces - 2018}
\\
\href{https://arxiv.org/abs/1908.03963}{MARL - A Review of Cooperative Multi-Agent Deep Reinforcement Learning - 2019}
\\
\href{https://link.springer.com/chapter/10.1007/978-3-642-29946-9_25}{MARL - Transfer Learning in Multi-agent ReinforcementLearning Domains - 2011}
\\
\href{https://ieeexplore.ieee.org/abstract/document/8851784}{MARL - Parallel Transfer Learning in Multi-Agent Systems What, when and how to transfer - 2019}
\\
\href{http://proceedings.mlr.press/v119/wang20f.html}{MARL - ROMA Multi-Agent Reinforcement Learning with Emergent Roles - 2020}
\\
\href{https://link.springer.com/chapter/10.1007/3-540-62934-3_39}{MARL - A modular approach to multi-agent reinforcement learning - 2005}
\\
\href{https://www.sciencedirect.com/science/article/pii/S0893608099000246}{MARL - Multi-agent reinforcement learning weighting and partitioning - 1999}
\\
\href{https://www.scitepress.org/Papers/2011/31856/}{MAS - Transfer Learning for Multi-agent Coordination - 2011}
\\
\href{https://arxiv.org/abs/2002.08030}{MARL - Transfer among Agents An Efficient Multiagent Transfer Learning Framework - 2020}
\\
\href{https://link.springer.com/article/10.1007/s10458-019-09430-0}{MARL - Agents teaching agents a survey on inter-agent transfer learning - 2019}
\\
\href{https://www.jair.org/index.php/jair/article/view/11396}{MARL - A Survey on Transfer Learning for Multiagent Reinforcement Learning Systems  - 2019}



\subsection{Multiagent Systems (MAS)}
\href{https://ieeexplore.ieee.org/abstract/document/8352646}{MAS - Multi-Agent Systems - A Survey - 2018}
\\
\href{https://link.springer.com/book/10.1007%2F978-981-33-6718-0}{MAS - Distributed Cooperative Control and Communication for Multi-agent Systems - 2021}
\\
\href{https://link.springer.com/book/10.1007%2F978-3-030-69322-0}{MAS - PRIMA 2020 Principles and Practice of Multi-Agent Systems - 2021}
\\
\href{https://www.hindawi.com/journals/complexity/2017/3813912/}{MAS - The Multiagent Planning Problem - 2016}
\\
\href{https://link.springer.com/book/10.1007/978-3-642-15461-4}{MAS - Swarm Intelligence - 2010}
\\
\href{https://link.springer.com/book/10.1007/978-3-642-32650-9}{MAS - Swarm Intelligence - 2012}
\\
\href{https://link.springer.com/book/10.1007/978-3-319-09952-1}{MAS - Swarm Intelligence - 2014}
\\
\href{https://link.springer.com/book/10.1007/978-3-319-44427-7}{MAS - Swarm Intelligence - 2016}
\\
\href{https://link.springer.com/book/10.1007/978-3-030-00533-7}{MAS - Swarm Intelligence - 2018}
\\
\href{https://link.springer.com/book/10.1007/978-3-030-60376-2}{MAS - Swarm Intelligence - 2020}
\\
\href{https://ulir.ul.ie/handle/10344/3305}{MAS - Transfer learning in multi-agent systems through paralllel transfer - 2013}
\\
\href{https://link.springer.com/chapter/10.1007/978-3-540-71618-1_8}{MAS - Co-evolutionary Multi-agent System with Predator-Prey Mechanism for Multi-objective Optimization - 2007}
\\
\href{https://ieeexplore.ieee.org/abstract/document/4427756}{MAS - Hierarchical Control in a Multiagent System - 2007}
\\
\href{https://link.springer.com/chapter/10.1007/978-3-540-25928-2_6}{MAS - Holonic - A Taxonomy of Autonomy in Multiagent Organisation - 2003}
\\
\href{https://dl.acm.org/doi/abs/10.1017/S0269888905000317}{MAS - A survey of multi-agent organizational paradigms - 2004}
\\
\href{https://link.springer.com/article/10.1023/A:1008942012299}{MAS - HomoHetero - Multiagent Systems A Survey from a Machine Learning Perspective - 2000}

\subsection{Applications}
\href{https://ieeexplore.ieee.org/abstract/document/4012019}{MAS - TrafficControl - Neural Networks for Continuous Online Learning and Control - 2006}
\\