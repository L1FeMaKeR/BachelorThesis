% !TeX spellcheck = en_US
% !TeX encoding = UTF-8
% !TeX root = ../thesis.tex

\chapter*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}
% 224-234 words

Multi-Agent Reinforcement Learning (MARL) is an ever expanding field of research, which deals with real world problems that require multiple entities, called agents, to work cooperatively, competitively, or as a mixture of both. Practical applications range from the simulation of ants, bees and birds, coordinating network systems aswell as search-and-rescue operations. The agents need to learn to correctly use potentially large amounts of observation data to make informed decisions. Therefore an efficient way to process observation data is needed for effective learning, that can scale to a large number of agents. Graph Neural Networks (GNN) allow for learning processes to work on graphs. Currently, they are a growing field of research. GNNs are a natural representation for modelling inter-agent communications. We want to investigate the effect of GNNs on learning and the resulting policy on MARL tasks. We will focus on the effect of multiple GNN passes on information propagation, especially in partial observability with a short range.\par

This work will introduce an architecture, that uses Proximal Policy Optimization (PPO) for training with a graph base. This allows scaling to multiple GNN passes. Our approach is based on the previosly proposed multi-agent deep reinforcement learning architecture found in \citet{RobinRuede2021}. Furthermore we expand the architecture to work on heterogeneous graphs, which is required for our observation model of more complex tasks. We then evaluate this architecture in multiple multi-agent tasks to show the benefit of multiple GNN passes. 
\par

Our results show that under different kinds of constraints, multiple GNN passes do improve performance. These include small latent dimensions, small observation radii and low amount of observation information. Therefore, MARL Problems can benefit from multiple hops.
